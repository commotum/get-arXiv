<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api/+EABYAHs6N8BJPUHFjPe5nCu5Ek</id>
  <title>arXiv Query: search_query=au:"Ilya Sutskever"&amp;id_list=&amp;start=0&amp;max_results=50</title>
  <updated>2026-02-06T20:27:01Z</updated>
  <link href="https://arxiv.org/api/query?search_query=au:%22Ilya+Sutskever%22&amp;start=0&amp;max_results=50&amp;id_list=" type="application/atom+xml"/>
  <opensearch:itemsPerPage>50</opensearch:itemsPerPage>
  <opensearch:totalResults>59</opensearch:totalResults>
  <opensearch:startIndex>0</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/2412.16720v1</id>
    <title>OpenAI o1 System Card</title>
    <updated>2024-12-21T18:04:31Z</updated>
    <link href="https://arxiv.org/abs/2412.16720v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2412.16720v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The o1 model series is trained with large-scale reinforcement learning to reason using chain of thought. These advanced reasoning capabilities provide new avenues for improving the safety and robustness of our models. In particular, our models can reason about our safety policies in context when responding to potentially unsafe prompts, through deliberative alignment. This leads to state-of-the-art performance on certain benchmarks for risks such as generating illicit advice, choosing stereotyped responses, and succumbing to known jailbreaks. Training models to incorporate a chain of thought before answering has the potential to unlock substantial benefits, while also increasing potential risks that stem from heightened intelligence. Our results underscore the need for building robust alignment methods, extensively stress-testing their efficacy, and maintaining meticulous risk management protocols. This report outlines the safety work carried out for the OpenAI o1 and OpenAI o1-mini models, including safety evaluations, external red teaming, and Preparedness Framework evaluations.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2024-12-21T18:04:31Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name> OpenAI</name>
    </author>
    <author>
      <name> :</name>
    </author>
    <author>
      <name>Aaron Jaech</name>
    </author>
    <author>
      <name>Adam Kalai</name>
    </author>
    <author>
      <name>Adam Lerer</name>
    </author>
    <author>
      <name>Adam Richardson</name>
    </author>
    <author>
      <name>Ahmed El-Kishky</name>
    </author>
    <author>
      <name>Aiden Low</name>
    </author>
    <author>
      <name>Alec Helyar</name>
    </author>
    <author>
      <name>Aleksander Madry</name>
    </author>
    <author>
      <name>Alex Beutel</name>
    </author>
    <author>
      <name>Alex Carney</name>
    </author>
    <author>
      <name>Alex Iftimie</name>
    </author>
    <author>
      <name>Alex Karpenko</name>
    </author>
    <author>
      <name>Alex Tachard Passos</name>
    </author>
    <author>
      <name>Alexander Neitz</name>
    </author>
    <author>
      <name>Alexander Prokofiev</name>
    </author>
    <author>
      <name>Alexander Wei</name>
    </author>
    <author>
      <name>Allison Tam</name>
    </author>
    <author>
      <name>Ally Bennett</name>
    </author>
    <author>
      <name>Ananya Kumar</name>
    </author>
    <author>
      <name>Andre Saraiva</name>
    </author>
    <author>
      <name>Andrea Vallone</name>
    </author>
    <author>
      <name>Andrew Duberstein</name>
    </author>
    <author>
      <name>Andrew Kondrich</name>
    </author>
    <author>
      <name>Andrey Mishchenko</name>
    </author>
    <author>
      <name>Andy Applebaum</name>
    </author>
    <author>
      <name>Angela Jiang</name>
    </author>
    <author>
      <name>Ashvin Nair</name>
    </author>
    <author>
      <name>Barret Zoph</name>
    </author>
    <author>
      <name>Behrooz Ghorbani</name>
    </author>
    <author>
      <name>Ben Rossen</name>
    </author>
    <author>
      <name>Benjamin Sokolowsky</name>
    </author>
    <author>
      <name>Boaz Barak</name>
    </author>
    <author>
      <name>Bob McGrew</name>
    </author>
    <author>
      <name>Borys Minaiev</name>
    </author>
    <author>
      <name>Botao Hao</name>
    </author>
    <author>
      <name>Bowen Baker</name>
    </author>
    <author>
      <name>Brandon Houghton</name>
    </author>
    <author>
      <name>Brandon McKinzie</name>
    </author>
    <author>
      <name>Brydon Eastman</name>
    </author>
    <author>
      <name>Camillo Lugaresi</name>
    </author>
    <author>
      <name>Cary Bassin</name>
    </author>
    <author>
      <name>Cary Hudson</name>
    </author>
    <author>
      <name>Chak Ming Li</name>
    </author>
    <author>
      <name>Charles de Bourcy</name>
    </author>
    <author>
      <name>Chelsea Voss</name>
    </author>
    <author>
      <name>Chen Shen</name>
    </author>
    <author>
      <name>Chong Zhang</name>
    </author>
    <author>
      <name>Chris Koch</name>
    </author>
    <author>
      <name>Chris Orsinger</name>
    </author>
    <author>
      <name>Christopher Hesse</name>
    </author>
    <author>
      <name>Claudia Fischer</name>
    </author>
    <author>
      <name>Clive Chan</name>
    </author>
    <author>
      <name>Dan Roberts</name>
    </author>
    <author>
      <name>Daniel Kappler</name>
    </author>
    <author>
      <name>Daniel Levy</name>
    </author>
    <author>
      <name>Daniel Selsam</name>
    </author>
    <author>
      <name>David Dohan</name>
    </author>
    <author>
      <name>David Farhi</name>
    </author>
    <author>
      <name>David Mely</name>
    </author>
    <author>
      <name>David Robinson</name>
    </author>
    <author>
      <name>Dimitris Tsipras</name>
    </author>
    <author>
      <name>Doug Li</name>
    </author>
    <author>
      <name>Dragos Oprica</name>
    </author>
    <author>
      <name>Eben Freeman</name>
    </author>
    <author>
      <name>Eddie Zhang</name>
    </author>
    <author>
      <name>Edmund Wong</name>
    </author>
    <author>
      <name>Elizabeth Proehl</name>
    </author>
    <author>
      <name>Enoch Cheung</name>
    </author>
    <author>
      <name>Eric Mitchell</name>
    </author>
    <author>
      <name>Eric Wallace</name>
    </author>
    <author>
      <name>Erik Ritter</name>
    </author>
    <author>
      <name>Evan Mays</name>
    </author>
    <author>
      <name>Fan Wang</name>
    </author>
    <author>
      <name>Felipe Petroski Such</name>
    </author>
    <author>
      <name>Filippo Raso</name>
    </author>
    <author>
      <name>Florencia Leoni</name>
    </author>
    <author>
      <name>Foivos Tsimpourlas</name>
    </author>
    <author>
      <name>Francis Song</name>
    </author>
    <author>
      <name>Fred von Lohmann</name>
    </author>
    <author>
      <name>Freddie Sulit</name>
    </author>
    <author>
      <name>Geoff Salmon</name>
    </author>
    <author>
      <name>Giambattista Parascandolo</name>
    </author>
    <author>
      <name>Gildas Chabot</name>
    </author>
    <author>
      <name>Grace Zhao</name>
    </author>
    <author>
      <name>Greg Brockman</name>
    </author>
    <author>
      <name>Guillaume Leclerc</name>
    </author>
    <author>
      <name>Hadi Salman</name>
    </author>
    <author>
      <name>Haiming Bao</name>
    </author>
    <author>
      <name>Hao Sheng</name>
    </author>
    <author>
      <name>Hart Andrin</name>
    </author>
    <author>
      <name>Hessam Bagherinezhad</name>
    </author>
    <author>
      <name>Hongyu Ren</name>
    </author>
    <author>
      <name>Hunter Lightman</name>
    </author>
    <author>
      <name>Hyung Won Chung</name>
    </author>
    <author>
      <name>Ian Kivlichan</name>
    </author>
    <author>
      <name>Ian O'Connell</name>
    </author>
    <author>
      <name>Ian Osband</name>
    </author>
    <author>
      <name>Ignasi Clavera Gilaberte</name>
    </author>
    <author>
      <name>Ilge Akkaya</name>
    </author>
    <author>
      <name>Ilya Kostrikov</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Irina Kofman</name>
    </author>
    <author>
      <name>Jakub Pachocki</name>
    </author>
    <author>
      <name>James Lennon</name>
    </author>
    <author>
      <name>Jason Wei</name>
    </author>
    <author>
      <name>Jean Harb</name>
    </author>
    <author>
      <name>Jerry Twore</name>
    </author>
    <author>
      <name>Jiacheng Feng</name>
    </author>
    <author>
      <name>Jiahui Yu</name>
    </author>
    <author>
      <name>Jiayi Weng</name>
    </author>
    <author>
      <name>Jie Tang</name>
    </author>
    <author>
      <name>Jieqi Yu</name>
    </author>
    <author>
      <name>Joaquin Quiñonero Candela</name>
    </author>
    <author>
      <name>Joe Palermo</name>
    </author>
    <author>
      <name>Joel Parish</name>
    </author>
    <author>
      <name>Johannes Heidecke</name>
    </author>
    <author>
      <name>John Hallman</name>
    </author>
    <author>
      <name>John Rizzo</name>
    </author>
    <author>
      <name>Jonathan Gordon</name>
    </author>
    <author>
      <name>Jonathan Uesato</name>
    </author>
    <author>
      <name>Jonathan Ward</name>
    </author>
    <author>
      <name>Joost Huizinga</name>
    </author>
    <author>
      <name>Julie Wang</name>
    </author>
    <author>
      <name>Kai Chen</name>
    </author>
    <author>
      <name>Kai Xiao</name>
    </author>
    <author>
      <name>Karan Singhal</name>
    </author>
    <author>
      <name>Karina Nguyen</name>
    </author>
    <author>
      <name>Karl Cobbe</name>
    </author>
    <author>
      <name>Katy Shi</name>
    </author>
    <author>
      <name>Kayla Wood</name>
    </author>
    <author>
      <name>Kendra Rimbach</name>
    </author>
    <author>
      <name>Keren Gu-Lemberg</name>
    </author>
    <author>
      <name>Kevin Liu</name>
    </author>
    <author>
      <name>Kevin Lu</name>
    </author>
    <author>
      <name>Kevin Stone</name>
    </author>
    <author>
      <name>Kevin Yu</name>
    </author>
    <author>
      <name>Lama Ahmad</name>
    </author>
    <author>
      <name>Lauren Yang</name>
    </author>
    <author>
      <name>Leo Liu</name>
    </author>
    <author>
      <name>Leon Maksin</name>
    </author>
    <author>
      <name>Leyton Ho</name>
    </author>
    <author>
      <name>Liam Fedus</name>
    </author>
    <author>
      <name>Lilian Weng</name>
    </author>
    <author>
      <name>Linden Li</name>
    </author>
    <author>
      <name>Lindsay McCallum</name>
    </author>
    <author>
      <name>Lindsey Held</name>
    </author>
    <author>
      <name>Lorenz Kuhn</name>
    </author>
    <author>
      <name>Lukas Kondraciuk</name>
    </author>
    <author>
      <name>Lukasz Kaiser</name>
    </author>
    <author>
      <name>Luke Metz</name>
    </author>
    <author>
      <name>Madelaine Boyd</name>
    </author>
    <author>
      <name>Maja Trebacz</name>
    </author>
    <author>
      <name>Manas Joglekar</name>
    </author>
    <author>
      <name>Mark Chen</name>
    </author>
    <author>
      <name>Marko Tintor</name>
    </author>
    <author>
      <name>Mason Meyer</name>
    </author>
    <author>
      <name>Matt Jones</name>
    </author>
    <author>
      <name>Matt Kaufer</name>
    </author>
    <author>
      <name>Max Schwarzer</name>
    </author>
    <author>
      <name>Meghan Shah</name>
    </author>
    <author>
      <name>Mehmet Yatbaz</name>
    </author>
    <author>
      <name>Melody Y. Guan</name>
    </author>
    <author>
      <name>Mengyuan Xu</name>
    </author>
    <author>
      <name>Mengyuan Yan</name>
    </author>
    <author>
      <name>Mia Glaese</name>
    </author>
    <author>
      <name>Mianna Chen</name>
    </author>
    <author>
      <name>Michael Lampe</name>
    </author>
    <author>
      <name>Michael Malek</name>
    </author>
    <author>
      <name>Michele Wang</name>
    </author>
    <author>
      <name>Michelle Fradin</name>
    </author>
    <author>
      <name>Mike McClay</name>
    </author>
    <author>
      <name>Mikhail Pavlov</name>
    </author>
    <author>
      <name>Miles Wang</name>
    </author>
    <author>
      <name>Mingxuan Wang</name>
    </author>
    <author>
      <name>Mira Murati</name>
    </author>
    <author>
      <name>Mo Bavarian</name>
    </author>
    <author>
      <name>Mostafa Rohaninejad</name>
    </author>
    <author>
      <name>Nat McAleese</name>
    </author>
    <author>
      <name>Neil Chowdhury</name>
    </author>
    <author>
      <name>Neil Chowdhury</name>
    </author>
    <author>
      <name>Nick Ryder</name>
    </author>
    <author>
      <name>Nikolas Tezak</name>
    </author>
    <author>
      <name>Noam Brown</name>
    </author>
    <author>
      <name>Ofir Nachum</name>
    </author>
    <author>
      <name>Oleg Boiko</name>
    </author>
    <author>
      <name>Oleg Murk</name>
    </author>
    <author>
      <name>Olivia Watkins</name>
    </author>
    <author>
      <name>Patrick Chao</name>
    </author>
    <author>
      <name>Paul Ashbourne</name>
    </author>
    <author>
      <name>Pavel Izmailov</name>
    </author>
    <author>
      <name>Peter Zhokhov</name>
    </author>
    <author>
      <name>Rachel Dias</name>
    </author>
    <author>
      <name>Rahul Arora</name>
    </author>
    <author>
      <name>Randall Lin</name>
    </author>
    <author>
      <name>Rapha Gontijo Lopes</name>
    </author>
    <author>
      <name>Raz Gaon</name>
    </author>
    <author>
      <name>Reah Miyara</name>
    </author>
    <author>
      <name>Reimar Leike</name>
    </author>
    <author>
      <name>Renny Hwang</name>
    </author>
    <author>
      <name>Rhythm Garg</name>
    </author>
    <author>
      <name>Robin Brown</name>
    </author>
    <author>
      <name>Roshan James</name>
    </author>
    <author>
      <name>Rui Shu</name>
    </author>
    <author>
      <name>Ryan Cheu</name>
    </author>
    <author>
      <name>Ryan Greene</name>
    </author>
    <author>
      <name>Saachi Jain</name>
    </author>
    <author>
      <name>Sam Altman</name>
    </author>
    <author>
      <name>Sam Toizer</name>
    </author>
    <author>
      <name>Sam Toyer</name>
    </author>
    <author>
      <name>Samuel Miserendino</name>
    </author>
    <author>
      <name>Sandhini Agarwal</name>
    </author>
    <author>
      <name>Santiago Hernandez</name>
    </author>
    <author>
      <name>Sasha Baker</name>
    </author>
    <author>
      <name>Scott McKinney</name>
    </author>
    <author>
      <name>Scottie Yan</name>
    </author>
    <author>
      <name>Shengjia Zhao</name>
    </author>
    <author>
      <name>Shengli Hu</name>
    </author>
    <author>
      <name>Shibani Santurkar</name>
    </author>
    <author>
      <name>Shraman Ray Chaudhuri</name>
    </author>
    <author>
      <name>Shuyuan Zhang</name>
    </author>
    <author>
      <name>Siyuan Fu</name>
    </author>
    <author>
      <name>Spencer Papay</name>
    </author>
    <author>
      <name>Steph Lin</name>
    </author>
    <author>
      <name>Suchir Balaji</name>
    </author>
    <author>
      <name>Suvansh Sanjeev</name>
    </author>
    <author>
      <name>Szymon Sidor</name>
    </author>
    <author>
      <name>Tal Broda</name>
    </author>
    <author>
      <name>Aidan Clark</name>
    </author>
    <author>
      <name>Tao Wang</name>
    </author>
    <author>
      <name>Taylor Gordon</name>
    </author>
    <author>
      <name>Ted Sanders</name>
    </author>
    <author>
      <name>Tejal Patwardhan</name>
    </author>
    <author>
      <name>Thibault Sottiaux</name>
    </author>
    <author>
      <name>Thomas Degry</name>
    </author>
    <author>
      <name>Thomas Dimson</name>
    </author>
    <author>
      <name>Tianhao Zheng</name>
    </author>
    <author>
      <name>Timur Garipov</name>
    </author>
    <author>
      <name>Tom Stasi</name>
    </author>
    <author>
      <name>Trapit Bansal</name>
    </author>
    <author>
      <name>Trevor Creech</name>
    </author>
    <author>
      <name>Troy Peterson</name>
    </author>
    <author>
      <name>Tyna Eloundou</name>
    </author>
    <author>
      <name>Valerie Qi</name>
    </author>
    <author>
      <name>Vineet Kosaraju</name>
    </author>
    <author>
      <name>Vinnie Monaco</name>
    </author>
    <author>
      <name>Vitchyr Pong</name>
    </author>
    <author>
      <name>Vlad Fomenko</name>
    </author>
    <author>
      <name>Weiyi Zheng</name>
    </author>
    <author>
      <name>Wenda Zhou</name>
    </author>
    <author>
      <name>Wes McCabe</name>
    </author>
    <author>
      <name>Wojciech Zaremba</name>
    </author>
    <author>
      <name>Yann Dubois</name>
    </author>
    <author>
      <name>Yinghai Lu</name>
    </author>
    <author>
      <name>Yining Chen</name>
    </author>
    <author>
      <name>Young Cha</name>
    </author>
    <author>
      <name>Yu Bai</name>
    </author>
    <author>
      <name>Yuchen He</name>
    </author>
    <author>
      <name>Yuchen Zhang</name>
    </author>
    <author>
      <name>Yunyun Wang</name>
    </author>
    <author>
      <name>Zheng Shao</name>
    </author>
    <author>
      <name>Zhuohan Li</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.21276v1</id>
    <title>GPT-4o System Card</title>
    <updated>2024-10-25T17:43:01Z</updated>
    <link href="https://arxiv.org/abs/2410.21276v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2410.21276v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>GPT-4o is an autoregressive omni model that accepts as input any combination of text, audio, image, and video, and generates any combination of text, audio, and image outputs. It's trained end-to-end across text, vision, and audio, meaning all inputs and outputs are processed by the same neural network. GPT-4o can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time in conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50\% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models. In line with our commitment to building AI safely and consistent with our voluntary commitments to the White House, we are sharing the GPT-4o System Card, which includes our Preparedness Framework evaluations. In this System Card, we provide a detailed look at GPT-4o's capabilities, limitations, and safety evaluations across multiple categories, focusing on speech-to-speech while also evaluating text and image capabilities, and measures we've implemented to ensure the model is safe and aligned. We also include third-party assessments on dangerous capabilities, as well as discussion of potential societal impacts of GPT-4o's text and vision capabilities.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <published>2024-10-25T17:43:01Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name> OpenAI</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name> :</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Aaron Hurst</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Adam Lerer</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Adam P. Goucher</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Adam Perelman</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Aditya Ramesh</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Aidan Clark</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>AJ Ostrow</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Akila Welihinda</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Alan Hayes</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Alec Radford</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Aleksander Mądry</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Alex Baker-Whitcomb</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Alex Beutel</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Alex Borzunov</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Alex Carney</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Alex Chow</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Alex Kirillov</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Alex Nichol</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Alex Paino</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Alex Renzin</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Alex Tachard Passos</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Alexander Kirillov</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Alexi Christakis</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Alexis Conneau</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Ali Kamali</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Allan Jabri</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Allison Moyer</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Allison Tam</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Amadou Crookes</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Amin Tootoochian</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Amin Tootoonchian</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Ananya Kumar</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Andrea Vallone</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Andrej Karpathy</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Andrew Braunstein</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Andrew Cann</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Andrew Codispoti</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Andrew Galu</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Andrew Kondrich</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Andrew Tulloch</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Andrey Mishchenko</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Angela Baek</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Angela Jiang</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Antoine Pelisse</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Antonia Woodford</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Anuj Gosalia</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Arka Dhar</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Ashley Pantuliano</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Avi Nayak</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Avital Oliver</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Barret Zoph</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Behrooz Ghorbani</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Ben Leimberger</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Ben Rossen</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Ben Sokolowsky</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Ben Wang</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Benjamin Zweig</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Beth Hoover</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Blake Samic</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Bob McGrew</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Bobby Spero</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Bogo Giertler</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Bowen Cheng</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Brad Lightcap</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Brandon Walkin</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Brendan Quinn</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Brian Guarraci</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Brian Hsu</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Bright Kellogg</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Brydon Eastman</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Camillo Lugaresi</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Carroll Wainwright</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Cary Bassin</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Cary Hudson</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Casey Chu</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Chad Nelson</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Chak Li</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Chan Jun Shern</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Channing Conger</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Charlotte Barette</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Chelsea Voss</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Chen Ding</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Cheng Lu</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Chong Zhang</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Chris Beaumont</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Chris Hallacy</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Chris Koch</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Christian Gibson</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Christina Kim</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Christine Choi</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Christine McLeavey</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Christopher Hesse</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Claudia Fischer</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Clemens Winter</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Coley Czarnecki</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Colin Jarvis</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Colin Wei</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Constantin Koumouzelis</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Dane Sherburn</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Daniel Kappler</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Daniel Levin</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Daniel Levy</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>David Carr</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>David Farhi</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>David Mely</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>David Robinson</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>David Sasaki</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Denny Jin</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Dev Valladares</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Dimitris Tsipras</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Doug Li</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Duc Phong Nguyen</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Duncan Findlay</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Edede Oiwoh</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Edmund Wong</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Ehsan Asdar</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Elizabeth Proehl</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Elizabeth Yang</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Eric Antonow</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Eric Kramer</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Eric Peterson</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Eric Sigler</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Eric Wallace</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Eugene Brevdo</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Evan Mays</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Farzad Khorasani</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Felipe Petroski Such</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Filippo Raso</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Francis Zhang</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Fred von Lohmann</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Freddie Sulit</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Gabriel Goh</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Gene Oden</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Geoff Salmon</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Giulio Starace</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Greg Brockman</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Hadi Salman</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Haiming Bao</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Haitang Hu</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Hannah Wong</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Haoyu Wang</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Heather Schmidt</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Heather Whitney</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Heewoo Jun</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Hendrik Kirchner</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Henrique Ponde de Oliveira Pinto</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Hongyu Ren</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Huiwen Chang</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Hyung Won Chung</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Ian Kivlichan</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Ian O'Connell</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Ian O'Connell</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Ian Osband</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Ian Silber</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Ian Sohl</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Ibrahim Okuyucu</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Ikai Lan</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Ilya Kostrikov</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Ilya Sutskever</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Ingmar Kanitscheider</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Ishaan Gulrajani</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Jacob Coxon</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Jacob Menick</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Jakub Pachocki</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>James Aung</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>James Betker</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>James Crooks</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>James Lennon</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Jamie Kiros</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Jan Leike</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Jane Park</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Jason Kwon</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Jason Phang</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Jason Teplitz</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Jason Wei</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Jason Wolfe</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Jay Chen</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Jeff Harris</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Jenia Varavva</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Jessica Gan Lee</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Jessica Shieh</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Ji Lin</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Jiahui Yu</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Jiayi Weng</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Jie Tang</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Jieqi Yu</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Joanne Jang</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Joaquin Quinonero Candela</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Joe Beutler</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Joe Landers</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Joel Parish</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Johannes Heidecke</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>John Schulman</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Jonathan Lachman</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Jonathan McKay</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Jonathan Uesato</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Jonathan Ward</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Jong Wook Kim</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Joost Huizinga</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Jordan Sitkin</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Jos Kraaijeveld</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Josh Gross</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Josh Kaplan</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Josh Snyder</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Joshua Achiam</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Joy Jiao</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Joyce Lee</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Juntang Zhuang</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Justyn Harriman</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Kai Fricke</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Kai Hayashi</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Karan Singhal</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Katy Shi</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Kavin Karthik</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Kayla Wood</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Kendra Rimbach</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Kenny Hsu</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Kenny Nguyen</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Keren Gu-Lemberg</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Kevin Button</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Kevin Liu</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Kiel Howe</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Krithika Muthukumar</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Kyle Luther</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Lama Ahmad</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Larry Kai</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Lauren Itow</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Lauren Workman</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Leher Pathak</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Leo Chen</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Li Jing</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Lia Guy</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Liam Fedus</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Liang Zhou</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Lien Mamitsuka</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Lilian Weng</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Lindsay McCallum</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Lindsey Held</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Long Ouyang</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Louis Feuvrier</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Lu Zhang</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Lukas Kondraciuk</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Lukasz Kaiser</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Luke Hewitt</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Luke Metz</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Lyric Doshi</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Mada Aflak</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Maddie Simens</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Madelaine Boyd</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Madeleine Thompson</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Marat Dukhan</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Mark Chen</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Mark Gray</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Mark Hudnall</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Marvin Zhang</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Marwan Aljubeh</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Mateusz Litwin</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Matthew Zeng</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Max Johnson</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Maya Shetty</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Mayank Gupta</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Meghan Shah</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Mehmet Yatbaz</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Meng Jia Yang</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Mengchao Zhong</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Mia Glaese</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Mianna Chen</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Michael Janner</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Michael Lampe</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Michael Petrov</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Michael Wu</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Michele Wang</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Michelle Fradin</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Michelle Pokrass</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Miguel Castro</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Miguel Oom Temudo de Castro</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Mikhail Pavlov</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Miles Brundage</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Miles Wang</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Minal Khan</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Mira Murati</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Mo Bavarian</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Molly Lin</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Murat Yesildal</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Nacho Soto</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Natalia Gimelshein</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Natalie Cone</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Natalie Staudacher</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Natalie Summers</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Natan LaFontaine</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Neil Chowdhury</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Nick Ryder</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Nick Stathas</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Nick Turley</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Nik Tezak</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Niko Felix</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Nithanth Kudige</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Nitish Keskar</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Noah Deutsch</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Noel Bundick</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Nora Puckett</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Ofir Nachum</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Ola Okelola</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Oleg Boiko</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Oleg Murk</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Oliver Jaffe</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Olivia Watkins</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Olivier Godement</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Owen Campbell-Moore</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Patrick Chao</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Paul McMillan</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Pavel Belov</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Peng Su</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Peter Bak</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Peter Bakkum</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Peter Deng</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Peter Dolan</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Peter Hoeschele</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Peter Welinder</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Phil Tillet</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Philip Pronin</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Philippe Tillet</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Prafulla Dhariwal</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Qiming Yuan</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Rachel Dias</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Rachel Lim</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Rahul Arora</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Rajan Troll</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Randall Lin</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Rapha Gontijo Lopes</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Raul Puri</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Reah Miyara</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Reimar Leike</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Renaud Gaubert</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Reza Zamani</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Ricky Wang</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Rob Donnelly</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Rob Honsby</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Rocky Smith</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Rohan Sahai</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Rohit Ramchandani</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Romain Huet</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Rory Carmichael</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Rowan Zellers</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Roy Chen</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Ruby Chen</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Ruslan Nigmatullin</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Ryan Cheu</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Saachi Jain</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Sam Altman</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Sam Schoenholz</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Sam Toizer</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Samuel Miserendino</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Sandhini Agarwal</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Sara Culver</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Scott Ethersmith</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Scott Gray</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Sean Grove</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Sean Metzger</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Shamez Hermani</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Shantanu Jain</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Shengjia Zhao</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Sherwin Wu</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Shino Jomoto</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name>Shirong Wu</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name> Shuaiqi</name>
      <arxiv:affiliation>Tony</arxiv:affiliation>
    </author>
    <author>
      <name> Xia</name>
    </author>
    <author>
      <name>Sonia Phene</name>
    </author>
    <author>
      <name>Spencer Papay</name>
    </author>
    <author>
      <name>Srinivas Narayanan</name>
    </author>
    <author>
      <name>Steve Coffey</name>
    </author>
    <author>
      <name>Steve Lee</name>
    </author>
    <author>
      <name>Stewart Hall</name>
    </author>
    <author>
      <name>Suchir Balaji</name>
    </author>
    <author>
      <name>Tal Broda</name>
    </author>
    <author>
      <name>Tal Stramer</name>
    </author>
    <author>
      <name>Tao Xu</name>
    </author>
    <author>
      <name>Tarun Gogineni</name>
    </author>
    <author>
      <name>Taya Christianson</name>
    </author>
    <author>
      <name>Ted Sanders</name>
    </author>
    <author>
      <name>Tejal Patwardhan</name>
    </author>
    <author>
      <name>Thomas Cunninghman</name>
    </author>
    <author>
      <name>Thomas Degry</name>
    </author>
    <author>
      <name>Thomas Dimson</name>
    </author>
    <author>
      <name>Thomas Raoux</name>
    </author>
    <author>
      <name>Thomas Shadwell</name>
    </author>
    <author>
      <name>Tianhao Zheng</name>
    </author>
    <author>
      <name>Todd Underwood</name>
    </author>
    <author>
      <name>Todor Markov</name>
    </author>
    <author>
      <name>Toki Sherbakov</name>
    </author>
    <author>
      <name>Tom Rubin</name>
    </author>
    <author>
      <name>Tom Stasi</name>
    </author>
    <author>
      <name>Tomer Kaftan</name>
    </author>
    <author>
      <name>Tristan Heywood</name>
    </author>
    <author>
      <name>Troy Peterson</name>
    </author>
    <author>
      <name>Tyce Walters</name>
    </author>
    <author>
      <name>Tyna Eloundou</name>
    </author>
    <author>
      <name>Valerie Qi</name>
    </author>
    <author>
      <name>Veit Moeller</name>
    </author>
    <author>
      <name>Vinnie Monaco</name>
    </author>
    <author>
      <name>Vishal Kuo</name>
    </author>
    <author>
      <name>Vlad Fomenko</name>
    </author>
    <author>
      <name>Wayne Chang</name>
    </author>
    <author>
      <name>Weiyi Zheng</name>
    </author>
    <author>
      <name>Wenda Zhou</name>
    </author>
    <author>
      <name>Wesam Manassra</name>
    </author>
    <author>
      <name>Will Sheu</name>
    </author>
    <author>
      <name>Wojciech Zaremba</name>
    </author>
    <author>
      <name>Yash Patil</name>
    </author>
    <author>
      <name>Yilei Qian</name>
    </author>
    <author>
      <name>Yongjik Kim</name>
    </author>
    <author>
      <name>Youlong Cheng</name>
    </author>
    <author>
      <name>Yu Zhang</name>
    </author>
    <author>
      <name>Yuchen He</name>
    </author>
    <author>
      <name>Yuchen Zhang</name>
    </author>
    <author>
      <name>Yujia Jin</name>
    </author>
    <author>
      <name>Yunxing Dai</name>
    </author>
    <author>
      <name>Yury Malkov</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.04093v1</id>
    <title>Scaling and evaluating sparse autoencoders</title>
    <updated>2024-06-06T14:10:12Z</updated>
    <link href="https://arxiv.org/abs/2406.04093v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2406.04093v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Sparse autoencoders provide a promising unsupervised approach for extracting interpretable features from a language model by reconstructing activations from a sparse bottleneck layer. Since language models learn many concepts, autoencoders need to be very large to recover all relevant features. However, studying the properties of autoencoder scaling is difficult due to the need to balance reconstruction and sparsity objectives and the presence of dead latents. We propose using k-sparse autoencoders [Makhzani and Frey, 2013] to directly control sparsity, simplifying tuning and improving the reconstruction-sparsity frontier. Additionally, we find modifications that result in few dead latents, even at the largest scales we tried. Using these techniques, we find clean scaling laws with respect to autoencoder size and sparsity. We also introduce several new metrics for evaluating feature quality based on the recovery of hypothesized features, the explainability of activation patterns, and the sparsity of downstream effects. These metrics all generally improve with autoencoder size. To demonstrate the scalability of our approach, we train a 16 million latent autoencoder on GPT-4 activations for 40 billion tokens. We release training code and autoencoders for open-source models, as well as a visualizer.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2024-06-06T14:10:12Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Leo Gao</name>
    </author>
    <author>
      <name>Tom Dupré la Tour</name>
    </author>
    <author>
      <name>Henk Tillman</name>
    </author>
    <author>
      <name>Gabriel Goh</name>
    </author>
    <author>
      <name>Rajan Troll</name>
    </author>
    <author>
      <name>Alec Radford</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Jan Leike</name>
    </author>
    <author>
      <name>Jeffrey Wu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.09390v1</id>
    <title>Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision</title>
    <updated>2023-12-14T23:07:33Z</updated>
    <link href="https://arxiv.org/abs/2312.09390v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2312.09390v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Widely used alignment techniques, such as reinforcement learning from human feedback (RLHF), rely on the ability of humans to supervise model behavior - for example, to evaluate whether a model faithfully followed instructions or generated safe outputs. However, future superhuman models will behave in complex ways too difficult for humans to reliably evaluate; humans will only be able to weakly supervise superhuman models. We study an analogy to this problem: can weak model supervision elicit the full capabilities of a much stronger model? We test this using a range of pretrained language models in the GPT-4 family on natural language processing (NLP), chess, and reward modeling tasks. We find that when we naively finetune strong pretrained models on labels generated by a weak model, they consistently perform better than their weak supervisors, a phenomenon we call weak-to-strong generalization. However, we are still far from recovering the full capabilities of strong models with naive finetuning alone, suggesting that techniques like RLHF may scale poorly to superhuman models without further work. We find that simple methods can often significantly improve weak-to-strong generalization: for example, when finetuning GPT-4 with a GPT-2-level supervisor and an auxiliary confidence loss, we can recover close to GPT-3.5-level performance on NLP tasks. Our results suggest that it is feasible to make empirical progress today on a fundamental challenge of aligning superhuman models.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2023-12-14T23:07:33Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Collin Burns</name>
    </author>
    <author>
      <name>Pavel Izmailov</name>
    </author>
    <author>
      <name>Jan Hendrik Kirchner</name>
    </author>
    <author>
      <name>Bowen Baker</name>
    </author>
    <author>
      <name>Leo Gao</name>
    </author>
    <author>
      <name>Leopold Aschenbrenner</name>
    </author>
    <author>
      <name>Yining Chen</name>
    </author>
    <author>
      <name>Adrien Ecoffet</name>
    </author>
    <author>
      <name>Manas Joglekar</name>
    </author>
    <author>
      <name>Jan Leike</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Jeff Wu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.20050v1</id>
    <title>Let's Verify Step by Step</title>
    <updated>2023-05-31T17:24:00Z</updated>
    <link href="https://arxiv.org/abs/2305.20050v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2305.20050v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>In recent years, large language models have greatly improved in their ability to perform complex multi-step reasoning. However, even state-of-the-art models still regularly produce logical mistakes. To train more reliable models, we can turn either to outcome supervision, which provides feedback for a final result, or process supervision, which provides feedback for each intermediate reasoning step. Given the importance of training reliable models, and given the high cost of human feedback, it is important to carefully compare the both methods. Recent work has already begun this comparison, but many questions still remain. We conduct our own investigation, finding that process supervision significantly outperforms outcome supervision for training models to solve problems from the challenging MATH dataset. Our process-supervised model solves 78% of problems from a representative subset of the MATH test set. Additionally, we show that active learning significantly improves the efficacy of process supervision. To support related research, we also release PRM800K, the complete dataset of 800,000 step-level human feedback labels used to train our best reward model.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2023-05-31T17:24:00Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Hunter Lightman</name>
    </author>
    <author>
      <name>Vineet Kosaraju</name>
    </author>
    <author>
      <name>Yura Burda</name>
    </author>
    <author>
      <name>Harri Edwards</name>
    </author>
    <author>
      <name>Bowen Baker</name>
    </author>
    <author>
      <name>Teddy Lee</name>
    </author>
    <author>
      <name>Jan Leike</name>
    </author>
    <author>
      <name>John Schulman</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Karl Cobbe</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.08774v6</id>
    <title>GPT-4 Technical Report</title>
    <updated>2024-03-04T06:01:33Z</updated>
    <link href="https://arxiv.org/abs/2303.08774v6" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2303.08774v6" rel="related" type="application/pdf" title="pdf"/>
    <summary>We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2023-03-15T17:15:04Z</published>
    <arxiv:comment>100 pages; updated authors list; fixed author names and added citation</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name> OpenAI</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Josh Achiam</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Steven Adler</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Sandhini Agarwal</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Lama Ahmad</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Ilge Akkaya</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Florencia Leoni Aleman</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Diogo Almeida</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Janko Altenschmidt</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Sam Altman</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Shyamal Anadkat</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Red Avila</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Igor Babuschkin</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Suchir Balaji</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Valerie Balcom</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Paul Baltescu</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Haiming Bao</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Mohammad Bavarian</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Jeff Belgum</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Irwan Bello</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Jake Berdine</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Gabriel Bernadett-Shapiro</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Christopher Berner</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Lenny Bogdonoff</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Oleg Boiko</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Madelaine Boyd</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Anna-Luisa Brakman</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Greg Brockman</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Tim Brooks</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Miles Brundage</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Kevin Button</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Trevor Cai</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Rosie Campbell</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Andrew Cann</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Brittany Carey</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Chelsea Carlson</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Rory Carmichael</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Brooke Chan</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Che Chang</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Fotis Chantzis</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Derek Chen</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Sully Chen</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Ruby Chen</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Jason Chen</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Mark Chen</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Ben Chess</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Chester Cho</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Casey Chu</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Hyung Won Chung</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Dave Cummings</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Jeremiah Currier</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Yunxing Dai</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Cory Decareaux</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Thomas Degry</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Noah Deutsch</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Damien Deville</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Arka Dhar</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>David Dohan</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Steve Dowling</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Sheila Dunning</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Adrien Ecoffet</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Atty Eleti</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Tyna Eloundou</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>David Farhi</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Liam Fedus</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Niko Felix</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Simón Posada Fishman</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Juston Forte</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Isabella Fulford</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Leo Gao</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Elie Georges</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Christian Gibson</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Vik Goel</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Tarun Gogineni</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Gabriel Goh</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Rapha Gontijo-Lopes</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Jonathan Gordon</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Morgan Grafstein</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Scott Gray</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Ryan Greene</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Joshua Gross</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Shixiang Shane Gu</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Yufei Guo</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Chris Hallacy</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Jesse Han</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Jeff Harris</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Yuchen He</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Mike Heaton</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Johannes Heidecke</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Chris Hesse</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Alan Hickey</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Wade Hickey</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Peter Hoeschele</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Brandon Houghton</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Kenny Hsu</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Shengli Hu</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Xin Hu</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Joost Huizinga</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Shantanu Jain</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Shawn Jain</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Joanne Jang</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Angela Jiang</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Roger Jiang</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Haozhun Jin</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Denny Jin</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Shino Jomoto</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Billie Jonn</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Heewoo Jun</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Tomer Kaftan</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Łukasz Kaiser</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Ali Kamali</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Ingmar Kanitscheider</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Nitish Shirish Keskar</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Tabarak Khan</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Logan Kilpatrick</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Jong Wook Kim</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Christina Kim</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Yongjik Kim</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Jan Hendrik Kirchner</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Jamie Kiros</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Matt Knight</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Daniel Kokotajlo</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Łukasz Kondraciuk</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Andrew Kondrich</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Aris Konstantinidis</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Kyle Kosic</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Gretchen Krueger</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Vishal Kuo</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Michael Lampe</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Ikai Lan</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Teddy Lee</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Jan Leike</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Jade Leung</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Daniel Levy</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Chak Ming Li</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Rachel Lim</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Molly Lin</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Stephanie Lin</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Mateusz Litwin</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Theresa Lopez</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Ryan Lowe</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Patricia Lue</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Anna Makanju</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Kim Malfacini</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Sam Manning</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Todor Markov</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Yaniv Markovski</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Bianca Martin</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Katie Mayer</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Andrew Mayne</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Bob McGrew</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Scott Mayer McKinney</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Christine McLeavey</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Paul McMillan</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Jake McNeil</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>David Medina</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Aalok Mehta</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Jacob Menick</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Luke Metz</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Andrey Mishchenko</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Pamela Mishkin</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Vinnie Monaco</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Evan Morikawa</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Daniel Mossing</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Tong Mu</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Mira Murati</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Oleg Murk</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>David Mély</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Ashvin Nair</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Reiichiro Nakano</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Rajeev Nayak</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Arvind Neelakantan</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Richard Ngo</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Hyeonwoo Noh</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Long Ouyang</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Cullen O'Keefe</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Jakub Pachocki</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Alex Paino</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Joe Palermo</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Ashley Pantuliano</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Giambattista Parascandolo</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Joel Parish</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Emy Parparita</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Alex Passos</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Mikhail Pavlov</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Andrew Peng</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Adam Perelman</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Filipe de Avila Belbute Peres</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Michael Petrov</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name>Henrique Ponde de Oliveira Pinto</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name> Michael</name>
      <arxiv:affiliation>Rai</arxiv:affiliation>
    </author>
    <author>
      <name> Pokorny</name>
    </author>
    <author>
      <name>Michelle Pokrass</name>
    </author>
    <author>
      <name>Vitchyr H. Pong</name>
    </author>
    <author>
      <name>Tolly Powell</name>
    </author>
    <author>
      <name>Alethea Power</name>
    </author>
    <author>
      <name>Boris Power</name>
    </author>
    <author>
      <name>Elizabeth Proehl</name>
    </author>
    <author>
      <name>Raul Puri</name>
    </author>
    <author>
      <name>Alec Radford</name>
    </author>
    <author>
      <name>Jack Rae</name>
    </author>
    <author>
      <name>Aditya Ramesh</name>
    </author>
    <author>
      <name>Cameron Raymond</name>
    </author>
    <author>
      <name>Francis Real</name>
    </author>
    <author>
      <name>Kendra Rimbach</name>
    </author>
    <author>
      <name>Carl Ross</name>
    </author>
    <author>
      <name>Bob Rotsted</name>
    </author>
    <author>
      <name>Henri Roussez</name>
    </author>
    <author>
      <name>Nick Ryder</name>
    </author>
    <author>
      <name>Mario Saltarelli</name>
    </author>
    <author>
      <name>Ted Sanders</name>
    </author>
    <author>
      <name>Shibani Santurkar</name>
    </author>
    <author>
      <name>Girish Sastry</name>
    </author>
    <author>
      <name>Heather Schmidt</name>
    </author>
    <author>
      <name>David Schnurr</name>
    </author>
    <author>
      <name>John Schulman</name>
    </author>
    <author>
      <name>Daniel Selsam</name>
    </author>
    <author>
      <name>Kyla Sheppard</name>
    </author>
    <author>
      <name>Toki Sherbakov</name>
    </author>
    <author>
      <name>Jessica Shieh</name>
    </author>
    <author>
      <name>Sarah Shoker</name>
    </author>
    <author>
      <name>Pranav Shyam</name>
    </author>
    <author>
      <name>Szymon Sidor</name>
    </author>
    <author>
      <name>Eric Sigler</name>
    </author>
    <author>
      <name>Maddie Simens</name>
    </author>
    <author>
      <name>Jordan Sitkin</name>
    </author>
    <author>
      <name>Katarina Slama</name>
    </author>
    <author>
      <name>Ian Sohl</name>
    </author>
    <author>
      <name>Benjamin Sokolowsky</name>
    </author>
    <author>
      <name>Yang Song</name>
    </author>
    <author>
      <name>Natalie Staudacher</name>
    </author>
    <author>
      <name>Felipe Petroski Such</name>
    </author>
    <author>
      <name>Natalie Summers</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Jie Tang</name>
    </author>
    <author>
      <name>Nikolas Tezak</name>
    </author>
    <author>
      <name>Madeleine B. Thompson</name>
    </author>
    <author>
      <name>Phil Tillet</name>
    </author>
    <author>
      <name>Amin Tootoonchian</name>
    </author>
    <author>
      <name>Elizabeth Tseng</name>
    </author>
    <author>
      <name>Preston Tuggle</name>
    </author>
    <author>
      <name>Nick Turley</name>
    </author>
    <author>
      <name>Jerry Tworek</name>
    </author>
    <author>
      <name>Juan Felipe Cerón Uribe</name>
    </author>
    <author>
      <name>Andrea Vallone</name>
    </author>
    <author>
      <name>Arun Vijayvergiya</name>
    </author>
    <author>
      <name>Chelsea Voss</name>
    </author>
    <author>
      <name>Carroll Wainwright</name>
    </author>
    <author>
      <name>Justin Jay Wang</name>
    </author>
    <author>
      <name>Alvin Wang</name>
    </author>
    <author>
      <name>Ben Wang</name>
    </author>
    <author>
      <name>Jonathan Ward</name>
    </author>
    <author>
      <name>Jason Wei</name>
    </author>
    <author>
      <name>CJ Weinmann</name>
    </author>
    <author>
      <name>Akila Welihinda</name>
    </author>
    <author>
      <name>Peter Welinder</name>
    </author>
    <author>
      <name>Jiayi Weng</name>
    </author>
    <author>
      <name>Lilian Weng</name>
    </author>
    <author>
      <name>Matt Wiethoff</name>
    </author>
    <author>
      <name>Dave Willner</name>
    </author>
    <author>
      <name>Clemens Winter</name>
    </author>
    <author>
      <name>Samuel Wolrich</name>
    </author>
    <author>
      <name>Hannah Wong</name>
    </author>
    <author>
      <name>Lauren Workman</name>
    </author>
    <author>
      <name>Sherwin Wu</name>
    </author>
    <author>
      <name>Jeff Wu</name>
    </author>
    <author>
      <name>Michael Wu</name>
    </author>
    <author>
      <name>Kai Xiao</name>
    </author>
    <author>
      <name>Tao Xu</name>
    </author>
    <author>
      <name>Sarah Yoo</name>
    </author>
    <author>
      <name>Kevin Yu</name>
    </author>
    <author>
      <name>Qiming Yuan</name>
    </author>
    <author>
      <name>Wojciech Zaremba</name>
    </author>
    <author>
      <name>Rowan Zellers</name>
    </author>
    <author>
      <name>Chong Zhang</name>
    </author>
    <author>
      <name>Marvin Zhang</name>
    </author>
    <author>
      <name>Shengjia Zhao</name>
    </author>
    <author>
      <name>Tianhao Zheng</name>
    </author>
    <author>
      <name>Juntang Zhuang</name>
    </author>
    <author>
      <name>William Zhuk</name>
    </author>
    <author>
      <name>Barret Zoph</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.01469v2</id>
    <title>Consistency Models</title>
    <updated>2023-05-31T06:17:10Z</updated>
    <link href="https://arxiv.org/abs/2303.01469v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2303.01469v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>Diffusion models have significantly advanced the fields of image, audio, and video generation, but they depend on an iterative sampling process that causes slow generation. To overcome this limitation, we propose consistency models, a new family of models that generate high quality samples by directly mapping noise to data. They support fast one-step generation by design, while still allowing multistep sampling to trade compute for sample quality. They also support zero-shot data editing, such as image inpainting, colorization, and super-resolution, without requiring explicit training on these tasks. Consistency models can be trained either by distilling pre-trained diffusion models, or as standalone generative models altogether. Through extensive experiments, we demonstrate that they outperform existing distillation techniques for diffusion models in one- and few-step sampling, achieving the new state-of-the-art FID of 3.55 on CIFAR-10 and 6.20 on ImageNet 64x64 for one-step generation. When trained in isolation, consistency models become a new family of generative models that can outperform existing one-step, non-adversarial generative models on standard benchmarks such as CIFAR-10, ImageNet 64x64 and LSUN 256x256.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2023-03-02T18:30:16Z</published>
    <arxiv:comment>ICML 2023</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Yang Song</name>
    </author>
    <author>
      <name>Prafulla Dhariwal</name>
    </author>
    <author>
      <name>Mark Chen</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.04356v1</id>
    <title>Robust Speech Recognition via Large-Scale Weak Supervision</title>
    <updated>2022-12-06T18:46:04Z</updated>
    <link href="https://arxiv.org/abs/2212.04356v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2212.04356v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.</summary>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <published>2022-12-06T18:46:04Z</published>
    <arxiv:primary_category term="eess.AS"/>
    <author>
      <name>Alec Radford</name>
    </author>
    <author>
      <name>Jong Wook Kim</name>
    </author>
    <author>
      <name>Tao Xu</name>
    </author>
    <author>
      <name>Greg Brockman</name>
    </author>
    <author>
      <name>Christine McLeavey</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.01344v1</id>
    <title>Formal Mathematics Statement Curriculum Learning</title>
    <updated>2022-02-03T00:17:00Z</updated>
    <link href="https://arxiv.org/abs/2202.01344v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2202.01344v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We explore the use of expert iteration in the context of language modeling applied to formal mathematics. We show that at same compute budget, expert iteration, by which we mean proof search interleaved with learning, dramatically outperforms proof search only. We also observe that when applied to a collection of formal statements of sufficiently varied difficulty, expert iteration is capable of finding and solving a curriculum of increasingly difficult problems, without the need for associated ground-truth proofs. Finally, by applying this expert iteration to a manually curated set of problem statements, we achieve state-of-the-art on the miniF2F benchmark, automatically solving multiple challenging problems drawn from high school olympiads.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2022-02-03T00:17:00Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Stanislas Polu</name>
    </author>
    <author>
      <name>Jesse Michael Han</name>
    </author>
    <author>
      <name>Kunhao Zheng</name>
    </author>
    <author>
      <name>Mantas Baksys</name>
    </author>
    <author>
      <name>Igor Babuschkin</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.10741v3</id>
    <title>GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models</title>
    <updated>2022-03-08T18:18:49Z</updated>
    <link href="https://arxiv.org/abs/2112.10741v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2112.10741v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>Diffusion models have recently been shown to generate high-quality synthetic images, especially when paired with a guidance technique to trade off diversity for fidelity. We explore diffusion models for the problem of text-conditional image synthesis and compare two different guidance strategies: CLIP guidance and classifier-free guidance. We find that the latter is preferred by human evaluators for both photorealism and caption similarity, and often produces photorealistic samples. Samples from a 3.5 billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from DALL-E, even when the latter uses expensive CLIP reranking. Additionally, we find that our models can be fine-tuned to perform image inpainting, enabling powerful text-driven image editing. We train a smaller model on a filtered dataset and release the code and weights at https://github.com/openai/glide-text2im.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2021-12-20T18:42:55Z</published>
    <arxiv:comment>20 pages, 18 figures</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Alex Nichol</name>
    </author>
    <author>
      <name>Prafulla Dhariwal</name>
    </author>
    <author>
      <name>Aditya Ramesh</name>
    </author>
    <author>
      <name>Pranav Shyam</name>
    </author>
    <author>
      <name>Pamela Mishkin</name>
    </author>
    <author>
      <name>Bob McGrew</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Mark Chen</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.05448v1</id>
    <title>Unsupervised Neural Machine Translation with Generative Language Models Only</title>
    <updated>2021-10-11T17:35:34Z</updated>
    <link href="https://arxiv.org/abs/2110.05448v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2110.05448v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We show how to derive state-of-the-art unsupervised neural machine translation systems from generatively pre-trained language models. Our method consists of three steps: few-shot amplification, distillation, and backtranslation. We first use the zero-shot translation ability of large pre-trained language models to generate translations for a small set of unlabeled sentences. We then amplify these zero-shot translations by using them as few-shot demonstrations for sampling a larger synthetic dataset. This dataset is distilled by discarding the few-shot demonstrations and then fine-tuning. During backtranslation, we repeatedly generate translations for a set of inputs and then fine-tune a single language model on both directions of the translation task at once, ensuring cycle-consistency by swapping the roles of gold monotext and generated translations when fine-tuning. By using our method to leverage GPT-3's zero-shot translation capability, we achieve a new state-of-the-art in unsupervised translation on the WMT14 English-French benchmark, attaining a BLEU score of 42.1.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2021-10-11T17:35:34Z</published>
    <arxiv:comment>10 pages</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Jesse Michael Han</name>
    </author>
    <author>
      <name>Igor Babuschkin</name>
    </author>
    <author>
      <name>Harrison Edwards</name>
    </author>
    <author>
      <name>Arvind Neelakantan</name>
    </author>
    <author>
      <name>Tao Xu</name>
    </author>
    <author>
      <name>Stanislas Polu</name>
    </author>
    <author>
      <name>Alex Ray</name>
    </author>
    <author>
      <name>Pranav Shyam</name>
    </author>
    <author>
      <name>Aditya Ramesh</name>
    </author>
    <author>
      <name>Alec Radford</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.03374v2</id>
    <title>Evaluating Large Language Models Trained on Code</title>
    <updated>2021-07-14T17:16:02Z</updated>
    <link href="https://arxiv.org/abs/2107.03374v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2107.03374v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2021-07-07T17:41:24Z</published>
    <arxiv:comment>corrected typos, added references, added authors, added acknowledgements</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Mark Chen</name>
    </author>
    <author>
      <name>Jerry Tworek</name>
    </author>
    <author>
      <name>Heewoo Jun</name>
    </author>
    <author>
      <name>Qiming Yuan</name>
    </author>
    <author>
      <name>Henrique Ponde de Oliveira Pinto</name>
    </author>
    <author>
      <name>Jared Kaplan</name>
    </author>
    <author>
      <name>Harri Edwards</name>
    </author>
    <author>
      <name>Yuri Burda</name>
    </author>
    <author>
      <name>Nicholas Joseph</name>
    </author>
    <author>
      <name>Greg Brockman</name>
    </author>
    <author>
      <name>Alex Ray</name>
    </author>
    <author>
      <name>Raul Puri</name>
    </author>
    <author>
      <name>Gretchen Krueger</name>
    </author>
    <author>
      <name>Michael Petrov</name>
    </author>
    <author>
      <name>Heidy Khlaaf</name>
    </author>
    <author>
      <name>Girish Sastry</name>
    </author>
    <author>
      <name>Pamela Mishkin</name>
    </author>
    <author>
      <name>Brooke Chan</name>
    </author>
    <author>
      <name>Scott Gray</name>
    </author>
    <author>
      <name>Nick Ryder</name>
    </author>
    <author>
      <name>Mikhail Pavlov</name>
    </author>
    <author>
      <name>Alethea Power</name>
    </author>
    <author>
      <name>Lukasz Kaiser</name>
    </author>
    <author>
      <name>Mohammad Bavarian</name>
    </author>
    <author>
      <name>Clemens Winter</name>
    </author>
    <author>
      <name>Philippe Tillet</name>
    </author>
    <author>
      <name>Felipe Petroski Such</name>
    </author>
    <author>
      <name>Dave Cummings</name>
    </author>
    <author>
      <name>Matthias Plappert</name>
    </author>
    <author>
      <name>Fotios Chantzis</name>
    </author>
    <author>
      <name>Elizabeth Barnes</name>
    </author>
    <author>
      <name>Ariel Herbert-Voss</name>
    </author>
    <author>
      <name>William Hebgen Guss</name>
    </author>
    <author>
      <name>Alex Nichol</name>
    </author>
    <author>
      <name>Alex Paino</name>
    </author>
    <author>
      <name>Nikolas Tezak</name>
    </author>
    <author>
      <name>Jie Tang</name>
    </author>
    <author>
      <name>Igor Babuschkin</name>
    </author>
    <author>
      <name>Suchir Balaji</name>
    </author>
    <author>
      <name>Shantanu Jain</name>
    </author>
    <author>
      <name>William Saunders</name>
    </author>
    <author>
      <name>Christopher Hesse</name>
    </author>
    <author>
      <name>Andrew N. Carr</name>
    </author>
    <author>
      <name>Jan Leike</name>
    </author>
    <author>
      <name>Josh Achiam</name>
    </author>
    <author>
      <name>Vedant Misra</name>
    </author>
    <author>
      <name>Evan Morikawa</name>
    </author>
    <author>
      <name>Alec Radford</name>
    </author>
    <author>
      <name>Matthew Knight</name>
    </author>
    <author>
      <name>Miles Brundage</name>
    </author>
    <author>
      <name>Mira Murati</name>
    </author>
    <author>
      <name>Katie Mayer</name>
    </author>
    <author>
      <name>Peter Welinder</name>
    </author>
    <author>
      <name>Bob McGrew</name>
    </author>
    <author>
      <name>Dario Amodei</name>
    </author>
    <author>
      <name>Sam McCandlish</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Wojciech Zaremba</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.00020v1</id>
    <title>Learning Transferable Visual Models From Natural Language Supervision</title>
    <updated>2021-02-26T19:04:58Z</updated>
    <link href="https://arxiv.org/abs/2103.00020v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2103.00020v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2021-02-26T19:04:58Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Alec Radford</name>
    </author>
    <author>
      <name>Jong Wook Kim</name>
    </author>
    <author>
      <name>Chris Hallacy</name>
    </author>
    <author>
      <name>Aditya Ramesh</name>
    </author>
    <author>
      <name>Gabriel Goh</name>
    </author>
    <author>
      <name>Sandhini Agarwal</name>
    </author>
    <author>
      <name>Girish Sastry</name>
    </author>
    <author>
      <name>Amanda Askell</name>
    </author>
    <author>
      <name>Pamela Mishkin</name>
    </author>
    <author>
      <name>Jack Clark</name>
    </author>
    <author>
      <name>Gretchen Krueger</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.12092v2</id>
    <title>Zero-Shot Text-to-Image Generation</title>
    <updated>2021-02-26T23:26:05Z</updated>
    <link href="https://arxiv.org/abs/2102.12092v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2102.12092v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2021-02-24T06:42:31Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Aditya Ramesh</name>
    </author>
    <author>
      <name>Mikhail Pavlov</name>
    </author>
    <author>
      <name>Gabriel Goh</name>
    </author>
    <author>
      <name>Scott Gray</name>
    </author>
    <author>
      <name>Chelsea Voss</name>
    </author>
    <author>
      <name>Alec Radford</name>
    </author>
    <author>
      <name>Mark Chen</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.03393v1</id>
    <title>Generative Language Modeling for Automated Theorem Proving</title>
    <updated>2020-09-07T19:50:10Z</updated>
    <link href="https://arxiv.org/abs/2009.03393v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2009.03393v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We explore the application of transformer-based language models to automated theorem proving. This work is motivated by the possibility that a major limitation of automated theorem provers compared to humans -- the generation of original mathematical terms -- might be addressable via generation from language models. We present an automated prover and proof assistant, GPT-f, for the Metamath formalization language, and analyze its performance. GPT-f found new short proofs that were accepted into the main Metamath library, which is to our knowledge, the first time a deep-learning based system has contributed proofs that were adopted by a formal mathematics community.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2020-09-07T19:50:10Z</published>
    <arxiv:comment>15+5 pages</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Stanislas Polu</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.14165v4</id>
    <title>Language Models are Few-Shot Learners</title>
    <updated>2020-07-22T19:47:17Z</updated>
    <link href="https://arxiv.org/abs/2005.14165v4" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2005.14165v4" rel="related" type="application/pdf" title="pdf"/>
    <summary>Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2020-05-28T17:29:03Z</published>
    <arxiv:comment>40+32 pages</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Tom B. Brown</name>
    </author>
    <author>
      <name>Benjamin Mann</name>
    </author>
    <author>
      <name>Nick Ryder</name>
    </author>
    <author>
      <name>Melanie Subbiah</name>
    </author>
    <author>
      <name>Jared Kaplan</name>
    </author>
    <author>
      <name>Prafulla Dhariwal</name>
    </author>
    <author>
      <name>Arvind Neelakantan</name>
    </author>
    <author>
      <name>Pranav Shyam</name>
    </author>
    <author>
      <name>Girish Sastry</name>
    </author>
    <author>
      <name>Amanda Askell</name>
    </author>
    <author>
      <name>Sandhini Agarwal</name>
    </author>
    <author>
      <name>Ariel Herbert-Voss</name>
    </author>
    <author>
      <name>Gretchen Krueger</name>
    </author>
    <author>
      <name>Tom Henighan</name>
    </author>
    <author>
      <name>Rewon Child</name>
    </author>
    <author>
      <name>Aditya Ramesh</name>
    </author>
    <author>
      <name>Daniel M. Ziegler</name>
    </author>
    <author>
      <name>Jeffrey Wu</name>
    </author>
    <author>
      <name>Clemens Winter</name>
    </author>
    <author>
      <name>Christopher Hesse</name>
    </author>
    <author>
      <name>Mark Chen</name>
    </author>
    <author>
      <name>Eric Sigler</name>
    </author>
    <author>
      <name>Mateusz Litwin</name>
    </author>
    <author>
      <name>Scott Gray</name>
    </author>
    <author>
      <name>Benjamin Chess</name>
    </author>
    <author>
      <name>Jack Clark</name>
    </author>
    <author>
      <name>Christopher Berner</name>
    </author>
    <author>
      <name>Sam McCandlish</name>
    </author>
    <author>
      <name>Alec Radford</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Dario Amodei</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.00341v1</id>
    <title>Jukebox: A Generative Model for Music</title>
    <updated>2020-04-30T09:02:45Z</updated>
    <link href="https://arxiv.org/abs/2005.00341v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2005.00341v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multi-scale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples at https://jukebox.openai.com, along with model weights and code at https://github.com/openai/jukebox</summary>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2020-04-30T09:02:45Z</published>
    <arxiv:primary_category term="eess.AS"/>
    <author>
      <name>Prafulla Dhariwal</name>
    </author>
    <author>
      <name>Heewoo Jun</name>
    </author>
    <author>
      <name>Christine Payne</name>
    </author>
    <author>
      <name>Jong Wook Kim</name>
    </author>
    <author>
      <name>Alec Radford</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.06680v1</id>
    <title>Dota 2 with Large Scale Deep Reinforcement Learning</title>
    <updated>2019-12-13T19:56:40Z</updated>
    <link href="https://arxiv.org/abs/1912.06680v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1912.06680v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2019-12-13T19:56:40Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name> OpenAI</name>
    </author>
    <author>
      <name> :</name>
    </author>
    <author>
      <name>Christopher Berner</name>
    </author>
    <author>
      <name>Greg Brockman</name>
    </author>
    <author>
      <name>Brooke Chan</name>
    </author>
    <author>
      <name>Vicki Cheung</name>
    </author>
    <author>
      <name>Przemysław Dębiak</name>
    </author>
    <author>
      <name>Christy Dennison</name>
    </author>
    <author>
      <name>David Farhi</name>
    </author>
    <author>
      <name>Quirin Fischer</name>
    </author>
    <author>
      <name>Shariq Hashme</name>
    </author>
    <author>
      <name>Chris Hesse</name>
    </author>
    <author>
      <name>Rafal Józefowicz</name>
    </author>
    <author>
      <name>Scott Gray</name>
    </author>
    <author>
      <name>Catherine Olsson</name>
    </author>
    <author>
      <name>Jakub Pachocki</name>
    </author>
    <author>
      <name>Michael Petrov</name>
    </author>
    <author>
      <name>Henrique P. d. O. Pinto</name>
    </author>
    <author>
      <name>Jonathan Raiman</name>
    </author>
    <author>
      <name>Tim Salimans</name>
    </author>
    <author>
      <name>Jeremy Schlatter</name>
    </author>
    <author>
      <name>Jonas Schneider</name>
    </author>
    <author>
      <name>Szymon Sidor</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Jie Tang</name>
    </author>
    <author>
      <name>Filip Wolski</name>
    </author>
    <author>
      <name>Susan Zhang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.02292v1</id>
    <title>Deep Double Descent: Where Bigger Models and More Data Hurt</title>
    <updated>2019-12-04T22:47:31Z</updated>
    <link href="https://arxiv.org/abs/1912.02292v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1912.02292v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We show that a variety of modern deep learning tasks exhibit a "double-descent" phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2019-12-04T22:47:31Z</published>
    <arxiv:comment>G.K. and Y.B. contributed equally</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Preetum Nakkiran</name>
    </author>
    <author>
      <name>Gal Kaplun</name>
    </author>
    <author>
      <name>Yamini Bansal</name>
    </author>
    <author>
      <name>Tristan Yang</name>
    </author>
    <author>
      <name>Boaz Barak</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.10509v1</id>
    <title>Generating Long Sequences with Sparse Transformers</title>
    <updated>2019-04-23T19:29:47Z</updated>
    <link href="https://arxiv.org/abs/1904.10509v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1904.10509v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to $O(n \sqrt{n})$. We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR-10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2019-04-23T19:29:47Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Rewon Child</name>
    </author>
    <author>
      <name>Scott Gray</name>
    </author>
    <author>
      <name>Alec Radford</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.01367v3</id>
    <title>FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models</title>
    <updated>2018-10-22T17:56:45Z</updated>
    <link href="https://arxiv.org/abs/1810.01367v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1810.01367v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>A promising class of generative models maps points from a simple distribution to a complex distribution through an invertible neural network. Likelihood-based training of these models requires restricting their architectures to allow cheap computation of Jacobian determinants. Alternatively, the Jacobian trace can be used if the transformation is specified by an ordinary differential equation. In this paper, we use Hutchinson's trace estimator to give a scalable unbiased estimate of the log-density. The result is a continuous-time invertible generative model with unbiased density estimation and one-pass sampling, while allowing unrestricted neural network architectures. We demonstrate our approach on high-dimensional density estimation, image generation, and variational inference, achieving the state-of-the-art among exact likelihood methods with efficient sampling.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2018-10-02T16:56:37Z</published>
    <arxiv:comment>8 Pages, 6 figures</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Will Grathwohl</name>
    </author>
    <author>
      <name>Ricky T. Q. Chen</name>
    </author>
    <author>
      <name>Jesse Bettencourt</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>David Duvenaud</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.00608v2</id>
    <title>GamePad: A Learning Environment for Theorem Proving</title>
    <updated>2018-12-21T18:37:30Z</updated>
    <link href="https://arxiv.org/abs/1806.00608v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1806.00608v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>In this paper, we introduce a system called GamePad that can be used to explore the application of machine learning methods to theorem proving in the Coq proof assistant. Interactive theorem provers such as Coq enable users to construct machine-checkable proofs in a step-by-step manner. Hence, they provide an opportunity to explore theorem proving with human supervision. We use GamePad to synthesize proofs for a simple algebraic rewrite problem and train baseline models for a formalization of the Feit-Thompson theorem. We address position evaluation (i.e., predict the number of proof steps left) and tactic prediction (i.e., predict the next proof step) tasks, which arise naturally in tactic-based theorem proving.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2018-06-02T09:19:08Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Daniel Huang</name>
    </author>
    <author>
      <name>Prafulla Dhariwal</name>
    </author>
    <author>
      <name>Dawn Song</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.01118v2</id>
    <title>Some Considerations on Learning to Explore via Meta-Reinforcement Learning</title>
    <updated>2019-01-11T20:26:59Z</updated>
    <link href="https://arxiv.org/abs/1803.01118v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1803.01118v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>We consider the problem of exploration in meta reinforcement learning. Two new meta reinforcement learning algorithms are suggested: E-MAML and E-$\text{RL}^2$. Results are presented on a novel environment we call `Krazy World' and a set of maze environments. We show E-MAML and E-$\text{RL}^2$ deliver better performance on tasks where exploration is important.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2018-03-03T07:13:43Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Bradly C. Stadie</name>
    </author>
    <author>
      <name>Ge Yang</name>
    </author>
    <author>
      <name>Rein Houthooft</name>
    </author>
    <author>
      <name>Xi Chen</name>
    </author>
    <author>
      <name>Yan Duan</name>
    </author>
    <author>
      <name>Yuhuai Wu</name>
    </author>
    <author>
      <name>Pieter Abbeel</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.03748v3</id>
    <title>Emergent Complexity via Multi-Agent Competition</title>
    <updated>2018-03-14T21:09:49Z</updated>
    <link href="https://arxiv.org/abs/1710.03748v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1710.03748v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>Reinforcement learning algorithms can train agents that solve problems in complex, interesting environments. Normally, the complexity of the trained agent is closely related to the complexity of the environment. This suggests that a highly capable agent requires a complex environment for training. In this paper, we point out that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself. We also point out that such environments come with a natural curriculum, because for any skill level, an environment full of agents of this level will have the right level of difficulty. This work introduces several competitive multi-agent environments where agents compete in a 3D world with simulated physics. The trained agents learn a wide variety of complex and interesting skills, even though the environment themselves are relatively simple. The skills include behaviors such as running, blocking, ducking, tackling, fooling opponents, kicking, and defending using both arms and legs. A highlight of the learned behaviors can be found here: https://goo.gl/eR7fbX</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2017-10-10T17:59:41Z</published>
    <arxiv:comment>Published as a conference paper at ICLR 2018</arxiv:comment>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Trapit Bansal</name>
    </author>
    <author>
      <name>Jakub Pachocki</name>
    </author>
    <author>
      <name>Szymon Sidor</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Igor Mordatch</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.03641v2</id>
    <title>Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments</title>
    <updated>2018-02-23T17:27:36Z</updated>
    <link href="https://arxiv.org/abs/1710.03641v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1710.03641v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>Ability to continuously learn and adapt from limited experience in nonstationary environments is an important milestone on the path towards general intelligence. In this paper, we cast the problem of continuous adaptation into the learning-to-learn framework. We develop a simple gradient-based meta-learning algorithm suitable for adaptation in dynamically changing and adversarial scenarios. Additionally, we design a new multi-agent competitive environment, RoboSumo, and define iterated adaptation games for testing various aspects of continuous adaptation strategies. We demonstrate that meta-learning enables significantly more efficient adaptation than reactive baselines in the few-shot regime. Our experiments with a population of agents that learn and compete suggest that meta-learners are the fittest.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2017-10-10T15:00:37Z</published>
    <arxiv:comment>Published as a conference paper at ICLR 2018</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Maruan Al-Shedivat</name>
    </author>
    <author>
      <name>Trapit Bansal</name>
    </author>
    <author>
      <name>Yuri Burda</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Igor Mordatch</name>
    </author>
    <author>
      <name>Pieter Abbeel</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.06428v1</id>
    <title>An online sequence-to-sequence model for noisy speech recognition</title>
    <updated>2017-06-16T20:58:43Z</updated>
    <link href="https://arxiv.org/abs/1706.06428v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1706.06428v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Generative models have long been the dominant approach for speech recognition. The success of these models however relies on the use of sophisticated recipes and complicated machinery that is not easily accessible to non-practitioners. Recent innovations in Deep Learning have given rise to an alternative - discriminative models called Sequence-to-Sequence models, that can almost match the accuracy of state of the art generative models. While these models are easy to train as they can be trained end-to-end in a single step, they have a practical limitation that they can only be used for offline recognition. This is because the models require that the entirety of the input sequence be available at the beginning of inference, an assumption that is not valid for instantaneous speech recognition. To address this problem, online sequence-to-sequence models were recently introduced. These models are able to start producing outputs as data arrives, and the model feels confident enough to output partial transcripts. These models, like sequence-to-sequence are causal - the output produced by the model until any time, $t$, affects the features that are computed subsequently. This makes the model inherently more powerful than generative models that are unable to change features that are computed from the data. This paper highlights two main contributions - an improvement to online sequence-to-sequence model training, and its application to noisy settings with mixed speech from two speakers.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2017-06-16T20:58:43Z</published>
    <arxiv:comment>arXiv admin note: substantial text overlap with arXiv:1608.01281</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Chung-Cheng Chiu</name>
    </author>
    <author>
      <name>Dieterich Lawson</name>
    </author>
    <author>
      <name>Yuping Luo</name>
    </author>
    <author>
      <name>George Tucker</name>
    </author>
    <author>
      <name>Kevin Swersky</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Navdeep Jaitly</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.01444v2</id>
    <title>Learning to Generate Reviews and Discovering Sentiment</title>
    <updated>2017-04-06T09:48:20Z</updated>
    <link href="https://arxiv.org/abs/1704.01444v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1704.01444v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>We explore the properties of byte-level recurrent language models. When given sufficient amounts of capacity, training data, and compute time, the representations learned by these models include disentangled features corresponding to high-level concepts. Specifically, we find a single unit which performs sentiment analysis. These representations, learned in an unsupervised manner, achieve state of the art on the binary subset of the Stanford Sentiment Treebank. They are also very data efficient. When using only a handful of labeled examples, our approach matches the performance of strong baselines trained on full datasets. We also demonstrate the sentiment unit has a direct influence on the generative process of the model. Simply fixing its value to be positive or negative generates samples with the corresponding positive or negative sentiment.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2017-04-05T14:20:28Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Alec Radford</name>
    </author>
    <author>
      <name>Rafal Jozefowicz</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.07326v3</id>
    <title>One-Shot Imitation Learning</title>
    <updated>2017-12-04T21:53:23Z</updated>
    <link href="https://arxiv.org/abs/1703.07326v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1703.07326v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>Imitation learning has been commonly applied to solve different tasks in isolation. This usually requires either careful feature engineering, or a significant number of samples. This is far from what we desire: ideally, robots should be able to learn from very few demonstrations of any given task, and instantly generalize to new situations of the same task, without requiring task-specific engineering. In this paper, we propose a meta-learning framework for achieving such capability, which we call one-shot imitation learning.
  Specifically, we consider the setting where there is a very large set of tasks, and each task has many instantiations. For example, a task could be to stack all blocks on a table into a single tower, another task could be to place all blocks on a table into two-block towers, etc. In each case, different instances of the task would consist of different sets of blocks with different initial states. At training time, our algorithm is presented with pairs of demonstrations for a subset of all tasks. A neural net is trained that takes as input one demonstration and the current state (which initially is the initial state of the other demonstration of the pair), and outputs an action with the goal that the resulting sequence of states and actions matches as closely as possible with the second demonstration. At test time, a demonstration of a single instance of a new task is presented, and the neural net is expected to perform well on new instances of this new task. The use of soft attention allows the model to generalize to conditions and tasks unseen in the training data. We anticipate that by training this model on a much greater variety of tasks and settings, we will obtain a general system that can turn any demonstrations into robust policies that can accomplish an overwhelming variety of tasks.
  Videos available at https://bit.ly/nips2017-oneshot .</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <published>2017-03-21T17:22:29Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Yan Duan</name>
    </author>
    <author>
      <name>Marcin Andrychowicz</name>
    </author>
    <author>
      <name>Bradly C. Stadie</name>
    </author>
    <author>
      <name>Jonathan Ho</name>
    </author>
    <author>
      <name>Jonas Schneider</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Pieter Abbeel</name>
    </author>
    <author>
      <name>Wojciech Zaremba</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.03864v2</id>
    <title>Evolution Strategies as a Scalable Alternative to Reinforcement Learning</title>
    <updated>2017-09-07T23:28:48Z</updated>
    <link href="https://arxiv.org/abs/1703.03864v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1703.03864v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>We explore the use of Evolution Strategies (ES), a class of black box optimization algorithms, as an alternative to popular MDP-based RL techniques such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using a novel communication strategy based on common random numbers, our ES implementation only needs to communicate scalars, making it possible to scale to over a thousand parallel workers. This allows us to solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training. In addition, we highlight several advantages of ES as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation.</summary>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2017-03-10T23:02:19Z</published>
    <arxiv:primary_category term="stat.ML"/>
    <author>
      <name>Tim Salimans</name>
    </author>
    <author>
      <name>Jonathan Ho</name>
    </author>
    <author>
      <name>Xi Chen</name>
    </author>
    <author>
      <name>Szymon Sidor</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.01703v2</id>
    <title>Third-Person Imitation Learning</title>
    <updated>2019-09-22T18:31:15Z</updated>
    <link href="https://arxiv.org/abs/1703.01703v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1703.01703v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>Reinforcement learning (RL) makes it possible to train agents capable of achieving sophisticated goals in complex and uncertain environments. A key difficulty in reinforcement learning is specifying a reward function for the agent to optimize. Traditionally, imitation learning in RL has been used to overcome this problem. Unfortunately, hitherto imitation learning methods tend to require that demonstrations are supplied in the first-person: the agent is provided with a sequence of states and a specification of the actions that it should have taken. While powerful, this kind of imitation learning is limited by the relatively hard problem of collecting first-person demonstrations. Humans address this problem by learning from third-person demonstrations: they observe other humans perform tasks, infer the task, and accomplish the same task themselves.
  In this paper, we present a method for unsupervised third-person imitation learning. Here third-person refers to training an agent to correctly achieve a simple goal in a simple environment when it is provided a demonstration of a teacher achieving the same goal but from a different viewpoint; and unsupervised refers to the fact that the agent receives only these third-person demonstrations, and is not provided a correspondence between teacher states and student states. Our methods primary insight is that recent advances from domain confusion can be utilized to yield domain agnostic features which are crucial during the training process. To validate our approach, we report successful experiments on learning from third-person demonstrations in a pointmass domain, a reacher domain, and inverted pendulum.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2017-03-06T02:02:34Z</published>
    <arxiv:comment>Only changed the abstract to remove unneeded hyphens</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Bradly C. Stadie</name>
    </author>
    <author>
      <name>Pieter Abbeel</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.02779v2</id>
    <title>RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning</title>
    <updated>2016-11-10T01:17:36Z</updated>
    <link href="https://arxiv.org/abs/1611.02779v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1611.02779v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>Deep reinforcement learning (deep RL) has been successful in learning sophisticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, benefiting from their prior knowledge about the world. This paper seeks to bridge this gap. Rather than designing a "fast" reinforcement learning algorithm, we propose to represent it as a recurrent neural network (RNN) and learn it from data. In our proposed method, RL$^2$, the algorithm is encoded in the weights of the RNN, which are learned slowly through a general-purpose ("slow") RL algorithm. The RNN receives all information a typical RL algorithm would receive, including observations, actions, rewards, and termination flags; and it retains its state across episodes in a given Markov Decision Process (MDP). The activations of the RNN store the state of the "fast" RL algorithm on the current (previously unseen) MDP. We evaluate RL$^2$ experimentally on both small-scale and large-scale problems. On the small-scale side, we train it to solve randomly generated multi-arm bandit problems and finite MDPs. After RL$^2$ is trained, its performance on new MDPs is close to human-designed algorithms with optimality guarantees. On the large-scale side, we test RL$^2$ on a vision-based navigation task and show that it scales up to high-dimensional problems.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2016-11-09T00:13:29Z</published>
    <arxiv:comment>14 pages. Under review as a conference paper at ICLR 2017</arxiv:comment>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Yan Duan</name>
    </author>
    <author>
      <name>John Schulman</name>
    </author>
    <author>
      <name>Xi Chen</name>
    </author>
    <author>
      <name>Peter L. Bartlett</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Pieter Abbeel</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.02731v2</id>
    <title>Variational Lossy Autoencoder</title>
    <updated>2017-03-04T06:19:22Z</updated>
    <link href="https://arxiv.org/abs/1611.02731v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1611.02731v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>Representation learning seeks to expose certain aspects of observed data in a learned representation that's amenable to downstream tasks like classification. For instance, a good representation for 2D images might be one that describes only global structure and discards information about detailed texture. In this paper, we present a simple but principled method to learn such global representations by combining Variational Autoencoder (VAE) with neural autoregressive models such as RNN, MADE and PixelRNN/CNN. Our proposed VAE model allows us to have control over what the global latent code can learn and , by designing the architecture accordingly, we can force the global latent code to discard irrelevant information such as texture in 2D images, and hence the VAE only "autoencodes" data in a lossy fashion. In addition, by leveraging autoregressive models as both prior distribution $p(z)$ and decoding distribution $p(x|z)$, we can greatly improve generative modeling performance of VAEs, achieving new state-of-the-art results on MNIST, OMNIGLOT and Caltech-101 Silhouettes density estimation tasks.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2016-11-08T21:43:34Z</published>
    <arxiv:comment>Added CIFAR10 experiments; ICLR 2017</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Xi Chen</name>
    </author>
    <author>
      <name>Diederik P. Kingma</name>
    </author>
    <author>
      <name>Tim Salimans</name>
    </author>
    <author>
      <name>Yan Duan</name>
    </author>
    <author>
      <name>Prafulla Dhariwal</name>
    </author>
    <author>
      <name>John Schulman</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Pieter Abbeel</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.00736v2</id>
    <title>Extensions and Limitations of the Neural GPU</title>
    <updated>2016-11-04T20:46:40Z</updated>
    <link href="https://arxiv.org/abs/1611.00736v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1611.00736v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>The Neural GPU is a recent model that can learn algorithms such as multi-digit binary addition and binary multiplication in a way that generalizes to inputs of arbitrary length. We show that there are two simple ways of improving the performance of the Neural GPU: by carefully designing a curriculum, and by increasing model size. The latter requires a memory efficient implementation, as a naive implementation of the Neural GPU is memory intensive. We find that these techniques increase the set of algorithmic problems that can be solved by the Neural GPU: we have been able to learn to perform all the arithmetic operations (and generalize to arbitrarily long numbers) when the arguments are given in the decimal representation (which, surprisingly, has not been possible before). We have also been able to train the Neural GPU to evaluate long arithmetic expressions with multiple operands that require respecting the precedence order of the operands, although these have succeeded only in their binary representation, and not with perfect accuracy.
  In addition, we gain insight into the Neural GPU by investigating its failure modes. We find that Neural GPUs that correctly generalize to arbitrarily long numbers still fail to compute the correct answer on highly-symmetric, atypical inputs: for example, a Neural GPU that achieves near-perfect generalization on decimal multiplication of up to 100-digit long numbers can fail on $000000\dots002 \times 000000\dots002$ while succeeding at $2 \times 2$. These failure modes are reminiscent of adversarial examples.</summary>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2016-11-02T19:18:17Z</published>
    <arxiv:primary_category term="cs.NE"/>
    <author>
      <name>Eric Price</name>
    </author>
    <author>
      <name>Wojciech Zaremba</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.01281v1</id>
    <title>Learning Online Alignments with Continuous Rewards Policy Gradient</title>
    <updated>2016-08-03T18:35:12Z</updated>
    <link href="https://arxiv.org/abs/1608.01281v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1608.01281v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Sequence-to-sequence models with soft attention had significant success in machine translation, speech recognition, and question answering. Though capable and easy to use, they require that the entirety of the input sequence is available at the beginning of inference, an assumption that is not valid for instantaneous translation and speech recognition. To address this problem, we present a new method for solving sequence-to-sequence problems using hard online alignments instead of soft offline alignments. The online alignments model is able to start producing outputs without the need to first process the entire input sequence. A highly accurate online sequence-to-sequence model is useful because it can be used to build an accurate voice-based instantaneous translator. Our model uses hard binary stochastic decisions to select the timesteps at which outputs will be produced. The model is trained to produce these stochastic decisions using a standard policy gradient method. In our experiments, we show that this model achieves encouraging performance on TIMIT and Wall Street Journal (WSJ) speech recognition datasets.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2016-08-03T18:35:12Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Yuping Luo</name>
    </author>
    <author>
      <name>Chung-Cheng Chiu</name>
    </author>
    <author>
      <name>Navdeep Jaitly</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.04934v2</id>
    <title>Improving Variational Inference with Inverse Autoregressive Flow</title>
    <updated>2017-01-30T20:36:01Z</updated>
    <link href="https://arxiv.org/abs/1606.04934v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1606.04934v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>The framework of normalizing flows provides a general strategy for flexible variational inference of posteriors over latent variables. We propose a new type of normalizing flow, inverse autoregressive flow (IAF), that, in contrast to earlier published flows, scales well to high-dimensional latent spaces. The proposed flow consists of a chain of invertible transformations, where each transformation is based on an autoregressive neural network. In experiments, we show that IAF significantly improves upon diagonal Gaussian approximate posteriors. In addition, we demonstrate that a novel type of variational autoencoder, coupled with IAF, is competitive with neural autoregressive models in terms of attained log-likelihood on natural images, while allowing significantly faster synthesis.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2016-06-15T19:46:36Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Diederik P. Kingma</name>
    </author>
    <author>
      <name>Tim Salimans</name>
    </author>
    <author>
      <name>Rafal Jozefowicz</name>
    </author>
    <author>
      <name>Xi Chen</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Max Welling</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03657v1</id>
    <title>InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets</title>
    <updated>2016-06-12T02:14:31Z</updated>
    <link href="https://arxiv.org/abs/1606.03657v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1606.03657v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2016-06-12T02:14:31Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Xi Chen</name>
    </author>
    <author>
      <name>Yan Duan</name>
    </author>
    <author>
      <name>Rein Houthooft</name>
    </author>
    <author>
      <name>John Schulman</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Pieter Abbeel</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.04467v2</id>
    <title>TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems</title>
    <updated>2016-03-16T16:57:12Z</updated>
    <link href="https://arxiv.org/abs/1603.04467v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1603.04467v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.</summary>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2016-03-14T20:50:20Z</published>
    <arxiv:comment>Version 2 updates only the metadata, to correct the formatting of Martín Abadi's name</arxiv:comment>
    <arxiv:primary_category term="cs.DC"/>
    <author>
      <name>Martín Abadi</name>
    </author>
    <author>
      <name>Ashish Agarwal</name>
    </author>
    <author>
      <name>Paul Barham</name>
    </author>
    <author>
      <name>Eugene Brevdo</name>
    </author>
    <author>
      <name>Zhifeng Chen</name>
    </author>
    <author>
      <name>Craig Citro</name>
    </author>
    <author>
      <name>Greg S. Corrado</name>
    </author>
    <author>
      <name>Andy Davis</name>
    </author>
    <author>
      <name>Jeffrey Dean</name>
    </author>
    <author>
      <name>Matthieu Devin</name>
    </author>
    <author>
      <name>Sanjay Ghemawat</name>
    </author>
    <author>
      <name>Ian Goodfellow</name>
    </author>
    <author>
      <name>Andrew Harp</name>
    </author>
    <author>
      <name>Geoffrey Irving</name>
    </author>
    <author>
      <name>Michael Isard</name>
    </author>
    <author>
      <name>Yangqing Jia</name>
    </author>
    <author>
      <name>Rafal Jozefowicz</name>
    </author>
    <author>
      <name>Lukasz Kaiser</name>
    </author>
    <author>
      <name>Manjunath Kudlur</name>
    </author>
    <author>
      <name>Josh Levenberg</name>
    </author>
    <author>
      <name>Dan Mane</name>
    </author>
    <author>
      <name>Rajat Monga</name>
    </author>
    <author>
      <name>Sherry Moore</name>
    </author>
    <author>
      <name>Derek Murray</name>
    </author>
    <author>
      <name>Chris Olah</name>
    </author>
    <author>
      <name>Mike Schuster</name>
    </author>
    <author>
      <name>Jonathon Shlens</name>
    </author>
    <author>
      <name>Benoit Steiner</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Kunal Talwar</name>
    </author>
    <author>
      <name>Paul Tucker</name>
    </author>
    <author>
      <name>Vincent Vanhoucke</name>
    </author>
    <author>
      <name>Vijay Vasudevan</name>
    </author>
    <author>
      <name>Fernanda Viegas</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Pete Warden</name>
    </author>
    <author>
      <name>Martin Wattenberg</name>
    </author>
    <author>
      <name>Martin Wicke</name>
    </author>
    <author>
      <name>Yuan Yu</name>
    </author>
    <author>
      <name>Xiaoqiang Zheng</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.00748v1</id>
    <title>Continuous Deep Q-Learning with Model-based Acceleration</title>
    <updated>2016-03-02T15:28:25Z</updated>
    <link href="https://arxiv.org/abs/1603.00748v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1603.00748v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Model-free reinforcement learning has been successfully applied to a range of challenging problems, and has recently been extended to handle large neural network policies and value functions. However, the sample complexity of model-free algorithms, particularly when using high-dimensional function approximators, tends to limit their applicability to physical systems. In this paper, we explore algorithms and representations to reduce the sample complexity of deep reinforcement learning for continuous control tasks. We propose two complementary techniques for improving the efficiency of such algorithms. First, we derive a continuous variant of the Q-learning algorithm, which we call normalized adantage functions (NAF), as an alternative to the more commonly used policy gradient and actor-critic methods. NAF representation allows us to apply Q-learning with experience replay to continuous tasks, and substantially improves performance on a set of simulated robotic control tasks. To further improve the efficiency of our approach, we explore the use of learned models for accelerating model-free reinforcement learning. We show that iteratively refitted local linear models are especially effective for this, and demonstrate substantially faster learning on domains where such models are applicable.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <published>2016-03-02T15:28:25Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Shixiang Gu</name>
    </author>
    <author>
      <name>Timothy Lillicrap</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Sergey Levine</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.08228v3</id>
    <title>Neural GPUs Learn Algorithms</title>
    <updated>2016-03-15T00:20:54Z</updated>
    <link href="https://arxiv.org/abs/1511.08228v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1511.08228v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>Learning an algorithm from examples is a fundamental problem that has been widely studied. Recently it has been addressed using neural networks, in particular by Neural Turing Machines (NTMs). These are fully differentiable computers that use backpropagation to learn their own programming. Despite their appeal NTMs have a weakness that is caused by their sequential nature: they are not parallel and are are hard to train due to their large depth when unfolded.
  We present a neural network architecture to address this problem: the Neural GPU. It is based on a type of convolutional gated recurrent unit and, like the NTM, is computationally universal. Unlike the NTM, the Neural GPU is highly parallel which makes it easier to train and efficient to run.
  An essential property of algorithms is their ability to handle inputs of arbitrary size. We show that the Neural GPU can be trained on short instances of an algorithmic task and successfully generalize to long instances. We verified it on a number of tasks including long addition and long multiplication of numbers represented in binary. We train the Neural GPU on numbers with upto 20 bits and observe no errors whatsoever while testing it, even on much longer numbers.
  To achieve these results we introduce a technique for training deep recurrent networks: parameter sharing relaxation. We also found a small amount of dropout and gradient noise to have a large positive effect on learning and generalization.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2015-11-25T21:17:43Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Łukasz Kaiser</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.06807v1</id>
    <title>Adding Gradient Noise Improves Learning for Very Deep Networks</title>
    <updated>2015-11-21T01:11:29Z</updated>
    <link href="https://arxiv.org/abs/1511.06807v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1511.06807v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Deep feedforward and recurrent networks have achieved impressive results in many perception and language processing applications. This success is partially attributed to architectural innovations such as convolutional and long short-term memory networks. The main motivation for these architectural innovations is that they capture better domain knowledge, and importantly are easier to optimize than more basic architectures. Recently, more complex architectures such as Neural Turing Machines and Memory Networks have been proposed for tasks including question answering and general computation, creating a new set of optimization challenges. In this paper, we discuss a low-overhead and easy-to-implement technique of adding gradient noise which we find to be surprisingly effective when training these very deep architectures. The technique not only helps to avoid overfitting, but also can result in lower training loss. This method alone allows a fully-connected 20-layer deep network to be trained with standard gradient descent, even starting from a poor initialization. We see consistent improvements for many complex models, including a 72% relative reduction in error rate over a carefully-tuned baseline on a challenging question-answering task, and a doubling of the number of accurate binary multiplication models learned across 7,000 random restarts. We encourage further application of this technique to additional complex modern architectures.</summary>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2015-11-21T01:11:29Z</published>
    <arxiv:primary_category term="stat.ML"/>
    <author>
      <name>Arvind Neelakantan</name>
    </author>
    <author>
      <name>Luke Vilnis</name>
    </author>
    <author>
      <name>Quoc V. Le</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Lukasz Kaiser</name>
    </author>
    <author>
      <name>Karol Kurach</name>
    </author>
    <author>
      <name>James Martens</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.06440v2</id>
    <title>Towards Principled Unsupervised Learning</title>
    <updated>2015-12-03T17:24:22Z</updated>
    <link href="https://arxiv.org/abs/1511.06440v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1511.06440v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>General unsupervised learning is a long-standing conceptual problem in machine learning. Supervised learning is successful because it can be solved by the minimization of the training error cost function. Unsupervised learning is not as successful, because the unsupervised objective may be unrelated to the supervised task of interest. For an example, density modelling and reconstruction have often been used for unsupervised learning, but they did not produced the sought-after performance gains, because they have no knowledge of the supervised tasks.
  In this paper, we present an unsupervised cost function which we name the Output Distribution Matching (ODM) cost, which measures a divergence between the distribution of predictions and distributions of labels. The ODM cost is appealing because it is consistent with the supervised cost in the following sense: a perfect supervised classifier is also perfect according to the ODM cost. Therefore, by aggressively optimizing the ODM cost, we are almost guaranteed to improve our supervised performance whenever the space of possible predictions is exponentially large.
  We demonstrate that the ODM cost works well on number of small and semi-artificial datasets using no (or almost no) labelled training cases. Finally, we show that the ODM cost can be used for one-shot domain adaptation, which allows the model to classify inputs that differ from the input distribution in significant ways without the need for prior exposure to the new domain.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2015-11-19T23:04:23Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Rafal Jozefowicz</name>
    </author>
    <author>
      <name>Karol Gregor</name>
    </author>
    <author>
      <name>Danilo Rezende</name>
    </author>
    <author>
      <name>Tim Lillicrap</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.06392v3</id>
    <title>Neural Random-Access Machines</title>
    <updated>2016-02-09T21:29:07Z</updated>
    <link href="https://arxiv.org/abs/1511.06392v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1511.06392v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>In this paper, we propose and investigate a new neural network architecture called Neural Random Access Machine. It can manipulate and dereference pointers to an external variable-size random-access memory. The model is trained from pure input-output examples using backpropagation.
  We evaluate the new model on a number of simple algorithmic tasks whose solutions require pointer manipulation and dereferencing. Our results show that the proposed model can learn to solve algorithmic tasks of such type and is capable of operating on simple data structures like linked-lists and binary trees. For easier tasks, the learned solutions generalize to sequences of arbitrary length. Moreover, memory access during inference can be done in a constant time under some assumptions.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2015-11-19T21:36:28Z</published>
    <arxiv:comment>ICLR submission, 17 pages, 9 figures, 6 tables (with bibliography and appendix)</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Karol Kurach</name>
    </author>
    <author>
      <name>Marcin Andrychowicz</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.06114v4</id>
    <title>Multi-task Sequence to Sequence Learning</title>
    <updated>2016-03-01T10:55:58Z</updated>
    <link href="https://arxiv.org/abs/1511.06114v4" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1511.06114v4" rel="related" type="application/pdf" title="pdf"/>
    <summary>Sequence to sequence learning has recently emerged as a new paradigm in supervised learning. To date, most of its applications focused on only one task and not much work explored this framework for multiple tasks. This paper examines three multi-task learning (MTL) settings for sequence to sequence models: (a) the oneto-many setting - where the encoder is shared between several tasks such as machine translation and syntactic parsing, (b) the many-to-one setting - useful when only the decoder can be shared, as in the case of translation and image caption generation, and (c) the many-to-many setting - where multiple encoders and decoders are shared, which is the case with unsupervised objectives and translation. Our results show that training on a small amount of parsing and image caption data can improve the translation quality between English and German by up to 1.5 BLEU points over strong single-task baselines on the WMT benchmarks. Furthermore, we have established a new state-of-the-art result in constituent parsing with 93.0 F1. Lastly, we reveal interesting properties of the two unsupervised learning objectives, autoencoder and skip-thought, in the MTL context: autoencoder helps less in terms of perplexities but more on BLEU scores compared to skip-thought.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2015-11-19T10:24:14Z</published>
    <arxiv:comment>10 pages, 4 figures, ICLR 2016 camera-ready, added parsing SOTA results</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Minh-Thang Luong</name>
    </author>
    <author>
      <name>Quoc V. Le</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Lukasz Kaiser</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.05176v3</id>
    <title>MuProp: Unbiased Backpropagation for Stochastic Neural Networks</title>
    <updated>2016-02-25T20:36:21Z</updated>
    <link href="https://arxiv.org/abs/1511.05176v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1511.05176v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>Deep neural networks are powerful parametric models that can be trained efficiently using the backpropagation algorithm. Stochastic neural networks combine the power of large parametric functions with that of graphical models, which makes it possible to learn very complex distributions. However, as backpropagation is not directly applicable to stochastic networks that include discrete sampling operations within their computational graph, training such networks remains difficult. We present MuProp, an unbiased gradient estimator for stochastic networks, designed to make this task easier. MuProp improves on the likelihood-ratio estimator by reducing its variance using a control variate based on the first-order Taylor expansion of a mean-field network. Crucially, unlike prior attempts at using backpropagation for training stochastic networks, the resulting estimator is unbiased and well behaved. Our experiments on structured output prediction and discrete latent variable modeling demonstrate that MuProp yields consistently good performance across a range of difficult tasks.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2015-11-16T21:08:25Z</published>
    <arxiv:comment>Published as a conference paper at ICLR 2016</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Shixiang Gu</name>
    </author>
    <author>
      <name>Sergey Levine</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Andriy Mnih</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.04868v4</id>
    <title>A Neural Transducer</title>
    <updated>2016-08-04T23:31:46Z</updated>
    <link href="https://arxiv.org/abs/1511.04868v4" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1511.04868v4" rel="related" type="application/pdf" title="pdf"/>
    <summary>Sequence-to-sequence models have achieved impressive results on various tasks. However, they are unsuitable for tasks that require incremental predictions to be made as more data arrives or tasks that have long input sequences and output sequences. This is because they generate an output sequence conditioned on an entire input sequence. In this paper, we present a Neural Transducer that can make incremental predictions as more input arrives, without redoing the entire computation. Unlike sequence-to-sequence models, the Neural Transducer computes the next-step distribution conditioned on the partially observed input sequence and the partially generated sequence. At each time step, the transducer can decide to emit zero to many output symbols. The data can be processed using an encoder and presented as input to the transducer. The discrete decision to emit a symbol at every time step makes it difficult to learn with conventional backpropagation. It is however possible to train the transducer by using a dynamic programming algorithm to generate target discrete decisions. Our experiments show that the Neural Transducer works well in settings where it is required to produce output predictions as data come in. We also find that the Neural Transducer performs well for long sequences even when attention mechanisms are not used.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2015-11-16T08:53:44Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Navdeep Jaitly</name>
    </author>
    <author>
      <name>David Sussillo</name>
    </author>
    <author>
      <name>Quoc V. Le</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Samy Bengio</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.04834v3</id>
    <title>Neural Programmer: Inducing Latent Programs with Gradient Descent</title>
    <updated>2016-08-04T18:23:03Z</updated>
    <link href="https://arxiv.org/abs/1511.04834v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1511.04834v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>Deep neural networks have achieved impressive supervised classification performance in many tasks including image recognition, speech recognition, and sequence to sequence learning. However, this success has not been translated to applications like question answering that may involve complex arithmetic and logic reasoning. A major limitation of these models is in their inability to learn even simple arithmetic and logic operations. For example, it has been shown that neural networks fail to learn to add two binary numbers reliably. In this work, we propose Neural Programmer, an end-to-end differentiable neural network augmented with a small set of basic arithmetic and logic operations. Neural Programmer can call these augmented operations over several steps, thereby inducing compositional programs that are more complex than the built-in operations. The model learns from a weak supervision signal which is the result of execution of the correct program, hence it does not require expensive annotation of the correct program itself. The decisions of what operations to call, and what data segments to apply to are inferred by Neural Programmer. Such decisions, during training, are done in a differentiable fashion so that the entire network can be trained jointly by gradient descent. We find that training the model is difficult, but it can be greatly improved by adding random noise to the gradient. On a fairly complex synthetic table-comprehension dataset, traditional recurrent networks and attentional models perform poorly while Neural Programmer typically obtains nearly perfect accuracy.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2015-11-16T06:03:58Z</published>
    <arxiv:comment>Accepted as a conference paper at ICLR 2015</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Arvind Neelakantan</name>
    </author>
    <author>
      <name>Quoc V. Le</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.00521v3</id>
    <title>Reinforcement Learning Neural Turing Machines - Revised</title>
    <updated>2016-01-12T06:35:48Z</updated>
    <link href="https://arxiv.org/abs/1505.00521v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1505.00521v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>The Neural Turing Machine (NTM) is more expressive than all previously considered models because of its external memory. It can be viewed as a broader effort to use abstract external Interfaces and to learn a parametric model that interacts with them.
  The capabilities of a model can be extended by providing it with proper Interfaces that interact with the world. These external Interfaces include memory, a database, a search engine, or a piece of software such as a theorem verifier. Some of these Interfaces are provided by the developers of the model. However, many important existing Interfaces, such as databases and search engines, are discrete.
  We examine feasibility of learning models to interact with discrete Interfaces. We investigate the following discrete Interfaces: a memory Tape, an input Tape, and an output Tape. We use a Reinforcement Learning algorithm to train a neural network that interacts with such Interfaces to solve simple algorithmic tasks. Our Interfaces are expressive enough to make our model Turing complete.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2015-05-04T04:14:54Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Wojciech Zaremba</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.7449v3</id>
    <title>Grammar as a Foreign Language</title>
    <updated>2015-06-09T22:41:07Z</updated>
    <link href="https://arxiv.org/abs/1412.7449v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1412.7449v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>Syntactic constituency parsing is a fundamental problem in natural language processing and has been the subject of intensive research and engineering for decades. As a result, the most accurate parsers are domain specific, complex, and inefficient. In this paper we show that the domain agnostic attention-enhanced sequence-to-sequence model achieves state-of-the-art results on the most widely used syntactic constituency parsing dataset, when trained on a large synthetic corpus that was annotated using existing parsers. It also matches the performance of standard parsers when trained only on a small human-annotated dataset, which shows that this model is highly data-efficient, in contrast to sequence-to-sequence models without the attention mechanism. Our parser is also fast, processing over a hundred sentences per second with an unoptimized CPU implementation.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2014-12-23T17:16:24Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Lukasz Kaiser</name>
    </author>
    <author>
      <name>Terry Koo</name>
    </author>
    <author>
      <name>Slav Petrov</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Geoffrey Hinton</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.6564v2</id>
    <title>Move Evaluation in Go Using Deep Convolutional Neural Networks</title>
    <updated>2015-04-10T19:03:34Z</updated>
    <link href="https://arxiv.org/abs/1412.6564v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1412.6564v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>The game of Go is more challenging than other board games, due to the difficulty of constructing a position or move evaluation function. In this paper we investigate whether deep convolutional networks can be used to directly represent and learn this knowledge. We train a large 12-layer convolutional neural network by supervised learning from a database of human professional games. The network correctly predicts the expert move in 55% of positions, equalling the accuracy of a 6 dan human player. When the trained convolutional network was used directly to play games of Go, without any search, it beat the traditional search program GnuGo in 97% of games, and matched the performance of a state-of-the-art Monte-Carlo tree search that simulates a million positions per move.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2014-12-20T00:31:30Z</published>
    <arxiv:comment>Minor edits and included captures in Figure 2</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Chris J. Maddison</name>
    </author>
    <author>
      <name>Aja Huang</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>David Silver</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.8206v4</id>
    <title>Addressing the Rare Word Problem in Neural Machine Translation</title>
    <updated>2015-05-30T19:57:28Z</updated>
    <link href="https://arxiv.org/abs/1410.8206v4" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1410.8206v4" rel="related" type="application/pdf" title="pdf"/>
    <summary>Neural Machine Translation (NMT) is a new approach to machine translation that has shown promising results that are comparable to traditional approaches. A significant weakness in conventional NMT systems is their inability to correctly translate very rare words: end-to-end NMTs tend to have relatively small vocabularies with a single unk symbol that represents every possible out-of-vocabulary (OOV) word. In this paper, we propose and implement an effective technique to address this problem. We train an NMT system on data that is augmented by the output of a word alignment algorithm, allowing the NMT system to emit, for each OOV word in the target sentence, the position of its corresponding word in the source sentence. This information is later utilized in a post-processing step that translates every OOV word using a dictionary. Our experiments on the WMT14 English to French translation task show that this method provides a substantial improvement of up to 2.8 BLEU points over an equivalent NMT system that does not use this technique. With 37.5 BLEU points, our NMT system is the first to surpass the best result achieved on a WMT14 contest task.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2014-10-30T00:20:31Z</published>
    <arxiv:comment>ACL 2015 camera-ready version</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Minh-Thang Luong</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Quoc V. Le</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Wojciech Zaremba</name>
    </author>
  </entry>
</feed>
