<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api//E1ybzsaINuwEXkYHSdvLxU1MW4</id>
  <title>arXiv Query: search_query=au:"Ilya Sutskever"&amp;id_list=&amp;start=50&amp;max_results=50</title>
  <updated>2026-02-06T20:32:05Z</updated>
  <link href="https://arxiv.org/api/query?search_query=au:%22Ilya+Sutskever%22&amp;start=50&amp;max_results=50&amp;id_list=" type="application/atom+xml"/>
  <opensearch:itemsPerPage>50</opensearch:itemsPerPage>
  <opensearch:totalResults>59</opensearch:totalResults>
  <opensearch:startIndex>50</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/1410.4615v3</id>
    <title>Learning to Execute</title>
    <updated>2015-02-19T15:33:35Z</updated>
    <link href="https://arxiv.org/abs/1410.4615v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1410.4615v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>Recurrent Neural Networks (RNNs) with Long Short-Term Memory units (LSTM) are widely used because they are expressive and are easy to train. Our interest lies in empirically evaluating the expressiveness and the learnability of LSTMs in the sequence-to-sequence regime by training them to evaluate short computer programs, a domain that has traditionally been seen as too complex for neural networks. We consider a simple class of programs that can be evaluated with a single left-to-right pass using constant memory. Our main result is that LSTMs can learn to map the character-level representations of such programs to their correct outputs. Notably, it was necessary to use curriculum learning, and while conventional curriculum learning proved ineffective, we developed a new variant of curriculum learning that improved our networks' performance in all experimental conditions. The improved curriculum had a dramatic impact on an addition problem, making it possible to train an LSTM to add two 9-digit numbers with 99% accuracy.</summary>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2014-10-17T01:35:12Z</published>
    <arxiv:primary_category term="cs.NE"/>
    <author>
      <name>Wojciech Zaremba</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.3215v3</id>
    <title>Sequence to Sequence Learning with Neural Networks</title>
    <updated>2014-12-14T20:59:51Z</updated>
    <link href="https://arxiv.org/abs/1409.3215v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1409.3215v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2014-09-10T19:55:35Z</published>
    <arxiv:comment>9 pages</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Quoc V. Le</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.2329v5</id>
    <title>Recurrent Neural Network Regularization</title>
    <updated>2015-02-19T14:46:00Z</updated>
    <link href="https://arxiv.org/abs/1409.2329v5" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1409.2329v5" rel="related" type="application/pdf" title="pdf"/>
    <summary>We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation.</summary>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2014-09-08T13:08:00Z</published>
    <arxiv:primary_category term="cs.NE"/>
    <author>
      <name>Wojciech Zaremba</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.6199v4</id>
    <title>Intriguing properties of neural networks</title>
    <updated>2014-02-19T16:33:14Z</updated>
    <link href="https://arxiv.org/abs/1312.6199v4" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1312.6199v4" rel="related" type="application/pdf" title="pdf"/>
    <summary>Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties.
  First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks.
  Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2013-12-21T03:36:08Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Christian Szegedy</name>
    </author>
    <author>
      <name>Wojciech Zaremba</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Joan Bruna</name>
    </author>
    <author>
      <name>Dumitru Erhan</name>
    </author>
    <author>
      <name>Ian Goodfellow</name>
    </author>
    <author>
      <name>Rob Fergus</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.4314v3</id>
    <title>Learning Factored Representations in a Deep Mixture of Experts</title>
    <updated>2014-03-09T20:15:03Z</updated>
    <link href="https://arxiv.org/abs/1312.4314v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1312.4314v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>Mixtures of Experts combine the outputs of several "expert" networks, each of which specializes in a different part of the input space. This is achieved by training a "gating" network that maps each input to a distribution over the experts. Such models show promise for building larger networks that are still cheap to compute at test time, and more parallelizable at training time. In this this work, we extend the Mixture of Experts to a stacked model, the Deep Mixture of Experts, with multiple sets of gating and experts. This exponentially increases the number of effective experts by associating each input with a combination of experts at each layer, yet maintains a modest model size. On a randomly translated version of the MNIST dataset, we find that the Deep Mixture of Experts automatically learns to develop location-dependent ("where") experts at the first layer, and class-specific ("what") experts at the second layer. In addition, we see that the different combinations are in use when the model is applied to a dataset of speech monophones. These demonstrate effective use of all expert combinations.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2013-12-16T11:15:10Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>David Eigen</name>
    </author>
    <author>
      <name>Marc'Aurelio Ranzato</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.4546v1</id>
    <title>Distributed Representations of Words and Phrases and their Compositionality</title>
    <updated>2013-10-16T23:28:53Z</updated>
    <link href="https://arxiv.org/abs/1310.4546v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1310.4546v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2013-10-16T23:28:53Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Tomas Mikolov</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Kai Chen</name>
    </author>
    <author>
      <name>Greg Corrado</name>
    </author>
    <author>
      <name>Jeffrey Dean</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.4168v1</id>
    <title>Exploiting Similarities among Languages for Machine Translation</title>
    <updated>2013-09-17T03:23:13Z</updated>
    <link href="https://arxiv.org/abs/1309.4168v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1309.4168v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Dictionaries and phrase tables are the basis of modern statistical machine translation systems. This paper develops a method that can automate the process of generating and extending dictionaries and phrase tables. Our method can translate missing word and phrase entries by learning language structures based on large monolingual data and mapping between languages from small bilingual data. It uses distributed representation of words and learns a linear mapping between vector spaces of languages. Despite its simplicity, our method is surprisingly effective: we can achieve almost 90% precision@5 for translation of words between English and Spanish. This method makes little assumption about the languages, so it can be used to extend and refine dictionaries and translation tables for any language pairs.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2013-09-17T03:23:13Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Tomas Mikolov</name>
    </author>
    <author>
      <name>Quoc V. Le</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.0580v1</id>
    <title>Improving neural networks by preventing co-adaptation of feature detectors</title>
    <updated>2012-07-03T06:35:15Z</updated>
    <link href="https://arxiv.org/abs/1207.0580v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1207.0580v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.</summary>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2012-07-03T06:35:15Z</published>
    <arxiv:primary_category term="cs.NE"/>
    <author>
      <name>Geoffrey E. Hinton</name>
    </author>
    <author>
      <name>Nitish Srivastava</name>
    </author>
    <author>
      <name>Alex Krizhevsky</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Ruslan R. Salakhutdinov</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.6464v2</id>
    <title>Estimating the Hessian by Back-propagating Curvature</title>
    <updated>2012-09-04T18:32:03Z</updated>
    <link href="https://arxiv.org/abs/1206.6464v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1206.6464v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>In this work we develop Curvature Propagation (CP), a general technique for efficiently computing unbiased approximations of the Hessian of any function that is computed using a computational graph. At the cost of roughly two gradient evaluations, CP can give a rank-1 approximation of the whole Hessian, and can be repeatedly applied to give increasingly precise unbiased estimates of any or all of the entries of the Hessian. Of particular interest is the diagonal of the Hessian, for which no general approach is known to exist that is both efficient and accurate. We show in experiments that CP turns out to work well in practice, giving very accurate estimates of the Hessian of neural networks, for example, with a relatively small amount of work. We also apply CP to Score Matching, where a diagonal of a Hessian plays an integral role in the Score Matching objective, and where it is usually computed exactly using inefficient algorithms which do not scale to larger and more complex models.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2012-06-27T19:59:59Z</published>
    <arxiv:comment>Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>James Martens</name>
      <arxiv:affiliation>University of Toronto</arxiv:affiliation>
    </author>
    <author>
      <name>Ilya Sutskever</name>
      <arxiv:affiliation>University of Toronto</arxiv:affiliation>
    </author>
    <author>
      <name>Kevin Swersky</name>
      <arxiv:affiliation>University of Toronto</arxiv:affiliation>
    </author>
  </entry>
</feed>
