<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api/y7jSAj5M+yr3DWp0AQ3tFtuh6XU</id>
  <title>arXiv Query: search_query=au:"Dario Amodei"&amp;id_list=&amp;start=0&amp;max_results=50</title>
  <updated>2026-02-06T21:38:24Z</updated>
  <link href="https://arxiv.org/api/query?search_query=au:%22Dario+Amodei%22&amp;start=0&amp;max_results=50&amp;id_list=" type="application/atom+xml"/>
  <opensearch:itemsPerPage>50</opensearch:itemsPerPage>
  <opensearch:totalResults>33</opensearch:totalResults>
  <opensearch:startIndex>0</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/2503.04761v1</id>
    <title>Which Economic Tasks are Performed with AI? Evidence from Millions of Claude Conversations</title>
    <updated>2025-02-11T00:46:43Z</updated>
    <link href="https://arxiv.org/abs/2503.04761v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2503.04761v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Despite widespread speculation about artificial intelligence's impact on the future of work, we lack systematic empirical evidence about how these systems are actually being used for different tasks. Here, we present a novel framework for measuring AI usage patterns across the economy. We leverage a recent privacy-preserving system to analyze over four million Claude.ai conversations through the lens of tasks and occupations in the U.S. Department of Labor's O*NET Database. Our analysis reveals that AI usage primarily concentrates in software development and writing tasks, which together account for nearly half of all total usage. However, usage of AI extends more broadly across the economy, with approximately 36% of occupations using AI for at least a quarter of their associated tasks. We also analyze how AI is being used for tasks, finding 57% of usage suggests augmentation of human capabilities (e.g., learning or iterating on an output) while 43% suggests automation (e.g., fulfilling a request with minimal human involvement). While our data and methods face important limitations and only paint a picture of AI usage on a single platform, they provide an automated, granular approach for tracking AI's evolving role in the economy and identifying leading indicators of future impact as these technologies continue to advance.</summary>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-02-11T00:46:43Z</published>
    <arxiv:primary_category term="cs.CY"/>
    <author>
      <name>Kunal Handa</name>
    </author>
    <author>
      <name>Alex Tamkin</name>
    </author>
    <author>
      <name>Miles McCain</name>
    </author>
    <author>
      <name>Saffron Huang</name>
    </author>
    <author>
      <name>Esin Durmus</name>
    </author>
    <author>
      <name>Sarah Heck</name>
    </author>
    <author>
      <name>Jared Mueller</name>
    </author>
    <author>
      <name>Jerry Hong</name>
    </author>
    <author>
      <name>Stuart Ritchie</name>
    </author>
    <author>
      <name>Tim Belonax</name>
    </author>
    <author>
      <name>Kevin K. Troy</name>
    </author>
    <author>
      <name>Dario Amodei</name>
    </author>
    <author>
      <name>Jared Kaplan</name>
    </author>
    <author>
      <name>Jack Clark</name>
    </author>
    <author>
      <name>Deep Ganguli</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.07459v2</id>
    <title>The Capacity for Moral Self-Correction in Large Language Models</title>
    <updated>2023-02-18T21:30:27Z</updated>
    <link href="https://arxiv.org/abs/2302.07459v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2302.07459v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>We test the hypothesis that language models trained with reinforcement learning from human feedback (RLHF) have the capability to "morally self-correct" -- to avoid producing harmful outputs -- if instructed to do so. We find strong evidence in support of this hypothesis across three different experiments, each of which reveal different facets of moral self-correction. We find that the capability for moral self-correction emerges at 22B model parameters, and typically improves with increasing model size and RLHF training. We believe that at this level of scale, language models obtain two capabilities that they can use for moral self-correction: (1) they can follow instructions and (2) they can learn complex normative concepts of harm like stereotyping, bias, and discrimination. As such, they can follow instructions to avoid certain kinds of morally harmful outputs. We believe our results are cause for cautious optimism regarding the ability to train language models to abide by ethical principles.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2023-02-15T04:25:40Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Deep Ganguli</name>
    </author>
    <author>
      <name>Amanda Askell</name>
    </author>
    <author>
      <name>Nicholas Schiefer</name>
    </author>
    <author>
      <name>Thomas I. Liao</name>
    </author>
    <author>
      <name>Kamilė Lukošiūtė</name>
    </author>
    <author>
      <name>Anna Chen</name>
    </author>
    <author>
      <name>Anna Goldie</name>
    </author>
    <author>
      <name>Azalia Mirhoseini</name>
    </author>
    <author>
      <name>Catherine Olsson</name>
    </author>
    <author>
      <name>Danny Hernandez</name>
    </author>
    <author>
      <name>Dawn Drain</name>
    </author>
    <author>
      <name>Dustin Li</name>
    </author>
    <author>
      <name>Eli Tran-Johnson</name>
    </author>
    <author>
      <name>Ethan Perez</name>
    </author>
    <author>
      <name>Jackson Kernion</name>
    </author>
    <author>
      <name>Jamie Kerr</name>
    </author>
    <author>
      <name>Jared Mueller</name>
    </author>
    <author>
      <name>Joshua Landau</name>
    </author>
    <author>
      <name>Kamal Ndousse</name>
    </author>
    <author>
      <name>Karina Nguyen</name>
    </author>
    <author>
      <name>Liane Lovitt</name>
    </author>
    <author>
      <name>Michael Sellitto</name>
    </author>
    <author>
      <name>Nelson Elhage</name>
    </author>
    <author>
      <name>Noemi Mercado</name>
    </author>
    <author>
      <name>Nova DasSarma</name>
    </author>
    <author>
      <name>Oliver Rausch</name>
    </author>
    <author>
      <name>Robert Lasenby</name>
    </author>
    <author>
      <name>Robin Larson</name>
    </author>
    <author>
      <name>Sam Ringer</name>
    </author>
    <author>
      <name>Sandipan Kundu</name>
    </author>
    <author>
      <name>Saurav Kadavath</name>
    </author>
    <author>
      <name>Scott Johnston</name>
    </author>
    <author>
      <name>Shauna Kravec</name>
    </author>
    <author>
      <name>Sheer El Showk</name>
    </author>
    <author>
      <name>Tamera Lanham</name>
    </author>
    <author>
      <name>Timothy Telleen-Lawton</name>
    </author>
    <author>
      <name>Tom Henighan</name>
    </author>
    <author>
      <name>Tristan Hume</name>
    </author>
    <author>
      <name>Yuntao Bai</name>
    </author>
    <author>
      <name>Zac Hatfield-Dodds</name>
    </author>
    <author>
      <name>Ben Mann</name>
    </author>
    <author>
      <name>Dario Amodei</name>
    </author>
    <author>
      <name>Nicholas Joseph</name>
    </author>
    <author>
      <name>Sam McCandlish</name>
    </author>
    <author>
      <name>Tom Brown</name>
    </author>
    <author>
      <name>Christopher Olah</name>
    </author>
    <author>
      <name>Jack Clark</name>
    </author>
    <author>
      <name>Samuel R. Bowman</name>
    </author>
    <author>
      <name>Jared Kaplan</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.09251v1</id>
    <title>Discovering Language Model Behaviors with Model-Written Evaluations</title>
    <updated>2022-12-19T05:13:52Z</updated>
    <link href="https://arxiv.org/abs/2212.09251v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2212.09251v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>As language models (LMs) scale, they develop many novel behaviors, good and bad, exacerbating the need to evaluate how they behave. Prior work creates evaluations with crowdwork (which is time-consuming and expensive) or existing data sources (which are not always available). Here, we automatically generate evaluations with LMs. We explore approaches with varying amounts of human effort, from instructing LMs to write yes/no questions to making complex Winogender schemas with multiple stages of LM-based generation and filtering. Crowdworkers rate the examples as highly relevant and agree with 90-100% of labels, sometimes more so than corresponding human-written datasets. We generate 154 datasets and discover new cases of inverse scaling where LMs get worse with size. Larger LMs repeat back a dialog user's preferred answer ("sycophancy") and express greater desire to pursue concerning goals like resource acquisition and goal preservation. We also find some of the first examples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF makes LMs worse. For example, RLHF makes LMs express stronger political views (on gun rights and immigration) and a greater desire to avoid shut down. Overall, LM-written evaluations are high-quality and let us quickly discover many novel LM behaviors.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2022-12-19T05:13:52Z</published>
    <arxiv:comment>for associated data visualizations, see https://www.evals.anthropic.com/model-written/ for full datasets, see https://github.com/anthropics/evals</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Ethan Perez</name>
    </author>
    <author>
      <name>Sam Ringer</name>
    </author>
    <author>
      <name>Kamilė Lukošiūtė</name>
    </author>
    <author>
      <name>Karina Nguyen</name>
    </author>
    <author>
      <name>Edwin Chen</name>
    </author>
    <author>
      <name>Scott Heiner</name>
    </author>
    <author>
      <name>Craig Pettit</name>
    </author>
    <author>
      <name>Catherine Olsson</name>
    </author>
    <author>
      <name>Sandipan Kundu</name>
    </author>
    <author>
      <name>Saurav Kadavath</name>
    </author>
    <author>
      <name>Andy Jones</name>
    </author>
    <author>
      <name>Anna Chen</name>
    </author>
    <author>
      <name>Ben Mann</name>
    </author>
    <author>
      <name>Brian Israel</name>
    </author>
    <author>
      <name>Bryan Seethor</name>
    </author>
    <author>
      <name>Cameron McKinnon</name>
    </author>
    <author>
      <name>Christopher Olah</name>
    </author>
    <author>
      <name>Da Yan</name>
    </author>
    <author>
      <name>Daniela Amodei</name>
    </author>
    <author>
      <name>Dario Amodei</name>
    </author>
    <author>
      <name>Dawn Drain</name>
    </author>
    <author>
      <name>Dustin Li</name>
    </author>
    <author>
      <name>Eli Tran-Johnson</name>
    </author>
    <author>
      <name>Guro Khundadze</name>
    </author>
    <author>
      <name>Jackson Kernion</name>
    </author>
    <author>
      <name>James Landis</name>
    </author>
    <author>
      <name>Jamie Kerr</name>
    </author>
    <author>
      <name>Jared Mueller</name>
    </author>
    <author>
      <name>Jeeyoon Hyun</name>
    </author>
    <author>
      <name>Joshua Landau</name>
    </author>
    <author>
      <name>Kamal Ndousse</name>
    </author>
    <author>
      <name>Landon Goldberg</name>
    </author>
    <author>
      <name>Liane Lovitt</name>
    </author>
    <author>
      <name>Martin Lucas</name>
    </author>
    <author>
      <name>Michael Sellitto</name>
    </author>
    <author>
      <name>Miranda Zhang</name>
    </author>
    <author>
      <name>Neerav Kingsland</name>
    </author>
    <author>
      <name>Nelson Elhage</name>
    </author>
    <author>
      <name>Nicholas Joseph</name>
    </author>
    <author>
      <name>Noemí Mercado</name>
    </author>
    <author>
      <name>Nova DasSarma</name>
    </author>
    <author>
      <name>Oliver Rausch</name>
    </author>
    <author>
      <name>Robin Larson</name>
    </author>
    <author>
      <name>Sam McCandlish</name>
    </author>
    <author>
      <name>Scott Johnston</name>
    </author>
    <author>
      <name>Shauna Kravec</name>
    </author>
    <author>
      <name>Sheer El Showk</name>
    </author>
    <author>
      <name>Tamera Lanham</name>
    </author>
    <author>
      <name>Timothy Telleen-Lawton</name>
    </author>
    <author>
      <name>Tom Brown</name>
    </author>
    <author>
      <name>Tom Henighan</name>
    </author>
    <author>
      <name>Tristan Hume</name>
    </author>
    <author>
      <name>Yuntao Bai</name>
    </author>
    <author>
      <name>Zac Hatfield-Dodds</name>
    </author>
    <author>
      <name>Jack Clark</name>
    </author>
    <author>
      <name>Samuel R. Bowman</name>
    </author>
    <author>
      <name>Amanda Askell</name>
    </author>
    <author>
      <name>Roger Grosse</name>
    </author>
    <author>
      <name>Danny Hernandez</name>
    </author>
    <author>
      <name>Deep Ganguli</name>
    </author>
    <author>
      <name>Evan Hubinger</name>
    </author>
    <author>
      <name>Nicholas Schiefer</name>
    </author>
    <author>
      <name>Jared Kaplan</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.08073v1</id>
    <title>Constitutional AI: Harmlessness from AI Feedback</title>
    <updated>2022-12-15T06:19:23Z</updated>
    <link href="https://arxiv.org/abs/2212.08073v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2212.08073v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2022-12-15T06:19:23Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Yuntao Bai</name>
    </author>
    <author>
      <name>Saurav Kadavath</name>
    </author>
    <author>
      <name>Sandipan Kundu</name>
    </author>
    <author>
      <name>Amanda Askell</name>
    </author>
    <author>
      <name>Jackson Kernion</name>
    </author>
    <author>
      <name>Andy Jones</name>
    </author>
    <author>
      <name>Anna Chen</name>
    </author>
    <author>
      <name>Anna Goldie</name>
    </author>
    <author>
      <name>Azalia Mirhoseini</name>
    </author>
    <author>
      <name>Cameron McKinnon</name>
    </author>
    <author>
      <name>Carol Chen</name>
    </author>
    <author>
      <name>Catherine Olsson</name>
    </author>
    <author>
      <name>Christopher Olah</name>
    </author>
    <author>
      <name>Danny Hernandez</name>
    </author>
    <author>
      <name>Dawn Drain</name>
    </author>
    <author>
      <name>Deep Ganguli</name>
    </author>
    <author>
      <name>Dustin Li</name>
    </author>
    <author>
      <name>Eli Tran-Johnson</name>
    </author>
    <author>
      <name>Ethan Perez</name>
    </author>
    <author>
      <name>Jamie Kerr</name>
    </author>
    <author>
      <name>Jared Mueller</name>
    </author>
    <author>
      <name>Jeffrey Ladish</name>
    </author>
    <author>
      <name>Joshua Landau</name>
    </author>
    <author>
      <name>Kamal Ndousse</name>
    </author>
    <author>
      <name>Kamile Lukosuite</name>
    </author>
    <author>
      <name>Liane Lovitt</name>
    </author>
    <author>
      <name>Michael Sellitto</name>
    </author>
    <author>
      <name>Nelson Elhage</name>
    </author>
    <author>
      <name>Nicholas Schiefer</name>
    </author>
    <author>
      <name>Noemi Mercado</name>
    </author>
    <author>
      <name>Nova DasSarma</name>
    </author>
    <author>
      <name>Robert Lasenby</name>
    </author>
    <author>
      <name>Robin Larson</name>
    </author>
    <author>
      <name>Sam Ringer</name>
    </author>
    <author>
      <name>Scott Johnston</name>
    </author>
    <author>
      <name>Shauna Kravec</name>
    </author>
    <author>
      <name>Sheer El Showk</name>
    </author>
    <author>
      <name>Stanislav Fort</name>
    </author>
    <author>
      <name>Tamera Lanham</name>
    </author>
    <author>
      <name>Timothy Telleen-Lawton</name>
    </author>
    <author>
      <name>Tom Conerly</name>
    </author>
    <author>
      <name>Tom Henighan</name>
    </author>
    <author>
      <name>Tristan Hume</name>
    </author>
    <author>
      <name>Samuel R. Bowman</name>
    </author>
    <author>
      <name>Zac Hatfield-Dodds</name>
    </author>
    <author>
      <name>Ben Mann</name>
    </author>
    <author>
      <name>Dario Amodei</name>
    </author>
    <author>
      <name>Nicholas Joseph</name>
    </author>
    <author>
      <name>Sam McCandlish</name>
    </author>
    <author>
      <name>Tom Brown</name>
    </author>
    <author>
      <name>Jared Kaplan</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.03540v2</id>
    <title>Measuring Progress on Scalable Oversight for Large Language Models</title>
    <updated>2022-11-11T20:17:18Z</updated>
    <link href="https://arxiv.org/abs/2211.03540v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2211.03540v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>Developing safe and useful general-purpose AI systems will require us to make progress on scalable oversight: the problem of supervising systems that potentially outperform us on most skills relevant to the task at hand. Empirical work on this problem is not straightforward, since we do not yet have systems that broadly exceed our abilities. This paper discusses one of the major ways we think about this problem, with a focus on ways it can be studied empirically. We first present an experimental design centered on tasks for which human specialists succeed but unaided humans and current general AI systems fail. We then present a proof-of-concept experiment meant to demonstrate a key feature of this experimental design and show its viability with two question-answering tasks: MMLU and time-limited QuALITY. On these tasks, we find that human participants who interact with an unreliable large-language-model dialog assistant through chat -- a trivial baseline strategy for scalable oversight -- substantially outperform both the model alone and their own unaided performance. These results are an encouraging sign that scalable oversight will be tractable to study with present models and bolster recent findings that large language models can productively assist humans with difficult tasks.</summary>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2022-11-04T17:03:49Z</published>
    <arxiv:comment>v2 fixes a few typos from v1</arxiv:comment>
    <arxiv:primary_category term="cs.HC"/>
    <author>
      <name>Samuel R. Bowman</name>
    </author>
    <author>
      <name>Jeeyoon Hyun</name>
    </author>
    <author>
      <name>Ethan Perez</name>
    </author>
    <author>
      <name>Edwin Chen</name>
    </author>
    <author>
      <name>Craig Pettit</name>
    </author>
    <author>
      <name>Scott Heiner</name>
    </author>
    <author>
      <name>Kamilė Lukošiūtė</name>
    </author>
    <author>
      <name>Amanda Askell</name>
    </author>
    <author>
      <name>Andy Jones</name>
    </author>
    <author>
      <name>Anna Chen</name>
    </author>
    <author>
      <name>Anna Goldie</name>
    </author>
    <author>
      <name>Azalia Mirhoseini</name>
    </author>
    <author>
      <name>Cameron McKinnon</name>
    </author>
    <author>
      <name>Christopher Olah</name>
    </author>
    <author>
      <name>Daniela Amodei</name>
    </author>
    <author>
      <name>Dario Amodei</name>
    </author>
    <author>
      <name>Dawn Drain</name>
    </author>
    <author>
      <name>Dustin Li</name>
    </author>
    <author>
      <name>Eli Tran-Johnson</name>
    </author>
    <author>
      <name>Jackson Kernion</name>
    </author>
    <author>
      <name>Jamie Kerr</name>
    </author>
    <author>
      <name>Jared Mueller</name>
    </author>
    <author>
      <name>Jeffrey Ladish</name>
    </author>
    <author>
      <name>Joshua Landau</name>
    </author>
    <author>
      <name>Kamal Ndousse</name>
    </author>
    <author>
      <name>Liane Lovitt</name>
    </author>
    <author>
      <name>Nelson Elhage</name>
    </author>
    <author>
      <name>Nicholas Schiefer</name>
    </author>
    <author>
      <name>Nicholas Joseph</name>
    </author>
    <author>
      <name>Noemí Mercado</name>
    </author>
    <author>
      <name>Nova DasSarma</name>
    </author>
    <author>
      <name>Robin Larson</name>
    </author>
    <author>
      <name>Sam McCandlish</name>
    </author>
    <author>
      <name>Sandipan Kundu</name>
    </author>
    <author>
      <name>Scott Johnston</name>
    </author>
    <author>
      <name>Shauna Kravec</name>
    </author>
    <author>
      <name>Sheer El Showk</name>
    </author>
    <author>
      <name>Stanislav Fort</name>
    </author>
    <author>
      <name>Timothy Telleen-Lawton</name>
    </author>
    <author>
      <name>Tom Brown</name>
    </author>
    <author>
      <name>Tom Henighan</name>
    </author>
    <author>
      <name>Tristan Hume</name>
    </author>
    <author>
      <name>Yuntao Bai</name>
    </author>
    <author>
      <name>Zac Hatfield-Dodds</name>
    </author>
    <author>
      <name>Ben Mann</name>
    </author>
    <author>
      <name>Jared Kaplan</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.11895v1</id>
    <title>In-context Learning and Induction Heads</title>
    <updated>2022-09-24T00:43:19Z</updated>
    <link href="https://arxiv.org/abs/2209.11895v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2209.11895v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>"Induction heads" are attention heads that implement a simple algorithm to complete token sequences like [A][B] ... [A] -&gt; [B]. In this work, we present preliminary and indirect evidence for a hypothesis that induction heads might constitute the mechanism for the majority of all "in-context learning" in large transformer models (i.e. decreasing loss at increasing token indices). We find that induction heads develop at precisely the same point as a sudden sharp increase in in-context learning ability, visible as a bump in the training loss. We present six complementary lines of evidence, arguing that induction heads may be the mechanistic source of general in-context learning in transformer models of any size. For small attention-only models, we present strong, causal evidence; for larger models with MLPs, we present correlational evidence.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2022-09-24T00:43:19Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Catherine Olsson</name>
    </author>
    <author>
      <name>Nelson Elhage</name>
    </author>
    <author>
      <name>Neel Nanda</name>
    </author>
    <author>
      <name>Nicholas Joseph</name>
    </author>
    <author>
      <name>Nova DasSarma</name>
    </author>
    <author>
      <name>Tom Henighan</name>
    </author>
    <author>
      <name>Ben Mann</name>
    </author>
    <author>
      <name>Amanda Askell</name>
    </author>
    <author>
      <name>Yuntao Bai</name>
    </author>
    <author>
      <name>Anna Chen</name>
    </author>
    <author>
      <name>Tom Conerly</name>
    </author>
    <author>
      <name>Dawn Drain</name>
    </author>
    <author>
      <name>Deep Ganguli</name>
    </author>
    <author>
      <name>Zac Hatfield-Dodds</name>
    </author>
    <author>
      <name>Danny Hernandez</name>
    </author>
    <author>
      <name>Scott Johnston</name>
    </author>
    <author>
      <name>Andy Jones</name>
    </author>
    <author>
      <name>Jackson Kernion</name>
    </author>
    <author>
      <name>Liane Lovitt</name>
    </author>
    <author>
      <name>Kamal Ndousse</name>
    </author>
    <author>
      <name>Dario Amodei</name>
    </author>
    <author>
      <name>Tom Brown</name>
    </author>
    <author>
      <name>Jack Clark</name>
    </author>
    <author>
      <name>Jared Kaplan</name>
    </author>
    <author>
      <name>Sam McCandlish</name>
    </author>
    <author>
      <name>Chris Olah</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.10652v1</id>
    <title>Toy Models of Superposition</title>
    <updated>2022-09-21T20:49:26Z</updated>
    <link href="https://arxiv.org/abs/2209.10652v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2209.10652v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Neural networks often pack many unrelated concepts into a single neuron - a puzzling phenomenon known as 'polysemanticity' which makes interpretability much more challenging. This paper provides a toy model where polysemanticity can be fully understood, arising as a result of models storing additional sparse features in "superposition." We demonstrate the existence of a phase change, a surprising connection to the geometry of uniform polytopes, and evidence of a link to adversarial examples. We also discuss potential implications for mechanistic interpretability.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2022-09-21T20:49:26Z</published>
    <arxiv:comment>Also available at https://transformer-circuits.pub/2022/toy_model/index.html</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Nelson Elhage</name>
    </author>
    <author>
      <name>Tristan Hume</name>
    </author>
    <author>
      <name>Catherine Olsson</name>
    </author>
    <author>
      <name>Nicholas Schiefer</name>
    </author>
    <author>
      <name>Tom Henighan</name>
    </author>
    <author>
      <name>Shauna Kravec</name>
    </author>
    <author>
      <name>Zac Hatfield-Dodds</name>
    </author>
    <author>
      <name>Robert Lasenby</name>
    </author>
    <author>
      <name>Dawn Drain</name>
    </author>
    <author>
      <name>Carol Chen</name>
    </author>
    <author>
      <name>Roger Grosse</name>
    </author>
    <author>
      <name>Sam McCandlish</name>
    </author>
    <author>
      <name>Jared Kaplan</name>
    </author>
    <author>
      <name>Dario Amodei</name>
    </author>
    <author>
      <name>Martin Wattenberg</name>
    </author>
    <author>
      <name>Christopher Olah</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.07858v2</id>
    <title>Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned</title>
    <updated>2022-11-22T19:12:57Z</updated>
    <link href="https://arxiv.org/abs/2209.07858v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2209.07858v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>We describe our early efforts to red team language models in order to simultaneously discover, measure, and attempt to reduce their potentially harmful outputs. We make three main contributions. First, we investigate scaling behaviors for red teaming across 3 model sizes (2.7B, 13B, and 52B parameters) and 4 model types: a plain language model (LM); an LM prompted to be helpful, honest, and harmless; an LM with rejection sampling; and a model trained to be helpful and harmless using reinforcement learning from human feedback (RLHF). We find that the RLHF models are increasingly difficult to red team as they scale, and we find a flat trend with scale for the other model types. Second, we release our dataset of 38,961 red team attacks for others to analyze and learn from. We provide our own analysis of the data and find a variety of harmful outputs, which range from offensive language to more subtly harmful non-violent unethical outputs. Third, we exhaustively describe our instructions, processes, statistical methodologies, and uncertainty about red teaming. We hope that this transparency accelerates our ability to work together as a community in order to develop shared norms, practices, and technical standards for how to red team language models.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <published>2022-08-23T23:37:14Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Deep Ganguli</name>
    </author>
    <author>
      <name>Liane Lovitt</name>
    </author>
    <author>
      <name>Jackson Kernion</name>
    </author>
    <author>
      <name>Amanda Askell</name>
    </author>
    <author>
      <name>Yuntao Bai</name>
    </author>
    <author>
      <name>Saurav Kadavath</name>
    </author>
    <author>
      <name>Ben Mann</name>
    </author>
    <author>
      <name>Ethan Perez</name>
    </author>
    <author>
      <name>Nicholas Schiefer</name>
    </author>
    <author>
      <name>Kamal Ndousse</name>
    </author>
    <author>
      <name>Andy Jones</name>
    </author>
    <author>
      <name>Sam Bowman</name>
    </author>
    <author>
      <name>Anna Chen</name>
    </author>
    <author>
      <name>Tom Conerly</name>
    </author>
    <author>
      <name>Nova DasSarma</name>
    </author>
    <author>
      <name>Dawn Drain</name>
    </author>
    <author>
      <name>Nelson Elhage</name>
    </author>
    <author>
      <name>Sheer El-Showk</name>
    </author>
    <author>
      <name>Stanislav Fort</name>
    </author>
    <author>
      <name>Zac Hatfield-Dodds</name>
    </author>
    <author>
      <name>Tom Henighan</name>
    </author>
    <author>
      <name>Danny Hernandez</name>
    </author>
    <author>
      <name>Tristan Hume</name>
    </author>
    <author>
      <name>Josh Jacobson</name>
    </author>
    <author>
      <name>Scott Johnston</name>
    </author>
    <author>
      <name>Shauna Kravec</name>
    </author>
    <author>
      <name>Catherine Olsson</name>
    </author>
    <author>
      <name>Sam Ringer</name>
    </author>
    <author>
      <name>Eli Tran-Johnson</name>
    </author>
    <author>
      <name>Dario Amodei</name>
    </author>
    <author>
      <name>Tom Brown</name>
    </author>
    <author>
      <name>Nicholas Joseph</name>
    </author>
    <author>
      <name>Sam McCandlish</name>
    </author>
    <author>
      <name>Chris Olah</name>
    </author>
    <author>
      <name>Jared Kaplan</name>
    </author>
    <author>
      <name>Jack Clark</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.05221v4</id>
    <title>Language Models (Mostly) Know What They Know</title>
    <updated>2022-11-21T16:38:35Z</updated>
    <link href="https://arxiv.org/abs/2207.05221v4" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2207.05221v4" rel="related" type="application/pdf" title="pdf"/>
    <summary>We study whether language models can evaluate the validity of their own claims and predict which questions they will be able to answer correctly. We first show that larger models are well-calibrated on diverse multiple choice and true/false questions when they are provided in the right format. Thus we can approach self-evaluation on open-ended sampling tasks by asking models to first propose answers, and then to evaluate the probability "P(True)" that their answers are correct. We find encouraging performance, calibration, and scaling for P(True) on a diverse array of tasks. Performance at self-evaluation further improves when we allow models to consider many of their own samples before predicting the validity of one specific possibility. Next, we investigate whether models can be trained to predict "P(IK)", the probability that "I know" the answer to a question, without reference to any particular proposed answer. Models perform well at predicting P(IK) and partially generalize across tasks, though they struggle with calibration of P(IK) on new tasks. The predicted P(IK) probabilities also increase appropriately in the presence of relevant source materials in the context, and in the presence of hints towards the solution of mathematical word problems. We hope these observations lay the groundwork for training more honest models, and for investigating how honesty generalizes to cases where models are trained on objectives other than the imitation of human writing.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2022-07-11T22:59:39Z</published>
    <arxiv:comment>23+17 pages; refs added, typos fixed</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Saurav Kadavath</name>
    </author>
    <author>
      <name>Tom Conerly</name>
    </author>
    <author>
      <name>Amanda Askell</name>
    </author>
    <author>
      <name>Tom Henighan</name>
    </author>
    <author>
      <name>Dawn Drain</name>
    </author>
    <author>
      <name>Ethan Perez</name>
    </author>
    <author>
      <name>Nicholas Schiefer</name>
    </author>
    <author>
      <name>Zac Hatfield-Dodds</name>
    </author>
    <author>
      <name>Nova DasSarma</name>
    </author>
    <author>
      <name>Eli Tran-Johnson</name>
    </author>
    <author>
      <name>Scott Johnston</name>
    </author>
    <author>
      <name>Sheer El-Showk</name>
    </author>
    <author>
      <name>Andy Jones</name>
    </author>
    <author>
      <name>Nelson Elhage</name>
    </author>
    <author>
      <name>Tristan Hume</name>
    </author>
    <author>
      <name>Anna Chen</name>
    </author>
    <author>
      <name>Yuntao Bai</name>
    </author>
    <author>
      <name>Sam Bowman</name>
    </author>
    <author>
      <name>Stanislav Fort</name>
    </author>
    <author>
      <name>Deep Ganguli</name>
    </author>
    <author>
      <name>Danny Hernandez</name>
    </author>
    <author>
      <name>Josh Jacobson</name>
    </author>
    <author>
      <name>Jackson Kernion</name>
    </author>
    <author>
      <name>Shauna Kravec</name>
    </author>
    <author>
      <name>Liane Lovitt</name>
    </author>
    <author>
      <name>Kamal Ndousse</name>
    </author>
    <author>
      <name>Catherine Olsson</name>
    </author>
    <author>
      <name>Sam Ringer</name>
    </author>
    <author>
      <name>Dario Amodei</name>
    </author>
    <author>
      <name>Tom Brown</name>
    </author>
    <author>
      <name>Jack Clark</name>
    </author>
    <author>
      <name>Nicholas Joseph</name>
    </author>
    <author>
      <name>Ben Mann</name>
    </author>
    <author>
      <name>Sam McCandlish</name>
    </author>
    <author>
      <name>Chris Olah</name>
    </author>
    <author>
      <name>Jared Kaplan</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.10487v1</id>
    <title>Scaling Laws and Interpretability of Learning from Repeated Data</title>
    <updated>2022-05-21T02:14:27Z</updated>
    <link href="https://arxiv.org/abs/2205.10487v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2205.10487v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Recent large language models have been trained on vast datasets, but also often on repeated data, either intentionally for the purpose of upweighting higher quality data, or unintentionally because data deduplication is not perfect and the model is exposed to repeated data at the sentence, paragraph, or document level. Some works have reported substantial negative performance effects of this repeated data. In this paper we attempt to study repeated data systematically and to understand its effects mechanistically. To do this, we train a family of models where most of the data is unique but a small fraction of it is repeated many times. We find a strong double descent phenomenon, in which repeated data can lead test loss to increase midway through training. A predictable range of repetition frequency leads to surprisingly severe degradation in performance. For instance, performance of an 800M parameter model can be degraded to that of a 2x smaller model (400M params) by repeating 0.1% of the data 100 times, despite the other 90% of the training tokens remaining unique. We suspect there is a range in the middle where the data can be memorized and doing so consumes a large fraction of the model's capacity, and this may be where the peak of degradation occurs. Finally, we connect these observations to recent mechanistic interpretability work - attempting to reverse engineer the detailed computations performed by the model - by showing that data repetition disproportionately damages copying and internal structures associated with generalization, such as induction heads, providing a possible mechanism for the shift from generalization to memorization. Taken together, these results provide a hypothesis for why repeating a relatively small fraction of data in large language models could lead to disproportionately large harms to performance.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2022-05-21T02:14:27Z</published>
    <arxiv:comment>23 pages, 22 figures</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Danny Hernandez</name>
    </author>
    <author>
      <name>Tom Brown</name>
    </author>
    <author>
      <name>Tom Conerly</name>
    </author>
    <author>
      <name>Nova DasSarma</name>
    </author>
    <author>
      <name>Dawn Drain</name>
    </author>
    <author>
      <name>Sheer El-Showk</name>
    </author>
    <author>
      <name>Nelson Elhage</name>
    </author>
    <author>
      <name>Zac Hatfield-Dodds</name>
    </author>
    <author>
      <name>Tom Henighan</name>
    </author>
    <author>
      <name>Tristan Hume</name>
    </author>
    <author>
      <name>Scott Johnston</name>
    </author>
    <author>
      <name>Ben Mann</name>
    </author>
    <author>
      <name>Chris Olah</name>
    </author>
    <author>
      <name>Catherine Olsson</name>
    </author>
    <author>
      <name>Dario Amodei</name>
    </author>
    <author>
      <name>Nicholas Joseph</name>
    </author>
    <author>
      <name>Jared Kaplan</name>
    </author>
    <author>
      <name>Sam McCandlish</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.05862v1</id>
    <title>Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback</title>
    <updated>2022-04-12T15:02:38Z</updated>
    <link href="https://arxiv.org/abs/2204.05862v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2204.05862v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We apply preference modeling and reinforcement learning from human feedback (RLHF) to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efficiently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2022-04-12T15:02:38Z</published>
    <arxiv:comment>Data available at https://github.com/anthropics/hh-rlhf</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Yuntao Bai</name>
    </author>
    <author>
      <name>Andy Jones</name>
    </author>
    <author>
      <name>Kamal Ndousse</name>
    </author>
    <author>
      <name>Amanda Askell</name>
    </author>
    <author>
      <name>Anna Chen</name>
    </author>
    <author>
      <name>Nova DasSarma</name>
    </author>
    <author>
      <name>Dawn Drain</name>
    </author>
    <author>
      <name>Stanislav Fort</name>
    </author>
    <author>
      <name>Deep Ganguli</name>
    </author>
    <author>
      <name>Tom Henighan</name>
    </author>
    <author>
      <name>Nicholas Joseph</name>
    </author>
    <author>
      <name>Saurav Kadavath</name>
    </author>
    <author>
      <name>Jackson Kernion</name>
    </author>
    <author>
      <name>Tom Conerly</name>
    </author>
    <author>
      <name>Sheer El-Showk</name>
    </author>
    <author>
      <name>Nelson Elhage</name>
    </author>
    <author>
      <name>Zac Hatfield-Dodds</name>
    </author>
    <author>
      <name>Danny Hernandez</name>
    </author>
    <author>
      <name>Tristan Hume</name>
    </author>
    <author>
      <name>Scott Johnston</name>
    </author>
    <author>
      <name>Shauna Kravec</name>
    </author>
    <author>
      <name>Liane Lovitt</name>
    </author>
    <author>
      <name>Neel Nanda</name>
    </author>
    <author>
      <name>Catherine Olsson</name>
    </author>
    <author>
      <name>Dario Amodei</name>
    </author>
    <author>
      <name>Tom Brown</name>
    </author>
    <author>
      <name>Jack Clark</name>
    </author>
    <author>
      <name>Sam McCandlish</name>
    </author>
    <author>
      <name>Chris Olah</name>
    </author>
    <author>
      <name>Ben Mann</name>
    </author>
    <author>
      <name>Jared Kaplan</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.07785v2</id>
    <title>Predictability and Surprise in Large Generative Models</title>
    <updated>2022-10-03T21:00:42Z</updated>
    <link href="https://arxiv.org/abs/2202.07785v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2202.07785v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>Large-scale pre-training has recently emerged as a technique for creating capable, general purpose, generative models such as GPT-3, Megatron-Turing NLG, Gopher, and many others. In this paper, we highlight a counterintuitive property of such models and discuss the policy implications of this property. Namely, these generative models have an unusual combination of predictable loss on a broad training distribution (as embodied in their "scaling laws"), and unpredictable specific capabilities, inputs, and outputs. We believe that the high-level predictability and appearance of useful capabilities drives rapid development of such models, while the unpredictable qualities make it difficult to anticipate the consequences of model deployment. We go through examples of how this combination can lead to socially harmful behavior with examples from the literature and real world observations, and we also perform two novel experiments to illustrate our point about harms from unpredictability. Furthermore, we analyze how these conflicting properties combine to give model developers various motivations for deploying these models, and challenges that can hinder deployment. We conclude with a list of possible interventions the AI community may take to increase the chance of these models having a beneficial impact. We intend this paper to be useful to policymakers who want to understand and regulate AI systems, technologists who care about the potential policy impact of their work, and academics who want to analyze, critique, and potentially develop large generative models.</summary>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <published>2022-02-15T23:21:23Z</published>
    <arxiv:comment>Updated to reflect the version submitted (and accepted) to ACM FAccT '22. This update incorporates feedback from peer-review and fixes minor typos. See open access FAccT conference version at: https://dl.acm.org/doi/abs/10.1145/3531146.3533229</arxiv:comment>
    <arxiv:primary_category term="cs.CY"/>
    <author>
      <name>Deep Ganguli</name>
    </author>
    <author>
      <name>Danny Hernandez</name>
    </author>
    <author>
      <name>Liane Lovitt</name>
    </author>
    <author>
      <name>Nova DasSarma</name>
    </author>
    <author>
      <name>Tom Henighan</name>
    </author>
    <author>
      <name>Andy Jones</name>
    </author>
    <author>
      <name>Nicholas Joseph</name>
    </author>
    <author>
      <name>Jackson Kernion</name>
    </author>
    <author>
      <name>Ben Mann</name>
    </author>
    <author>
      <name>Amanda Askell</name>
    </author>
    <author>
      <name>Yuntao Bai</name>
    </author>
    <author>
      <name>Anna Chen</name>
    </author>
    <author>
      <name>Tom Conerly</name>
    </author>
    <author>
      <name>Dawn Drain</name>
    </author>
    <author>
      <name>Nelson Elhage</name>
    </author>
    <author>
      <name>Sheer El Showk</name>
    </author>
    <author>
      <name>Stanislav Fort</name>
    </author>
    <author>
      <name>Zac Hatfield-Dodds</name>
    </author>
    <author>
      <name>Scott Johnston</name>
    </author>
    <author>
      <name>Shauna Kravec</name>
    </author>
    <author>
      <name>Neel Nanda</name>
    </author>
    <author>
      <name>Kamal Ndousse</name>
    </author>
    <author>
      <name>Catherine Olsson</name>
    </author>
    <author>
      <name>Daniela Amodei</name>
    </author>
    <author>
      <name>Dario Amodei</name>
    </author>
    <author>
      <name>Tom Brown</name>
    </author>
    <author>
      <name>Jared Kaplan</name>
    </author>
    <author>
      <name>Sam McCandlish</name>
    </author>
    <author>
      <name>Chris Olah</name>
    </author>
    <author>
      <name>Jack Clark</name>
    </author>
    <arxiv:doi>10.1145/3531146.3533229</arxiv:doi>
    <link rel="related" href="https://doi.org/10.1145/3531146.3533229" title="doi"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.00861v3</id>
    <title>A General Language Assistant as a Laboratory for Alignment</title>
    <updated>2021-12-09T21:40:22Z</updated>
    <link href="https://arxiv.org/abs/2112.00861v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2112.00861v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>Given the broad capabilities of large language models, it should be possible to work towards a general-purpose, text-based assistant that is aligned with human values, meaning that it is helpful, honest, and harmless. As an initial foray in this direction we study simple baseline techniques and evaluations, such as prompting. We find that the benefits from modest interventions increase with model size, generalize to a variety of alignment evaluations, and do not compromise the performance of large models. Next we investigate scaling trends for several training objectives relevant to alignment, comparing imitation learning, binary discrimination, and ranked preference modeling. We find that ranked preference modeling performs much better than imitation learning, and often scales more favorably with model size. In contrast, binary discrimination typically performs and scales very similarly to imitation learning. Finally we study a `preference model pre-training' stage of training, with the goal of improving sample efficiency when finetuning on human preferences.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2021-12-01T22:24:34Z</published>
    <arxiv:comment>26+19 pages; v2 typos fixed, refs added, figure scale / colors fixed; v3 correct very non-standard TruthfulQA formatting and metric, alignment implications slightly improved</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Amanda Askell</name>
    </author>
    <author>
      <name>Yuntao Bai</name>
    </author>
    <author>
      <name>Anna Chen</name>
    </author>
    <author>
      <name>Dawn Drain</name>
    </author>
    <author>
      <name>Deep Ganguli</name>
    </author>
    <author>
      <name>Tom Henighan</name>
    </author>
    <author>
      <name>Andy Jones</name>
    </author>
    <author>
      <name>Nicholas Joseph</name>
    </author>
    <author>
      <name>Ben Mann</name>
    </author>
    <author>
      <name>Nova DasSarma</name>
    </author>
    <author>
      <name>Nelson Elhage</name>
    </author>
    <author>
      <name>Zac Hatfield-Dodds</name>
    </author>
    <author>
      <name>Danny Hernandez</name>
    </author>
    <author>
      <name>Jackson Kernion</name>
    </author>
    <author>
      <name>Kamal Ndousse</name>
    </author>
    <author>
      <name>Catherine Olsson</name>
    </author>
    <author>
      <name>Dario Amodei</name>
    </author>
    <author>
      <name>Tom Brown</name>
    </author>
    <author>
      <name>Jack Clark</name>
    </author>
    <author>
      <name>Sam McCandlish</name>
    </author>
    <author>
      <name>Chris Olah</name>
    </author>
    <author>
      <name>Jared Kaplan</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.03374v2</id>
    <title>Evaluating Large Language Models Trained on Code</title>
    <updated>2021-07-14T17:16:02Z</updated>
    <link href="https://arxiv.org/abs/2107.03374v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2107.03374v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2021-07-07T17:41:24Z</published>
    <arxiv:comment>corrected typos, added references, added authors, added acknowledgements</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Mark Chen</name>
    </author>
    <author>
      <name>Jerry Tworek</name>
    </author>
    <author>
      <name>Heewoo Jun</name>
    </author>
    <author>
      <name>Qiming Yuan</name>
    </author>
    <author>
      <name>Henrique Ponde de Oliveira Pinto</name>
    </author>
    <author>
      <name>Jared Kaplan</name>
    </author>
    <author>
      <name>Harri Edwards</name>
    </author>
    <author>
      <name>Yuri Burda</name>
    </author>
    <author>
      <name>Nicholas Joseph</name>
    </author>
    <author>
      <name>Greg Brockman</name>
    </author>
    <author>
      <name>Alex Ray</name>
    </author>
    <author>
      <name>Raul Puri</name>
    </author>
    <author>
      <name>Gretchen Krueger</name>
    </author>
    <author>
      <name>Michael Petrov</name>
    </author>
    <author>
      <name>Heidy Khlaaf</name>
    </author>
    <author>
      <name>Girish Sastry</name>
    </author>
    <author>
      <name>Pamela Mishkin</name>
    </author>
    <author>
      <name>Brooke Chan</name>
    </author>
    <author>
      <name>Scott Gray</name>
    </author>
    <author>
      <name>Nick Ryder</name>
    </author>
    <author>
      <name>Mikhail Pavlov</name>
    </author>
    <author>
      <name>Alethea Power</name>
    </author>
    <author>
      <name>Lukasz Kaiser</name>
    </author>
    <author>
      <name>Mohammad Bavarian</name>
    </author>
    <author>
      <name>Clemens Winter</name>
    </author>
    <author>
      <name>Philippe Tillet</name>
    </author>
    <author>
      <name>Felipe Petroski Such</name>
    </author>
    <author>
      <name>Dave Cummings</name>
    </author>
    <author>
      <name>Matthias Plappert</name>
    </author>
    <author>
      <name>Fotios Chantzis</name>
    </author>
    <author>
      <name>Elizabeth Barnes</name>
    </author>
    <author>
      <name>Ariel Herbert-Voss</name>
    </author>
    <author>
      <name>William Hebgen Guss</name>
    </author>
    <author>
      <name>Alex Nichol</name>
    </author>
    <author>
      <name>Alex Paino</name>
    </author>
    <author>
      <name>Nikolas Tezak</name>
    </author>
    <author>
      <name>Jie Tang</name>
    </author>
    <author>
      <name>Igor Babuschkin</name>
    </author>
    <author>
      <name>Suchir Balaji</name>
    </author>
    <author>
      <name>Shantanu Jain</name>
    </author>
    <author>
      <name>William Saunders</name>
    </author>
    <author>
      <name>Christopher Hesse</name>
    </author>
    <author>
      <name>Andrew N. Carr</name>
    </author>
    <author>
      <name>Jan Leike</name>
    </author>
    <author>
      <name>Josh Achiam</name>
    </author>
    <author>
      <name>Vedant Misra</name>
    </author>
    <author>
      <name>Evan Morikawa</name>
    </author>
    <author>
      <name>Alec Radford</name>
    </author>
    <author>
      <name>Matthew Knight</name>
    </author>
    <author>
      <name>Miles Brundage</name>
    </author>
    <author>
      <name>Mira Murati</name>
    </author>
    <author>
      <name>Katie Mayer</name>
    </author>
    <author>
      <name>Peter Welinder</name>
    </author>
    <author>
      <name>Bob McGrew</name>
    </author>
    <author>
      <name>Dario Amodei</name>
    </author>
    <author>
      <name>Sam McCandlish</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Wojciech Zaremba</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.14701v2</id>
    <title>Scaling Laws for Autoregressive Generative Modeling</title>
    <updated>2020-11-06T04:16:36Z</updated>
    <link href="https://arxiv.org/abs/2010.14701v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2010.14701v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>We identify empirical scaling laws for the cross-entropy loss in four domains: generative image modeling, video modeling, multimodal image$\leftrightarrow$text models, and mathematical problem solving. In all cases autoregressive Transformers smoothly improve in performance as model size and compute budgets increase, following a power-law plus constant scaling law. The optimal model size also depends on the compute budget through a power-law, with exponents that are nearly universal across all data domains.
  The cross-entropy loss has an information theoretic interpretation as $S($True$) + D_{\mathrm{KL}}($True$||$Model$)$, and the empirical scaling laws suggest a prediction for both the true data distribution's entropy and the KL divergence between the true and model distributions. With this interpretation, billion-parameter Transformers are nearly perfect models of the YFCC100M image distribution downsampled to an $8\times 8$ resolution, and we can forecast the model size needed to achieve any given reducible loss (ie $D_{\mathrm{KL}}$) in nats/image for other resolutions.
  We find a number of additional scaling laws in specific domains: (a) we identify a scaling relation for the mutual information between captions and images in multimodal models, and show how to answer the question "Is a picture worth a thousand words?"; (b) in the case of mathematical problem solving, we identify scaling laws for model performance when extrapolating beyond the training distribution; (c) we finetune generative image models for ImageNet classification and find smooth scaling of the classification loss and error rate, even as the generative loss levels off. Taken together, these results strengthen the case that scaling laws have important implications for neural network performance, including on downstream tasks.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2020-10-28T02:17:24Z</published>
    <arxiv:comment>20+17 pages, 33 figures; added appendix with additional language results</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Tom Henighan</name>
    </author>
    <author>
      <name>Jared Kaplan</name>
    </author>
    <author>
      <name>Mor Katz</name>
    </author>
    <author>
      <name>Mark Chen</name>
    </author>
    <author>
      <name>Christopher Hesse</name>
    </author>
    <author>
      <name>Jacob Jackson</name>
    </author>
    <author>
      <name>Heewoo Jun</name>
    </author>
    <author>
      <name>Tom B. Brown</name>
    </author>
    <author>
      <name>Prafulla Dhariwal</name>
    </author>
    <author>
      <name>Scott Gray</name>
    </author>
    <author>
      <name>Chris Hallacy</name>
    </author>
    <author>
      <name>Benjamin Mann</name>
    </author>
    <author>
      <name>Alec Radford</name>
    </author>
    <author>
      <name>Aditya Ramesh</name>
    </author>
    <author>
      <name>Nick Ryder</name>
    </author>
    <author>
      <name>Daniel M. Ziegler</name>
    </author>
    <author>
      <name>John Schulman</name>
    </author>
    <author>
      <name>Dario Amodei</name>
    </author>
    <author>
      <name>Sam McCandlish</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.01325v3</id>
    <title>Learning to summarize from human feedback</title>
    <updated>2022-02-15T19:09:36Z</updated>
    <link href="https://arxiv.org/abs/2009.01325v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2009.01325v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>As language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task. For example, summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about -- summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences. We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the TL;DR dataset of Reddit posts and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to CNN/DM news articles, producing summaries nearly as good as the human reference without any news-specific fine-tuning. We conduct extensive analyses to understand our human feedback dataset and fine-tuned models We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans. We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2020-09-02T19:54:41Z</published>
    <arxiv:comment>NeurIPS 2020</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Nisan Stiennon</name>
    </author>
    <author>
      <name>Long Ouyang</name>
    </author>
    <author>
      <name>Jeff Wu</name>
    </author>
    <author>
      <name>Daniel M. Ziegler</name>
    </author>
    <author>
      <name>Ryan Lowe</name>
    </author>
    <author>
      <name>Chelsea Voss</name>
    </author>
    <author>
      <name>Alec Radford</name>
    </author>
    <author>
      <name>Dario Amodei</name>
    </author>
    <author>
      <name>Paul Christiano</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.14165v4</id>
    <title>Language Models are Few-Shot Learners</title>
    <updated>2020-07-22T19:47:17Z</updated>
    <link href="https://arxiv.org/abs/2005.14165v4" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2005.14165v4" rel="related" type="application/pdf" title="pdf"/>
    <summary>Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2020-05-28T17:29:03Z</published>
    <arxiv:comment>40+32 pages</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Tom B. Brown</name>
    </author>
    <author>
      <name>Benjamin Mann</name>
    </author>
    <author>
      <name>Nick Ryder</name>
    </author>
    <author>
      <name>Melanie Subbiah</name>
    </author>
    <author>
      <name>Jared Kaplan</name>
    </author>
    <author>
      <name>Prafulla Dhariwal</name>
    </author>
    <author>
      <name>Arvind Neelakantan</name>
    </author>
    <author>
      <name>Pranav Shyam</name>
    </author>
    <author>
      <name>Girish Sastry</name>
    </author>
    <author>
      <name>Amanda Askell</name>
    </author>
    <author>
      <name>Sandhini Agarwal</name>
    </author>
    <author>
      <name>Ariel Herbert-Voss</name>
    </author>
    <author>
      <name>Gretchen Krueger</name>
    </author>
    <author>
      <name>Tom Henighan</name>
    </author>
    <author>
      <name>Rewon Child</name>
    </author>
    <author>
      <name>Aditya Ramesh</name>
    </author>
    <author>
      <name>Daniel M. Ziegler</name>
    </author>
    <author>
      <name>Jeffrey Wu</name>
    </author>
    <author>
      <name>Clemens Winter</name>
    </author>
    <author>
      <name>Christopher Hesse</name>
    </author>
    <author>
      <name>Mark Chen</name>
    </author>
    <author>
      <name>Eric Sigler</name>
    </author>
    <author>
      <name>Mateusz Litwin</name>
    </author>
    <author>
      <name>Scott Gray</name>
    </author>
    <author>
      <name>Benjamin Chess</name>
    </author>
    <author>
      <name>Jack Clark</name>
    </author>
    <author>
      <name>Christopher Berner</name>
    </author>
    <author>
      <name>Sam McCandlish</name>
    </author>
    <author>
      <name>Alec Radford</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Dario Amodei</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.08361v1</id>
    <title>Scaling Laws for Neural Language Models</title>
    <updated>2020-01-23T03:59:20Z</updated>
    <link href="https://arxiv.org/abs/2001.08361v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2001.08361v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2020-01-23T03:59:20Z</published>
    <arxiv:comment>19 pages, 15 figures</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Jared Kaplan</name>
    </author>
    <author>
      <name>Sam McCandlish</name>
    </author>
    <author>
      <name>Tom Henighan</name>
    </author>
    <author>
      <name>Tom B. Brown</name>
    </author>
    <author>
      <name>Benjamin Chess</name>
    </author>
    <author>
      <name>Rewon Child</name>
    </author>
    <author>
      <name>Scott Gray</name>
    </author>
    <author>
      <name>Alec Radford</name>
    </author>
    <author>
      <name>Jeffrey Wu</name>
    </author>
    <author>
      <name>Dario Amodei</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.08593v2</id>
    <title>Fine-Tuning Language Models from Human Preferences</title>
    <updated>2020-01-08T23:02:36Z</updated>
    <link href="https://arxiv.org/abs/1909.08593v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1909.08593v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>Reward learning enables the application of reinforcement learning (RL) to tasks where reward is defined by human judgment, building a model of reward by asking humans questions. Most work on reward learning has used simulated environments, but complex information about values is often expressed in natural language, and we believe reward learning for language is a key to making RL practical and safe for real-world tasks. In this paper, we build on advances in generative pretraining of language models to apply reward learning to four natural language tasks: continuing text with positive sentiment or physically descriptive language, and summarization tasks on the TL;DR and CNN/Daily Mail datasets. For stylistic continuation we achieve good results with only 5,000 comparisons evaluated by humans. For summarization, models trained with 60,000 comparisons copy whole sentences from the input but skip irrelevant preamble; this leads to reasonable ROUGE scores and very good performance according to our human labelers, but may be exploiting the fact that labelers rely on simple heuristics.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2019-09-18T17:33:39Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Daniel M. Ziegler</name>
    </author>
    <author>
      <name>Nisan Stiennon</name>
    </author>
    <author>
      <name>Jeffrey Wu</name>
    </author>
    <author>
      <name>Tom B. Brown</name>
    </author>
    <author>
      <name>Alec Radford</name>
    </author>
    <author>
      <name>Dario Amodei</name>
    </author>
    <author>
      <name>Paul Christiano</name>
    </author>
    <author>
      <name>Geoffrey Irving</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.06162v1</id>
    <title>An Empirical Model of Large-Batch Training</title>
    <updated>2018-12-14T20:49:09Z</updated>
    <link href="https://arxiv.org/abs/1812.06162v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1812.06162v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>In an increasing number of domains it has been demonstrated that deep learning models can be trained using relatively large batch sizes without sacrificing data efficiency. However the limits of this massive data parallelism seem to differ from domain to domain, ranging from batches of tens of thousands in ImageNet to batches of millions in RL agents that play the game Dota 2. To our knowledge there is limited conceptual understanding of why these limits to batch size differ or how we might choose the correct batch size in a new domain. In this paper, we demonstrate that a simple and easy-to-measure statistic called the gradient noise scale predicts the largest useful batch size across many domains and applications, including a number of supervised learning datasets (MNIST, SVHN, CIFAR-10, ImageNet, Billion Word), reinforcement learning domains (Atari and Dota), and even generative model training (autoencoders on SVHN). We find that the noise scale increases as the loss decreases over a training run and depends on the model size primarily through improved model performance. Our empirically-motivated theory also describes the tradeoff between compute-efficiency and time-efficiency, and provides a rough model of the benefits of adaptive batch-size training.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2018-12-14T20:49:09Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Sam McCandlish</name>
    </author>
    <author>
      <name>Jared Kaplan</name>
    </author>
    <author>
      <name>Dario Amodei</name>
    </author>
    <author>
      <name>OpenAI Dota Team</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.06521v1</id>
    <title>Reward learning from human preferences and demonstrations in Atari</title>
    <updated>2018-11-15T18:33:43Z</updated>
    <link href="https://arxiv.org/abs/1811.06521v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1811.06521v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>To solve complex real-world problems with reinforcement learning, we cannot rely on manually specified reward functions. Instead, we can have humans communicate an objective to the agent directly. In this work, we combine two approaches to learning from human feedback: expert demonstrations and trajectory preferences. We train a deep neural network to model the reward function and use its predicted reward to train an DQN-based deep reinforcement learning agent on 9 Atari games. Our approach beats the imitation learning baseline in 7 games and achieves strictly superhuman performance on 2 games without using game rewards. Additionally, we investigate the goodness of fit of the reward model, present some reward hacking problems, and study the effects of noise in the human labels.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2018-11-15T18:33:43Z</published>
    <arxiv:comment>NIPS 2018</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Borja Ibarz</name>
    </author>
    <author>
      <name>Jan Leike</name>
    </author>
    <author>
      <name>Tobias Pohlen</name>
    </author>
    <author>
      <name>Geoffrey Irving</name>
    </author>
    <author>
      <name>Shane Legg</name>
    </author>
    <author>
      <name>Dario Amodei</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.08575v1</id>
    <title>Supervising strong learners by amplifying weak experts</title>
    <updated>2018-10-19T16:30:48Z</updated>
    <link href="https://arxiv.org/abs/1810.08575v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1810.08575v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Many real world learning tasks involve complex or hard-to-specify objectives, and using an easier-to-specify proxy can lead to poor performance or misaligned behavior. One solution is to have humans provide a training signal by demonstrating or judging performance, but this approach fails if the task is too complicated for a human to directly evaluate. We propose Iterated Amplification, an alternative training strategy which progressively builds up a training signal for difficult problems by combining solutions to easier subproblems. Iterated Amplification is closely related to Expert Iteration (Anthony et al., 2017; Silver et al., 2017), except that it uses no external reward function. We present results in algorithmic environments, showing that Iterated Amplification can efficiently learn complex behaviors.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2018-10-19T16:30:48Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Paul Christiano</name>
    </author>
    <author>
      <name>Buck Shlegeris</name>
    </author>
    <author>
      <name>Dario Amodei</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.10299v1</id>
    <title>Variational Option Discovery Algorithms</title>
    <updated>2018-07-26T18:05:45Z</updated>
    <link href="https://arxiv.org/abs/1807.10299v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1807.10299v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We explore methods for option discovery based on variational inference and make two algorithmic contributions. First: we highlight a tight connection between variational option discovery methods and variational autoencoders, and introduce Variational Autoencoding Learning of Options by Reinforcement (VALOR), a new method derived from the connection. In VALOR, the policy encodes contexts from a noise distribution into trajectories, and the decoder recovers the contexts from the complete trajectories. Second: we propose a curriculum learning approach where the number of contexts seen by the agent increases whenever the agent's performance is strong enough (as measured by the decoder) on the current set of contexts. We show that this simple trick stabilizes training for VALOR and prior variational option discovery methods, allowing a single agent to learn many more modes of behavior than it could with a fixed context distribution. Finally, we investigate other topics related to variational option discovery, including fundamental limitations of the general approach and the applicability of learned options to downstream tasks.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2018-07-26T18:05:45Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Joshua Achiam</name>
    </author>
    <author>
      <name>Harrison Edwards</name>
    </author>
    <author>
      <name>Dario Amodei</name>
    </author>
    <author>
      <name>Pieter Abbeel</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.00899v2</id>
    <title>AI safety via debate</title>
    <updated>2018-10-22T17:36:07Z</updated>
    <link href="https://arxiv.org/abs/1805.00899v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1805.00899v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>To make AI systems broadly useful for challenging real-world tasks, we need them to learn complex human goals and preferences. One approach to specifying complex goals asks humans to judge during training which agent behaviors are safe and useful, but this approach can fail if the task is too complicated for a human to directly judge. To help address this concern, we propose training agents via self play on a zero sum debate game. Given a question or proposed action, two agents take turns making short statements up to a limit, then a human judges which of the agents gave the most true, useful information. In an analogy to complexity theory, debate with optimal play can answer any question in PSPACE given polynomial time judges (direct judging answers only NP questions). In practice, whether debate works involves empirical questions about humans and the tasks we want AIs to perform, plus theoretical questions about the meaning of AI alignment. We report results on an initial MNIST experiment where agents compete to convince a sparse classifier, boosting the classifier's accuracy from 59.4% to 88.9% given 6 pixels and from 48.2% to 85.2% given 4 pixels. Finally, we discuss theoretical and practical aspects of the debate model, focusing on potential weaknesses as the model scales up, and we propose future human and computer experiments to test these properties.</summary>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2018-05-02T16:27:32Z</published>
    <arxiv:comment>24 pages, 6 figures</arxiv:comment>
    <arxiv:primary_category term="stat.ML"/>
    <author>
      <name>Geoffrey Irving</name>
    </author>
    <author>
      <name>Paul Christiano</name>
    </author>
    <author>
      <name>Dario Amodei</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.07228v2</id>
    <title>The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation</title>
    <updated>2024-12-01T17:59:04Z</updated>
    <link href="https://arxiv.org/abs/1802.07228v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1802.07228v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>This report surveys the landscape of potential security threats from malicious uses of AI, and proposes ways to better forecast, prevent, and mitigate these threats. After analyzing the ways in which AI may influence the threat landscape in the digital, physical, and political domains, we make four high-level recommendations for AI researchers and other stakeholders. We also suggest several promising areas for further research that could expand the portfolio of defenses, or make attacks less effective or harder to execute. Finally, we discuss, but do not conclusively resolve, the long-term equilibrium of attackers and defenders.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <published>2018-02-20T18:07:50Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Miles Brundage</name>
    </author>
    <author>
      <name>Shahar Avin</name>
    </author>
    <author>
      <name>Jack Clark</name>
    </author>
    <author>
      <name>Helen Toner</name>
    </author>
    <author>
      <name>Peter Eckersley</name>
    </author>
    <author>
      <name>Ben Garfinkel</name>
    </author>
    <author>
      <name>Allan Dafoe</name>
    </author>
    <author>
      <name>Paul Scharre</name>
    </author>
    <author>
      <name>Thomas Zeitzoff</name>
    </author>
    <author>
      <name>Bobby Filar</name>
    </author>
    <author>
      <name>Hyrum Anderson</name>
    </author>
    <author>
      <name>Heather Roff</name>
    </author>
    <author>
      <name>Gregory C. Allen</name>
    </author>
    <author>
      <name>Jacob Steinhardt</name>
    </author>
    <author>
      <name>Carrick Flynn</name>
    </author>
    <author>
      <name>Seán Ó hÉigeartaigh</name>
    </author>
    <author>
      <name>SJ Beard</name>
    </author>
    <author>
      <name>Haydn Belfield</name>
    </author>
    <author>
      <name>Sebastian Farquhar</name>
    </author>
    <author>
      <name>Clare Lyle</name>
    </author>
    <author>
      <name>Rebecca Crootof</name>
    </author>
    <author>
      <name>Owain Evans</name>
    </author>
    <author>
      <name>Michael Page</name>
    </author>
    <author>
      <name>Joanna Bryson</name>
    </author>
    <author>
      <name>Roman Yampolskiy</name>
    </author>
    <author>
      <name>Dario Amodei</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.03741v4</id>
    <title>Deep reinforcement learning from human preferences</title>
    <updated>2023-02-17T17:00:34Z</updated>
    <link href="https://arxiv.org/abs/1706.03741v4" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1706.03741v4" rel="related" type="application/pdf" title="pdf"/>
    <summary>For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.</summary>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2017-06-12T17:23:59Z</published>
    <arxiv:primary_category term="stat.ML"/>
    <author>
      <name>Paul Christiano</name>
    </author>
    <author>
      <name>Jan Leike</name>
    </author>
    <author>
      <name>Tom B. Brown</name>
    </author>
    <author>
      <name>Miljan Martic</name>
    </author>
    <author>
      <name>Shane Legg</name>
    </author>
    <author>
      <name>Dario Amodei</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.08945v4</id>
    <title>Learning a Natural Language Interface with Neural Programmer</title>
    <updated>2017-03-02T16:02:00Z</updated>
    <link href="https://arxiv.org/abs/1611.08945v4" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1611.08945v4" rel="related" type="application/pdf" title="pdf"/>
    <summary>Learning a natural language interface for database tables is a challenging task that involves deep language understanding and multi-step reasoning. The task is often approached by mapping natural language queries to logical forms or programs that provide the desired response when executed on the database. To our knowledge, this paper presents the first weakly supervised, end-to-end neural network model to induce such programs on a real-world dataset. We enhance the objective function of Neural Programmer, a neural network with built-in discrete operations, and apply it on WikiTableQuestions, a natural language question-answering dataset. The model is trained end-to-end with weak supervision of question-answer pairs, and does not require domain-specific grammars, rules, or annotations that are key elements in previous approaches to program induction. The main experimental result in this paper is that a single Neural Programmer model achieves 34.2% accuracy using only 10,000 examples with weak supervision. An ensemble of 15 models, with a trivial combination technique, achieves 37.7% accuracy, which is competitive to the current state-of-the-art accuracy of 37.1% obtained by a traditional natural language semantic parser.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2016-11-28T00:54:34Z</published>
    <arxiv:comment>Published as a conference paper at ICLR 2017</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Arvind Neelakantan</name>
    </author>
    <author>
      <name>Quoc V. Le</name>
    </author>
    <author>
      <name>Martin Abadi</name>
    </author>
    <author>
      <name>Andrew McCallum</name>
    </author>
    <author>
      <name>Dario Amodei</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.06565v2</id>
    <title>Concrete Problems in AI Safety</title>
    <updated>2016-07-25T17:23:29Z</updated>
    <link href="https://arxiv.org/abs/1606.06565v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1606.06565v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function ("avoiding side effects" and "avoiding reward hacking"), an objective function that is too expensive to evaluate frequently ("scalable supervision"), or undesirable behavior during the learning process ("safe exploration" and "distributional shift"). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2016-06-21T13:37:05Z</published>
    <arxiv:comment>29 pages</arxiv:comment>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Dario Amodei</name>
    </author>
    <author>
      <name>Chris Olah</name>
    </author>
    <author>
      <name>Jacob Steinhardt</name>
    </author>
    <author>
      <name>Paul Christiano</name>
    </author>
    <author>
      <name>John Schulman</name>
    </author>
    <author>
      <name>Dan Mané</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.02595v1</id>
    <title>Deep Speech 2: End-to-End Speech Recognition in English and Mandarin</title>
    <updated>2015-12-08T19:13:50Z</updated>
    <link href="https://arxiv.org/abs/1512.02595v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1512.02595v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech--two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system. Because of this efficiency, experiments that previously took weeks now run in days. This enables us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2015-12-08T19:13:50Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Dario Amodei</name>
    </author>
    <author>
      <name>Rishita Anubhai</name>
    </author>
    <author>
      <name>Eric Battenberg</name>
    </author>
    <author>
      <name>Carl Case</name>
    </author>
    <author>
      <name>Jared Casper</name>
    </author>
    <author>
      <name>Bryan Catanzaro</name>
    </author>
    <author>
      <name>Jingdong Chen</name>
    </author>
    <author>
      <name>Mike Chrzanowski</name>
    </author>
    <author>
      <name>Adam Coates</name>
    </author>
    <author>
      <name>Greg Diamos</name>
    </author>
    <author>
      <name>Erich Elsen</name>
    </author>
    <author>
      <name>Jesse Engel</name>
    </author>
    <author>
      <name>Linxi Fan</name>
    </author>
    <author>
      <name>Christopher Fougner</name>
    </author>
    <author>
      <name>Tony Han</name>
    </author>
    <author>
      <name>Awni Hannun</name>
    </author>
    <author>
      <name>Billy Jun</name>
    </author>
    <author>
      <name>Patrick LeGresley</name>
    </author>
    <author>
      <name>Libby Lin</name>
    </author>
    <author>
      <name>Sharan Narang</name>
    </author>
    <author>
      <name>Andrew Ng</name>
    </author>
    <author>
      <name>Sherjil Ozair</name>
    </author>
    <author>
      <name>Ryan Prenger</name>
    </author>
    <author>
      <name>Jonathan Raiman</name>
    </author>
    <author>
      <name>Sanjeev Satheesh</name>
    </author>
    <author>
      <name>David Seetapun</name>
    </author>
    <author>
      <name>Shubho Sengupta</name>
    </author>
    <author>
      <name>Yi Wang</name>
    </author>
    <author>
      <name>Zhiqian Wang</name>
    </author>
    <author>
      <name>Chong Wang</name>
    </author>
    <author>
      <name>Bo Xiao</name>
    </author>
    <author>
      <name>Dani Yogatama</name>
    </author>
    <author>
      <name>Jun Zhan</name>
    </author>
    <author>
      <name>Zhenyao Zhu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.5946v1</id>
    <title>Thermodynamics for a network of neurons: Signatures of criticality</title>
    <updated>2014-07-22T17:16:12Z</updated>
    <link href="https://arxiv.org/abs/1407.5946v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1407.5946v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The activity of a neural network is defined by patterns of spiking and silence from the individual neurons. Because spikes are (relatively) sparse, patterns of activity with increasing numbers of spikes are less probable, but with more spikes the number of possible patterns increases. This tradeoff between probability and numerosity is mathematically equivalent to the relationship between entropy and energy in statistical physics. We construct this relationship for populations of up to N=160 neurons in a small patch of the vertebrate retina, using a combination of direct and model-based analyses of experiments on the response of this network to naturalistic movies. We see signs of a thermodynamic limit, where the entropy per neuron approaches a smooth function of the energy per neuron as N increases. The form of this function corresponds to the distribution of activity being poised near an unusual kind of critical point. Networks with more or less correlation among neurons would not reach this critical state. We suggest further tests of criticality, and give a brief discussion of its functional significance.</summary>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <published>2014-07-22T17:16:12Z</published>
    <arxiv:primary_category term="q-bio.NC"/>
    <arxiv:journal_ref>Proceedings of the National Academy of Sciences (USA)112, 11508-11513 (2015)</arxiv:journal_ref>
    <author>
      <name>Gasper Tkacik</name>
    </author>
    <author>
      <name>Thierry Mora</name>
    </author>
    <author>
      <name>Olivier Marre</name>
    </author>
    <author>
      <name>Dario Amodei</name>
    </author>
    <author>
      <name>Michael J. Berry</name>
    </author>
    <author>
      <name>William Bialek</name>
    </author>
    <arxiv:doi>10.1073/pnas.151418811</arxiv:doi>
    <link rel="related" href="https://doi.org/10.1073/pnas.151418811" title="doi"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.5709v7</id>
    <title>Physical Principles for Scalable Neural Recording</title>
    <updated>2013-09-16T15:47:02Z</updated>
    <link href="https://arxiv.org/abs/1306.5709v7" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1306.5709v7" rel="related" type="application/pdf" title="pdf"/>
    <summary>Simultaneously measuring the activities of all neurons in a mammalian brain at millisecond resolution is a challenge beyond the limits of existing techniques in neuroscience. Entirely new approaches may be required, motivating an analysis of the fundamental physical constraints on the problem. We outline the physical principles governing brain activity mapping using optical, electrical,magnetic resonance, and molecular modalities of neural recording. Focusing on the mouse brain, we analyze the scalability of each method, concentrating on the limitations imposed by spatiotemporal resolution, energy dissipation, and volume displacement. We also study the physics of powering and communicating with microscale devices embedded in brain tissue.</summary>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <published>2013-06-24T19:04:50Z</published>
    <arxiv:primary_category term="q-bio.NC"/>
    <author>
      <name>Adam H. Marblestone</name>
    </author>
    <author>
      <name>Bradley M. Zamft</name>
    </author>
    <author>
      <name>Yael G. Maguire</name>
    </author>
    <author>
      <name>Mikhail G. Shapiro</name>
    </author>
    <author>
      <name>Thaddeus R. Cybulski</name>
    </author>
    <author>
      <name>Joshua I. Glaser</name>
    </author>
    <author>
      <name>Dario Amodei</name>
    </author>
    <author>
      <name>P. Benjamin Stranges</name>
    </author>
    <author>
      <name>Reza Kalhor</name>
    </author>
    <author>
      <name>David A. Dalrymple</name>
    </author>
    <author>
      <name>Dongjin Seo</name>
    </author>
    <author>
      <name>Elad Alon</name>
    </author>
    <author>
      <name>Michel M. Maharbiz</name>
    </author>
    <author>
      <name>Jose M. Carmena</name>
    </author>
    <author>
      <name>Jan M. Rabaey</name>
    </author>
    <author>
      <name>Edward S. Boyden</name>
    </author>
    <author>
      <name>George M. Church</name>
    </author>
    <author>
      <name>Konrad P. Kording</name>
    </author>
    <arxiv:doi>10.3389/fncom.2013.00137</arxiv:doi>
    <link rel="related" href="https://doi.org/10.3389/fncom.2013.00137" title="doi"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.3061v1</id>
    <title>Searching for collective behavior in a network of real neurons</title>
    <updated>2013-06-13T09:33:21Z</updated>
    <link href="https://arxiv.org/abs/1306.3061v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1306.3061v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Maximum entropy models are the least structured probability distributions that exactly reproduce a chosen set of statistics measured in an interacting network. Here we use this principle to construct probabilistic models which describe the correlated spiking activity of populations of up to 120 neurons in the salamander retina as it responds to natural movies. Already in groups as small as 10 neurons, interactions between spikes can no longer be regarded as small perturbations in an otherwise independent system; for 40 or more neurons pairwise interactions need to be supplemented by a global interaction that controls the distribution of synchrony in the population. Here we show that such "K-pairwise" models--being systematic extensions of the previously used pairwise Ising models--provide an excellent account of the data. We explore the properties of the neural vocabulary by: 1) estimating its entropy, which constrains the population's capacity to represent visual information; 2) classifying activity patterns into a small set of metastable collective modes; 3) showing that the neural codeword ensembles are extremely inhomogenous; 4) demonstrating that the state of individual neurons is highly predictable from the rest of the population, allowing the capacity for error correction.</summary>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <published>2013-06-13T09:33:21Z</published>
    <arxiv:comment>24 pages, 19 figures</arxiv:comment>
    <arxiv:primary_category term="q-bio.NC"/>
    <arxiv:journal_ref>PLOS Comput Biol 10 (2014): e1003408</arxiv:journal_ref>
    <author>
      <name>Gašper Tkačik</name>
    </author>
    <author>
      <name>Olivier Marre</name>
    </author>
    <author>
      <name>Dario Amodei</name>
    </author>
    <author>
      <name>Elad Schneidman</name>
    </author>
    <author>
      <name>William Bialek</name>
    </author>
    <author>
      <name>Michael J Berry</name>
    </author>
    <arxiv:doi>10.1371/journal.pcbi.1003408</arxiv:doi>
    <link rel="related" href="https://doi.org/10.1371/journal.pcbi.1003408" title="doi"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.6319v1</id>
    <title>The simplest maximum entropy model for collective behavior in a neural network</title>
    <updated>2012-07-26T16:28:11Z</updated>
    <link href="https://arxiv.org/abs/1207.6319v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1207.6319v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Recent work emphasizes that the maximum entropy principle provides a bridge between statistical mechanics models for collective behavior in neural networks and experiments on networks of real neurons. Most of this work has focused on capturing the measured correlations among pairs of neurons. Here we suggest an alternative, constructing models that are consistent with the distribution of global network activity, i.e. the probability that K out of N cells in the network generate action potentials in the same small time bin. The inverse problem that we need to solve in constructing the model is analytically tractable, and provides a natural "thermodynamics" for the network in the limit of large N. We analyze the responses of neurons in a small patch of the retina to naturalistic stimuli, and find that the implied thermodynamics is very close to an unusual critical point, in which the entropy (in proper units) is exactly equal to the energy.</summary>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <published>2012-07-26T16:28:11Z</published>
    <arxiv:primary_category term="q-bio.NC"/>
    <author>
      <name>Gasper Tkacik</name>
    </author>
    <author>
      <name>Olivier Marre</name>
    </author>
    <author>
      <name>Thierry Mora</name>
    </author>
    <author>
      <name>Dario Amodei</name>
    </author>
    <author>
      <name>Michael J. Berry</name>
    </author>
    <author>
      <name>William Bialek</name>
    </author>
    <arxiv:doi>10.1088/1742-5468/2013/03/P03011</arxiv:doi>
    <link rel="related" href="https://doi.org/10.1088/1742-5468/2013/03/P03011" title="doi"/>
  </entry>
</feed>
