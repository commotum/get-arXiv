<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api/nf5seiAwNqfmqcdEORuU1kIV27Q</id>
  <title>arXiv Query: search_query=au:"Oriol Vinyals"&amp;id_list=&amp;start=100&amp;max_results=50</title>
  <updated>2026-02-07T20:48:45Z</updated>
  <link href="https://arxiv.org/api/query?search_query=au:%22Oriol+Vinyals%22&amp;start=100&amp;max_results=50&amp;id_list=" type="application/atom+xml"/>
  <opensearch:itemsPerPage>50</opensearch:itemsPerPage>
  <opensearch:totalResults>118</opensearch:totalResults>
  <opensearch:startIndex>100</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/1511.06114v4</id>
    <title>Multi-task Sequence to Sequence Learning</title>
    <updated>2016-03-01T10:55:58Z</updated>
    <link href="https://arxiv.org/abs/1511.06114v4" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1511.06114v4" rel="related" type="application/pdf" title="pdf"/>
    <summary>Sequence to sequence learning has recently emerged as a new paradigm in supervised learning. To date, most of its applications focused on only one task and not much work explored this framework for multiple tasks. This paper examines three multi-task learning (MTL) settings for sequence to sequence models: (a) the oneto-many setting - where the encoder is shared between several tasks such as machine translation and syntactic parsing, (b) the many-to-one setting - useful when only the decoder can be shared, as in the case of translation and image caption generation, and (c) the many-to-many setting - where multiple encoders and decoders are shared, which is the case with unsupervised objectives and translation. Our results show that training on a small amount of parsing and image caption data can improve the translation quality between English and German by up to 1.5 BLEU points over strong single-task baselines on the WMT benchmarks. Furthermore, we have established a new state-of-the-art result in constituent parsing with 93.0 F1. Lastly, we reveal interesting properties of the two unsupervised learning objectives, autoencoder and skip-thought, in the MTL context: autoencoder helps less in terms of perplexities but more on BLEU scores compared to skip-thought.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2015-11-19T10:24:14Z</published>
    <arxiv:comment>10 pages, 4 figures, ICLR 2016 camera-ready, added parsing SOTA results</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Minh-Thang Luong</name>
    </author>
    <author>
      <name>Quoc V. Le</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Lukasz Kaiser</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.04868v4</id>
    <title>A Neural Transducer</title>
    <updated>2016-08-04T23:31:46Z</updated>
    <link href="https://arxiv.org/abs/1511.04868v4" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1511.04868v4" rel="related" type="application/pdf" title="pdf"/>
    <summary>Sequence-to-sequence models have achieved impressive results on various tasks. However, they are unsuitable for tasks that require incremental predictions to be made as more data arrives or tasks that have long input sequences and output sequences. This is because they generate an output sequence conditioned on an entire input sequence. In this paper, we present a Neural Transducer that can make incremental predictions as more input arrives, without redoing the entire computation. Unlike sequence-to-sequence models, the Neural Transducer computes the next-step distribution conditioned on the partially observed input sequence and the partially generated sequence. At each time step, the transducer can decide to emit zero to many output symbols. The data can be processed using an encoder and presented as input to the transducer. The discrete decision to emit a symbol at every time step makes it difficult to learn with conventional backpropagation. It is however possible to train the transducer by using a dynamic programming algorithm to generate target discrete decisions. Our experiments show that the Neural Transducer works well in settings where it is required to produce output predictions as data come in. We also find that the Neural Transducer performs well for long sequences even when attention mechanisms are not used.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2015-11-16T08:53:44Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Navdeep Jaitly</name>
    </author>
    <author>
      <name>David Sussillo</name>
    </author>
    <author>
      <name>Quoc V. Le</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Samy Bengio</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.01211v2</id>
    <title>Listen, Attend and Spell</title>
    <updated>2015-08-20T00:38:43Z</updated>
    <link href="https://arxiv.org/abs/1508.01211v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1508.01211v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>We present Listen, Attend and Spell (LAS), a neural network that learns to transcribe speech utterances to characters. Unlike traditional DNN-HMM models, this model learns all the components of a speech recognizer jointly. Our system has two components: a listener and a speller. The listener is a pyramidal recurrent network encoder that accepts filter bank spectra as inputs. The speller is an attention-based recurrent network decoder that emits characters as outputs. The network produces character sequences without making any independence assumptions between the characters. This is the key improvement of LAS over previous end-to-end CTC models. On a subset of the Google voice search task, LAS achieves a word error rate (WER) of 14.1% without a dictionary or a language model, and 10.3% with language model rescoring over the top 32 beams. By comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0%.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2015-08-05T20:17:58Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>William Chan</name>
    </author>
    <author>
      <name>Navdeep Jaitly</name>
    </author>
    <author>
      <name>Quoc V. Le</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.05869v3</id>
    <title>A Neural Conversational Model</title>
    <updated>2015-07-22T03:29:47Z</updated>
    <link href="https://arxiv.org/abs/1506.05869v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1506.05869v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>Conversational modeling is an important task in natural language understanding and machine intelligence. Although previous approaches exist, they are often restricted to specific domains (e.g., booking an airline ticket) and require hand-crafted rules. In this paper, we present a simple approach for this task which uses the recently proposed sequence to sequence framework. Our model converses by predicting the next sentence given the previous sentence or sentences in a conversation. The strength of our model is that it can be trained end-to-end and thus requires much fewer hand-crafted rules. We find that this straightforward model can generate simple conversations given a large conversational training dataset. Our preliminary results suggest that, despite optimizing the wrong objective function, the model is able to converse well. It is able extract knowledge from both a domain specific dataset, and from a large, noisy, and general domain dataset of movie subtitles. On a domain-specific IT helpdesk dataset, the model can find a solution to a technical problem via conversations. On a noisy open-domain movie transcript dataset, the model can perform simple forms of common sense reasoning. As expected, we also find that the lack of consistency is a common failure mode of our model.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2015-06-19T02:52:23Z</published>
    <arxiv:comment>ICML Deep Learning Workshop 2015</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Quoc Le</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.03134v2</id>
    <title>Pointer Networks</title>
    <updated>2017-01-02T10:25:29Z</updated>
    <link href="https://arxiv.org/abs/1506.03134v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1506.03134v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>We introduce a new neural architecture to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. Such problems cannot be trivially addressed by existent approaches such as sequence-to-sequence and Neural Turing Machines, because the number of target classes in each step of the output depends on the length of the input, which is variable. Problems such as sorting variable sized sequences, and various combinatorial optimization problems belong to this class. Our model solves the problem of variable size output dictionaries using a recently proposed mechanism of neural attention. It differs from the previous attention attempts in that, instead of using attention to blend hidden units of an encoder to a context vector at each decoder step, it uses attention as a pointer to select a member of the input sequence as the output. We call this architecture a Pointer Net (Ptr-Net). We show Ptr-Nets can be used to learn approximate solutions to three challenging geometric problems -- finding planar convex hulls, computing Delaunay triangulations, and the planar Travelling Salesman Problem -- using training examples alone. Ptr-Nets not only improve over sequence-to-sequence with input attention, but also allow us to generalize to variable size output dictionaries. We show that the learnt models generalize beyond the maximum lengths they were trained on. We hope our results on these tasks will encourage a broader exploration of neural learning for discrete problems.</summary>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2015-06-09T23:38:16Z</published>
    <arxiv:primary_category term="stat.ML"/>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Meire Fortunato</name>
    </author>
    <author>
      <name>Navdeep Jaitly</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.03099v3</id>
    <title>Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks</title>
    <updated>2015-09-23T16:35:42Z</updated>
    <link href="https://arxiv.org/abs/1506.03099v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1506.03099v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>Recurrent Neural Networks can be trained to produce sequences of tokens given some input, as exemplified by recent results in machine translation and image captioning. The current approach to training them consists of maximizing the likelihood of each token in the sequence given the current (recurrent) state and the previous token. At inference, the unknown previous token is then replaced by a token generated by the model itself. This discrepancy between training and inference can yield errors that can accumulate quickly along the generated sequence. We propose a curriculum learning strategy to gently change the training process from a fully guided scheme using the true previous token, towards a less guided scheme which mostly uses the generated token instead. Experiments on several sequence prediction tasks show that this approach yields significant improvements. Moreover, it was used successfully in our winning entry to the MSCOCO image captioning challenge, 2015.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2015-06-09T20:33:47Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Samy Bengio</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Navdeep Jaitly</name>
    </author>
    <author>
      <name>Noam Shazeer</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.08909v2</id>
    <title>Beyond Short Snippets: Deep Networks for Video Classification</title>
    <updated>2015-04-13T19:44:25Z</updated>
    <link href="https://arxiv.org/abs/1503.08909v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1503.08909v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>Convolutional neural networks (CNNs) have been extensively applied for image recognition problems giving state-of-the-art results on recognition, detection, segmentation and retrieval. In this work we propose and evaluate several deep neural network architectures to combine image information across a video over longer time periods than previously attempted. We propose two methods capable of handling full length videos. The first method explores various convolutional temporal feature pooling architectures, examining the various design choices which need to be made when adapting a CNN for this task. The second proposed method explicitly models the video as an ordered sequence of frames. For this purpose we employ a recurrent neural network that uses Long Short-Term Memory (LSTM) cells which are connected to the output of the underlying CNN. Our best networks exhibit significant performance improvements over previously published results on the Sports 1 million dataset (73.1% vs. 60.9%) and the UCF-101 datasets with (88.6% vs. 88.0%) and without additional optical flow information (82.6% vs. 72.8%).</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2015-03-31T04:34:12Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Joe Yue-Hei Ng</name>
    </author>
    <author>
      <name>Matthew Hausknecht</name>
    </author>
    <author>
      <name>Sudheendra Vijayanarasimhan</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Rajat Monga</name>
    </author>
    <author>
      <name>George Toderici</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.02531v1</id>
    <title>Distilling the Knowledge in a Neural Network</title>
    <updated>2015-03-09T15:44:49Z</updated>
    <link href="https://arxiv.org/abs/1503.02531v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1503.02531v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.</summary>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2015-03-09T15:44:49Z</published>
    <arxiv:comment>NIPS 2014 Deep Learning Workshop</arxiv:comment>
    <arxiv:primary_category term="stat.ML"/>
    <author>
      <name>Geoffrey Hinton</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Jeff Dean</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.7449v3</id>
    <title>Grammar as a Foreign Language</title>
    <updated>2015-06-09T22:41:07Z</updated>
    <link href="https://arxiv.org/abs/1412.7449v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1412.7449v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>Syntactic constituency parsing is a fundamental problem in natural language processing and has been the subject of intensive research and engineering for decades. As a result, the most accurate parsers are domain specific, complex, and inefficient. In this paper we show that the domain agnostic attention-enhanced sequence-to-sequence model achieves state-of-the-art results on the most widely used syntactic constituency parsing dataset, when trained on a large synthetic corpus that was annotated using existing parsers. It also matches the performance of standard parsers when trained only on a small human-annotated dataset, which shows that this model is highly data-efficient, in contrast to sequence-to-sequence models without the attention mechanism. Our parser is also fast, processing over a hundred sentences per second with an unoptimized CPU implementation.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2014-12-23T17:16:24Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Lukasz Kaiser</name>
    </author>
    <author>
      <name>Terry Koo</name>
    </author>
    <author>
      <name>Slav Petrov</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Geoffrey Hinton</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.6544v6</id>
    <title>Qualitatively characterizing neural network optimization problems</title>
    <updated>2015-05-21T21:44:31Z</updated>
    <link href="https://arxiv.org/abs/1412.6544v6" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1412.6544v6" rel="related" type="application/pdf" title="pdf"/>
    <summary>Training neural networks involves solving large-scale non-convex optimization problems. This task has long been believed to be extremely difficult, with fear of local minima and other obstacles motivating a variety of schemes to improve optimization, such as unsupervised pretraining. However, modern neural networks are able to achieve negligible training error on complex tasks, using only direct training with stochastic gradient descent. We introduce a simple analysis technique to look for evidence that such networks are overcoming local optima. We find that, in fact, on a straight path from initialization to solution, a variety of state of the art neural networks never encounter any significant obstacles.</summary>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2014-12-19T21:55:01Z</published>
    <arxiv:primary_category term="cs.NE"/>
    <author>
      <name>Ian J. Goodfellow</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Andrew M. Saxe</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.4555v2</id>
    <title>Show and Tell: A Neural Image Caption Generator</title>
    <updated>2015-04-20T22:26:11Z</updated>
    <link href="https://arxiv.org/abs/1411.4555v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1411.4555v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2014-11-17T17:15:41Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Alexander Toshev</name>
    </author>
    <author>
      <name>Samy Bengio</name>
    </author>
    <author>
      <name>Dumitru Erhan</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.8206v4</id>
    <title>Addressing the Rare Word Problem in Neural Machine Translation</title>
    <updated>2015-05-30T19:57:28Z</updated>
    <link href="https://arxiv.org/abs/1410.8206v4" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1410.8206v4" rel="related" type="application/pdf" title="pdf"/>
    <summary>Neural Machine Translation (NMT) is a new approach to machine translation that has shown promising results that are comparable to traditional approaches. A significant weakness in conventional NMT systems is their inability to correctly translate very rare words: end-to-end NMTs tend to have relatively small vocabularies with a single unk symbol that represents every possible out-of-vocabulary (OOV) word. In this paper, we propose and implement an effective technique to address this problem. We train an NMT system on data that is augmented by the output of a word alignment algorithm, allowing the NMT system to emit, for each OOV word in the target sentence, the position of its corresponding word in the source sentence. This information is later utilized in a post-processing step that translates every OOV word using a dictionary. Our experiments on the WMT14 English to French translation task show that this method provides a substantial improvement of up to 2.8 BLEU points over an equivalent NMT system that does not use this technique. With 37.5 BLEU points, our NMT system is the first to surpass the best result achieved on a WMT14 contest task.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2014-10-30T00:20:31Z</published>
    <arxiv:comment>ACL 2015 camera-ready version</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Minh-Thang Luong</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Quoc V. Le</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Wojciech Zaremba</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.3215v3</id>
    <title>Sequence to Sequence Learning with Neural Networks</title>
    <updated>2014-12-14T20:59:51Z</updated>
    <link href="https://arxiv.org/abs/1409.3215v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1409.3215v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2014-09-10T19:55:35Z</published>
    <arxiv:comment>9 pages</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Quoc V. Le</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.2329v5</id>
    <title>Recurrent Neural Network Regularization</title>
    <updated>2015-02-19T14:46:00Z</updated>
    <link href="https://arxiv.org/abs/1409.2329v5" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1409.2329v5" rel="related" type="application/pdf" title="pdf"/>
    <summary>We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation.</summary>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2014-09-08T13:08:00Z</published>
    <arxiv:primary_category term="cs.NE"/>
    <author>
      <name>Wojciech Zaremba</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.1531v1</id>
    <title>DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition</title>
    <updated>2013-10-06T02:48:17Z</updated>
    <link href="https://arxiv.org/abs/1310.1531v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1310.1531v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be re-purposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges. We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2013-10-06T02:48:17Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Jeff Donahue</name>
    </author>
    <author>
      <name>Yangqing Jia</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Judy Hoffman</name>
    </author>
    <author>
      <name>Ning Zhang</name>
    </author>
    <author>
      <name>Eric Tzeng</name>
    </author>
    <author>
      <name>Trevor Darrell</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.5348v2</id>
    <title>Why Size Matters: Feature Coding as Nystrom Sampling</title>
    <updated>2013-04-16T00:17:54Z</updated>
    <link href="https://arxiv.org/abs/1301.5348v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1301.5348v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>Recently, the computer vision and machine learning community has been in favor of feature extraction pipelines that rely on a coding step followed by a linear classifier, due to their overall simplicity, well understood properties of linear classifiers, and their computational efficiency. In this paper we propose a novel view of this pipeline based on kernel methods and Nystrom sampling. In particular, we focus on the coding of a data point with a local representation based on a dictionary with fewer elements than the number of data points, and view it as an approximation to the actual function that would compute pair-wise similarity to all data points (often too many to compute in practice), followed by a Nystrom sampling step to select a subset of all data points.
  Furthermore, since bounds are known on the approximation power of Nystrom sampling as a function of how many samples (i.e. dictionary size) we consider, we can derive bounds on the approximation of the exact (but expensive to compute) kernel matrix, and use it as a proxy to predict accuracy as a function of the dictionary size, which has been observed to increase but also to saturate as we increase its size. This model may help explaining the positive effect of the codebook size and justifying the need to stack more layers (often referred to as deep learning), as flat models empirically saturate as we add more complexity.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2013-01-15T21:36:06Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Yangqing Jia</name>
    </author>
    <author>
      <name>Trevor Darrell</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.5056v1</id>
    <title>Pooling-Invariant Image Feature Learning</title>
    <updated>2013-01-15T18:47:11Z</updated>
    <link href="https://arxiv.org/abs/1302.5056v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1302.5056v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Unsupervised dictionary learning has been a key component in state-of-the-art computer vision recognition architectures. While highly effective methods exist for patch-based dictionary learning, these methods may learn redundant features after the pooling stage in a given early vision architecture. In this paper, we offer a novel dictionary learning scheme to efficiently take into account the invariance of learned features after the spatial pooling stage. The algorithm is built on simple clustering, and thus enjoys efficiency and scalability. We discuss the underlying mechanism that justifies the use of clustering algorithms, and empirically show that the algorithm finds better dictionaries than patch-based methods with the same dictionary size.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2013-01-15T18:47:11Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Yangqing Jia</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Trevor Darrell</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.4259v1</id>
    <title>Krylov Subspace Descent for Deep Learning</title>
    <updated>2011-11-18T02:15:32Z</updated>
    <link href="https://arxiv.org/abs/1111.4259v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1111.4259v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>In this paper, we propose a second order optimization method to learn models where both the dimensionality of the parameter space and the number of training samples is high. In our method, we construct on each iteration a Krylov subspace formed by the gradient and an approximation to the Hessian matrix, and then use a subset of the training data samples to optimize over this subspace. As with the Hessian Free (HF) method of [7], the Hessian matrix is never explicitly constructed, and is computed using a subset of data. In practice, as in HF, we typically use a positive definite substitute for the Hessian matrix such as the Gauss-Newton matrix. We investigate the effectiveness of our proposed method on deep neural networks, and compare its performance to widely used methods such as stochastic gradient descent, conjugate gradient descent and L-BFGS, and also to HF. Our method leads to faster convergence than either L-BFGS or HF, and generally performs better than either of them in cross-validation accuracy. It is also simpler and more general than HF, as it does not require a positive semi-definite approximation of the Hessian matrix to work well nor the setting of a damping parameter. The chief drawback versus HF is the need for memory to store a basis for the Krylov subspace.</summary>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <published>2011-11-18T02:15:32Z</published>
    <arxiv:primary_category term="stat.ML"/>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Daniel Povey</name>
    </author>
  </entry>
</feed>
