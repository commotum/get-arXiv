<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api/3/DCtPNfFECFTEiLw0zH3S3opvE</id>
  <title>arXiv Query: search_query=au:"Ruslan Salakhutdinov"&amp;id_list=&amp;start=50&amp;max_results=50</title>
  <updated>2026-02-07T20:14:06Z</updated>
  <link href="https://arxiv.org/api/query?search_query=au:%22Ruslan+Salakhutdinov%22&amp;start=50&amp;max_results=50&amp;id_list=" type="application/atom+xml"/>
  <opensearch:itemsPerPage>50</opensearch:itemsPerPage>
  <opensearch:totalResults>259</opensearch:totalResults>
  <opensearch:startIndex>50</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/2305.16309v3</id>
    <title>Imitating Task and Motion Planning with Visuomotor Transformers</title>
    <updated>2023-10-17T16:34:46Z</updated>
    <link href="https://arxiv.org/abs/2305.16309v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2305.16309v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>Imitation learning is a powerful tool for training robot manipulation policies, allowing them to learn from expert demonstrations without manual programming or trial-and-error. However, common methods of data collection, such as human supervision, scale poorly, as they are time-consuming and labor-intensive. In contrast, Task and Motion Planning (TAMP) can autonomously generate large-scale datasets of diverse demonstrations. In this work, we show that the combination of large-scale datasets generated by TAMP supervisors and flexible Transformer models to fit them is a powerful paradigm for robot manipulation. To that end, we present a novel imitation learning system called OPTIMUS that trains large-scale visuomotor Transformer policies by imitating a TAMP agent. OPTIMUS introduces a pipeline for generating TAMP data that is specifically curated for imitation learning and can be used to train performant transformer-based policies. In this paper, we present a thorough study of the design decisions required to imitate TAMP and demonstrate that OPTIMUS can solve a wide variety of challenging vision-based manipulation tasks with over 70 different objects, ranging from long-horizon pick-and-place tasks, to shelf and articulated object manipulation, achieving 70 to 80% success rates. Video results and code at https://mihdalal.github.io/optimus/</summary>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2023-05-25T17:58:14Z</published>
    <arxiv:comment>Conference on Robot Learning (CoRL) 2023. 8 pages, 5 figures, 2 tables; 11 pages appendix (10 additional figures)</arxiv:comment>
    <arxiv:primary_category term="cs.RO"/>
    <author>
      <name>Murtaza Dalal</name>
    </author>
    <author>
      <name>Ajay Mandlekar</name>
    </author>
    <author>
      <name>Caelan Garrett</name>
    </author>
    <author>
      <name>Ankur Handa</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
    <author>
      <name>Dieter Fox</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.15486v3</id>
    <title>SPRING: Studying the Paper and Reasoning to Play Games</title>
    <updated>2023-12-11T22:18:51Z</updated>
    <link href="https://arxiv.org/abs/2305.15486v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2305.15486v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>Open-world survival games pose significant challenges for AI algorithms due to their multi-tasking, deep exploration, and goal prioritization requirements. Despite reinforcement learning (RL) being popular for solving games, its high sample complexity limits its effectiveness in complex open-world games like Crafter or Minecraft. We propose a novel approach, SPRING, to read the game's original academic paper and use the knowledge learned to reason and play the game through a large language model (LLM). Prompted with the LaTeX source as game context and a description of the agent's current observation, our SPRING framework employs a directed acyclic graph (DAG) with game-related questions as nodes and dependencies as edges. We identify the optimal action to take in the environment by traversing the DAG and calculating LLM responses for each node in topological order, with the LLM's answer to final node directly translating to environment actions. In our experiments, we study the quality of in-context "reasoning" induced by different forms of prompts under the setting of the Crafter open-world environment. Our experiments suggest that LLMs, when prompted with consistent chain-of-thought, have great potential in completing sophisticated high-level trajectories. Quantitatively, SPRING with GPT-4 outperforms all state-of-the-art RL baselines, trained for 1M steps, without any training. Finally, we show the potential of games as a test bed for LLMs.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2023-05-24T18:14:35Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Yue Wu</name>
    </author>
    <author>
      <name>Shrimai Prabhumoye</name>
    </author>
    <author>
      <name>So Yeon Min</name>
    </author>
    <author>
      <name>Yonatan Bisk</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
    <author>
      <name>Amos Azaria</name>
    </author>
    <author>
      <name>Tom Mitchell</name>
    </author>
    <author>
      <name>Yuanzhi Li</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.02412v2</id>
    <title>Plan, Eliminate, and Track -- Language Models are Good Teachers for Embodied Agents</title>
    <updated>2023-05-07T05:33:10Z</updated>
    <link href="https://arxiv.org/abs/2305.02412v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2305.02412v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>Pre-trained large language models (LLMs) capture procedural knowledge about the world. Recent work has leveraged LLM's ability to generate abstract plans to simplify challenging control tasks, either by action scoring, or action modeling (fine-tuning). However, the transformer architecture inherits several constraints that make it difficult for the LLM to directly serve as the agent: e.g. limited input lengths, fine-tuning inefficiency, bias from pre-training, and incompatibility with non-text environments. To maintain compatibility with a low-level trainable actor, we propose to instead use the knowledge in LLMs to simplify the control problem, rather than solving it. We propose the Plan, Eliminate, and Track (PET) framework. The Plan module translates a task description into a list of high-level sub-tasks. The Eliminate module masks out irrelevant objects and receptacles from the observation for the current sub-task. Finally, the Track module determines whether the agent has accomplished each sub-task. On the AlfWorld instruction following benchmark, the PET framework leads to a significant 15% improvement over SOTA for generalization to human goal specifications.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2023-05-03T20:11:22Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Yue Wu</name>
    </author>
    <author>
      <name>So Yeon Min</name>
    </author>
    <author>
      <name>Yonatan Bisk</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
    <author>
      <name>Amos Azaria</name>
    </author>
    <author>
      <name>Yuanzhi Li</name>
    </author>
    <author>
      <name>Tom Mitchell</name>
    </author>
    <author>
      <name>Shrimai Prabhumoye</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.12247v5</id>
    <title>Quantifying &amp; Modeling Multimodal Interactions: An Information Decomposition Framework</title>
    <updated>2023-12-10T19:54:36Z</updated>
    <link href="https://arxiv.org/abs/2302.12247v5" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2302.12247v5" rel="related" type="application/pdf" title="pdf"/>
    <summary>The recent explosion of interest in multimodal applications has resulted in a wide selection of datasets and methods for representing and integrating information from different modalities. Despite these empirical advances, there remain fundamental research questions: How can we quantify the interactions that are necessary to solve a multimodal task? Subsequently, what are the most suitable multimodal models to capture these interactions? To answer these questions, we propose an information-theoretic approach to quantify the degree of redundancy, uniqueness, and synergy relating input modalities with an output task. We term these three measures as the PID statistics of a multimodal distribution (or PID for short), and introduce two new estimators for these PID statistics that scale to high-dimensional distributions. To validate PID estimation, we conduct extensive experiments on both synthetic datasets where the PID is known and on large-scale multimodal benchmarks where PID estimations are compared with human annotations. Finally, we demonstrate their usefulness in (1) quantifying interactions within multimodal datasets, (2) quantifying interactions captured by multimodal models, (3) principled approaches for model selection, and (4) three real-world case studies engaging with domain experts in pathology, mood prediction, and robotic perception where our framework helps to recommend strong multimodal models for each application.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <published>2023-02-23T18:59:05Z</published>
    <arxiv:comment>NeurIPS 2023. Code available at: https://github.com/pliang279/PID</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Paul Pu Liang</name>
    </author>
    <author>
      <name>Yun Cheng</name>
    </author>
    <author>
      <name>Xiang Fan</name>
    </author>
    <author>
      <name>Chun Kai Ling</name>
    </author>
    <author>
      <name>Suzanne Nie</name>
    </author>
    <author>
      <name>Richard Chen</name>
    </author>
    <author>
      <name>Zihao Deng</name>
    </author>
    <author>
      <name>Nicholas Allen</name>
    </author>
    <author>
      <name>Randy Auerbach</name>
    </author>
    <author>
      <name>Faisal Mahmood</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
    <author>
      <name>Louis-Philippe Morency</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.07944v3</id>
    <title>Effective Data Augmentation With Diffusion Models</title>
    <updated>2025-06-10T20:01:59Z</updated>
    <link href="https://arxiv.org/abs/2302.07944v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2302.07944v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>Data augmentation is one of the most prevalent tools in deep learning, underpinning many recent advances, including those from classification, generative models, and representation learning. The standard approach to data augmentation combines simple transformations like rotations and flips to generate new images from existing ones. However, these new images lack diversity along key semantic axes present in the data. Current augmentations cannot alter the high-level semantic attributes, such as animal species present in a scene, to enhance the diversity of data. We address the lack of diversity in data augmentation with image-to-image transformations parameterized by pre-trained text-to-image diffusion models. Our method edits images to change their semantics using an off-the-shelf diffusion model, and generalizes to novel visual concepts from a few labelled examples. We evaluate our approach on few-shot image classification tasks, and on a real-world weed recognition task, and observe an improvement in accuracy in tested domains.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2023-02-07T20:42:28Z</published>
    <arxiv:comment>Update to ICLR 2024 manuscript (https://openreview.net/forum?id=ZWzUA9zeAg), add leafy spurge citations</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Brandon Trabucco</name>
    </author>
    <author>
      <name>Kyle Doherty</name>
    </author>
    <author>
      <name>Max Gurinas</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.13823v4</id>
    <title>Grounding Language Models to Images for Multimodal Inputs and Outputs</title>
    <updated>2023-06-13T21:54:58Z</updated>
    <link href="https://arxiv.org/abs/2301.13823v4" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2301.13823v4" rel="related" type="application/pdf" title="pdf"/>
    <summary>We propose an efficient method to ground pretrained text-only language models to the visual domain, enabling them to process arbitrarily interleaved image-and-text data, and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and finetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained language models in visually grounded settings.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2023-01-31T18:33:44Z</published>
    <arxiv:comment>Published in ICML 2023. Project page: https://jykoh.com/fromage</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Jing Yu Koh</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
    <author>
      <name>Daniel Fried</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.10549v2</id>
    <title>Cross-modal Attention Congruence Regularization for Vision-Language Relation Alignment</title>
    <updated>2023-07-04T13:18:29Z</updated>
    <link href="https://arxiv.org/abs/2212.10549v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2212.10549v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>Despite recent progress towards scaling up multimodal vision-language models, these models are still known to struggle on compositional generalization benchmarks such as Winoground. We find that a critical component lacking from current vision-language models is relation-level alignment: the ability to match directional semantic relations in text (e.g., "mug in grass") with spatial relationships in the image (e.g., the position of the mug relative to the grass). To tackle this problem, we show that relation alignment can be enforced by encouraging the directed language attention from 'mug' to 'grass' (capturing the semantic relation 'in') to match the directed visual attention from the mug to the grass. Tokens and their corresponding objects are softly identified using the cross-modal attention. We prove that this notion of soft relation alignment is equivalent to enforcing congruence between vision and language attention matrices under a 'change of basis' provided by the cross-modal attention matrix. Intuitively, our approach projects visual attention into the language attention space to calculate its divergence from the actual language attention, and vice versa. We apply our Cross-modal Attention Congruence Regularization (CACR) loss to UNITER and improve on the state-of-the-art approach to Winoground.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2022-12-20T18:53:14Z</published>
    <arxiv:comment>ACL 2023</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Rohan Pandey</name>
    </author>
    <author>
      <name>Rulin Shao</name>
    </author>
    <author>
      <name>Paul Pu Liang</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
    <author>
      <name>Louis-Philippe Morency</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.05923v2</id>
    <title>Self-Supervised Object Goal Navigation with In-Situ Finetuning</title>
    <updated>2023-04-02T01:39:47Z</updated>
    <link href="https://arxiv.org/abs/2212.05923v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2212.05923v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>A household robot should be able to navigate to target objects without requiring users to first annotate everything in their home. Most current approaches to object navigation do not test on real robots and rely solely on reconstructed scans of houses and their expensively labeled semantic 3D meshes. In this work, our goal is to build an agent that builds self-supervised models of the world via exploration, the same as a child might - thus we (1) eschew the expense of labeled 3D mesh and (2) enable self-supervised in-situ finetuning in the real world. We identify a strong source of self-supervision (Location Consistency - LocCon) that can train all components of an ObjectNav agent, using unannotated simulated houses. Our key insight is that embodied agents can leverage location consistency as a self-supervision signal - collecting images from different views/angles and applying contrastive learning. We show that our agent can perform competitively in the real world and simulation. Our results also indicate that supervised training with 3D mesh annotations causes models to learn simulation artifacts, which are not transferrable to the real world. In contrast, our LocCon shows the most robust transfer in the real world among the set of models we compare to, and that the real-world performance of all models can be further improved with self-supervised LocCon in-situ training.</summary>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2022-12-09T03:41:40Z</published>
    <arxiv:primary_category term="cs.RO"/>
    <author>
      <name>So Yeon Min</name>
    </author>
    <author>
      <name>Yao-Hung Hubert Tsai</name>
    </author>
    <author>
      <name>Wei Ding</name>
    </author>
    <author>
      <name>Ali Farhadi</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
    <author>
      <name>Yonatan Bisk</name>
    </author>
    <author>
      <name>Jian Zhang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.05750v3</id>
    <title>Nano: Nested Human-in-the-Loop Reward Learning for Few-shot Language Model Control</title>
    <updated>2023-09-22T18:06:45Z</updated>
    <link href="https://arxiv.org/abs/2211.05750v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2211.05750v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>Pretrained language models have demonstrated extraordinary capabilities in language generation. However, real-world tasks often require controlling the distribution of generated text in order to mitigate bias, promote fairness, and achieve personalization. Existing techniques for controlling the distribution of generated text only work with quantified distributions, which require pre-defined categories, proportions of the distribution, or an existing corpus following the desired distributions. However, many important distributions, such as personal preferences, are unquantified. In this work, we tackle the problem of generating text following arbitrary distributions (quantified and unquantified) by proposing Nano, a few-shot human-in-the-loop training algorithm that continuously learns from human feedback. Nano achieves state-of-the-art results on single topic/attribute as well as quantified distribution control compared to previous works. We also show that Nano is able to learn unquantified distributions, achieves personalization, and captures differences between different individuals' personal preferences with high sample efficiency.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2022-11-10T18:31:56Z</published>
    <arxiv:comment>Accepted to ACL Findings 2023</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Xiang Fan</name>
    </author>
    <author>
      <name>Yiwei Lyu</name>
    </author>
    <author>
      <name>Paul Pu Liang</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
    <author>
      <name>Louis-Philippe Morency</name>
    </author>
    <arxiv:doi>10.18653/v1/2023.findings-acl.758</arxiv:doi>
    <link rel="related" href="https://doi.org/10.18653/v1/2023.findings-acl.758" title="doi"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.04714v2</id>
    <title>Uncertainty Quantification with Pre-trained Language Models: A Large-Scale Empirical Analysis</title>
    <updated>2022-10-14T07:17:10Z</updated>
    <link href="https://arxiv.org/abs/2210.04714v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2210.04714v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>Pre-trained language models (PLMs) have gained increasing popularity due to their compelling prediction performance in diverse natural language processing (NLP) tasks. When formulating a PLM-based prediction pipeline for NLP tasks, it is also crucial for the pipeline to minimize the calibration error, especially in safety-critical applications. That is, the pipeline should reliably indicate when we can trust its predictions. In particular, there are various considerations behind the pipeline: (1) the choice and (2) the size of PLM, (3) the choice of uncertainty quantifier, (4) the choice of fine-tuning loss, and many more. Although prior work has looked into some of these considerations, they usually draw conclusions based on a limited scope of empirical studies. There still lacks a holistic analysis on how to compose a well-calibrated PLM-based prediction pipeline. To fill this void, we compare a wide range of popular options for each consideration based on three prevalent NLP classification tasks and the setting of domain shift. In response, we recommend the following: (1) use ELECTRA for PLM encoding, (2) use larger PLMs if possible, (3) use Temp Scaling as the uncertainty quantifier, and (4) use Focal Loss for fine-tuning.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2022-10-10T14:16:01Z</published>
    <arxiv:comment>Accepted by EMNLP 2022 (Findings)</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Yuxin Xiao</name>
    </author>
    <author>
      <name>Paul Pu Liang</name>
    </author>
    <author>
      <name>Umang Bhatt</name>
    </author>
    <author>
      <name>Willie Neiswanger</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
    <author>
      <name>Louis-Philippe Morency</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.04443v2</id>
    <title>Don't Copy the Teacher: Data and Model Challenges in Embodied Dialogue</title>
    <updated>2022-10-11T20:16:41Z</updated>
    <link href="https://arxiv.org/abs/2210.04443v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2210.04443v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>Embodied dialogue instruction following requires an agent to complete a complex sequence of tasks from a natural language exchange. The recent introduction of benchmarks (Padmakumar et al., 2022) raises the question of how best to train and evaluate models for this multi-turn, multi-agent, long-horizon task. This paper contributes to that conversation, by arguing that imitation learning (IL) and related low-level metrics are actually misleading and do not align with the goals of embodied dialogue research and may hinder progress. We provide empirical comparisons of metrics, analysis of three models, and make suggestions for how the field might best progress. First, we observe that models trained with IL take spurious actions during evaluation. Second, we find that existing models fail to ground query utterances, which are essential for task completion. Third, we argue evaluation should focus on higher-level semantic goals.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2022-10-10T05:51:40Z</published>
    <arxiv:comment>To Appear in the Proceedings of EMNLP 2022</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>So Yeon Min</name>
    </author>
    <author>
      <name>Hao Zhu</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
    <author>
      <name>Yonatan Bisk</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.12343v1</id>
    <title>Paraphrasing Is All You Need for Novel Object Captioning</title>
    <updated>2022-09-25T22:56:04Z</updated>
    <link href="https://arxiv.org/abs/2209.12343v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2209.12343v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Novel object captioning (NOC) aims to describe images containing objects without observing their ground truth captions during training. Due to the absence of caption annotation, captioning models cannot be directly optimized via sequence-to-sequence training or CIDEr optimization. As a result, we present Paraphrasing-to-Captioning (P2C), a two-stage learning framework for NOC, which would heuristically optimize the output captions via paraphrasing. With P2C, the captioning model first learns paraphrasing from a language model pre-trained on text-only corpus, allowing expansion of the word bank for improving linguistic fluency. To further enforce the output caption sufficiently describing the visual content of the input image, we perform self-paraphrasing for the captioning model with fidelity and adequacy objectives introduced. Since no ground truth captions are available for novel object images during training, our P2C leverages cross-modality (image-text) association modules to ensure the above caption characteristics can be properly preserved. In the experiments, we not only show that our P2C achieves state-of-the-art performances on nocaps and COCO Caption datasets, we also verify the effectiveness and flexibility of our learning framework by replacing language and cross-modality association models for NOC. Implementation details and code are available in the supplementary materials.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2022-09-25T22:56:04Z</published>
    <arxiv:comment>Accepted at NeurIPS 2022</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Cheng-Fu Yang</name>
    </author>
    <author>
      <name>Yao-Hung Hubert Tsai</name>
    </author>
    <author>
      <name>Wan-Cyuan Fan</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
    <author>
      <name>Louis-Philippe Morency</name>
    </author>
    <author>
      <name>Yu-Chiang Frank Wang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.08466v3</id>
    <title>Simplifying Model-based RL: Learning Representations, Latent-space Models, and Policies with One Objective</title>
    <updated>2023-06-24T19:05:46Z</updated>
    <link href="https://arxiv.org/abs/2209.08466v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2209.08466v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>While reinforcement learning (RL) methods that learn an internal model of the environment have the potential to be more sample efficient than their model-free counterparts, learning to model raw observations from high dimensional sensors can be challenging. Prior work has addressed this challenge by learning low-dimensional representation of observations through auxiliary objectives, such as reconstruction or value prediction. However, the alignment between these auxiliary objectives and the RL objective is often unclear. In this work, we propose a single objective which jointly optimizes a latent-space model and policy to achieve high returns while remaining self-consistent. This objective is a lower bound on expected returns. Unlike prior bounds for model-based RL on policy exploration or model guarantees, our bound is directly on the overall RL objective. We demonstrate that the resulting algorithm matches or improves the sample-efficiency of the best prior model-based and model-free RL methods. While sample efficient methods typically are computationally demanding, our method attains the performance of SAC in about 50% less wall-clock time.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <published>2022-09-18T03:51:58Z</published>
    <arxiv:comment>ICLR 2023, Project website with code: https://alignedlatentmodels.github.io/</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Raj Ghugare</name>
    </author>
    <author>
      <name>Homanga Bharadhwaj</name>
    </author>
    <author>
      <name>Benjamin Eysenbach</name>
    </author>
    <author>
      <name>Sergey Levine</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.04396v4</id>
    <title>Graph Generative Model for Benchmarking Graph Neural Networks</title>
    <updated>2023-06-09T11:52:42Z</updated>
    <link href="https://arxiv.org/abs/2207.04396v4" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2207.04396v4" rel="related" type="application/pdf" title="pdf"/>
    <summary>As the field of Graph Neural Networks (GNN) continues to grow, it experiences a corresponding increase in the need for large, real-world datasets to train and test new GNN models on challenging, realistic problems. Unfortunately, such graph datasets are often generated from online, highly privacy-restricted ecosystems, which makes research and development on these datasets hard, if not impossible. This greatly reduces the amount of benchmark graphs available to researchers, causing the field to rely only on a handful of publicly-available datasets. To address this problem, we introduce a novel graph generative model, Computation Graph Transformer (CGT) that learns and reproduces the distribution of real-world graphs in a privacy-controlled way. More specifically, CGT (1) generates effective benchmark graphs on which GNNs show similar task performance as on the source graphs, (2) scales to process large-scale graphs, (3) incorporates off-the-shelf privacy modules to guarantee end-user privacy of the generated graph. Extensive experiments across a vast body of graph generative models show that only our model can successfully generate privacy-controlled, synthetic substitutes of large-scale real-world graphs that can be effectively used to benchmark GNN models.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <published>2022-07-10T06:42:02Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Minji Yoon</name>
    </author>
    <author>
      <name>Yue Wu</name>
    </author>
    <author>
      <name>John Palowitch</name>
    </author>
    <author>
      <name>Bryan Perozzi</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.00056v3</id>
    <title>MultiViz: Towards Visualizing and Understanding Multimodal Models</title>
    <updated>2023-03-06T19:39:18Z</updated>
    <link href="https://arxiv.org/abs/2207.00056v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2207.00056v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>The promise of multimodal models for real-world applications has inspired research in visualizing and understanding their internal mechanics with the end goal of empowering stakeholders to visualize model behavior, perform model debugging, and promote trust in machine learning models. However, modern multimodal models are typically black-box neural networks, which makes it challenging to understand their internal mechanics. How can we visualize the internal modeling of multimodal interactions in these models? Our paper aims to fill this gap by proposing MultiViz, a method for analyzing the behavior of multimodal models by scaffolding the problem of interpretability into 4 stages: (1) unimodal importance: how each modality contributes towards downstream modeling and prediction, (2) cross-modal interactions: how different modalities relate with each other, (3) multimodal representations: how unimodal and cross-modal interactions are represented in decision-level features, and (4) multimodal prediction: how decision-level features are composed to make a prediction. MultiViz is designed to operate on diverse modalities, models, tasks, and research areas. Through experiments on 8 trained models across 6 real-world tasks, we show that the complementary stages in MultiViz together enable users to (1) simulate model predictions, (2) assign interpretable concepts to features, (3) perform error analysis on model misclassifications, and (4) use insights from error analysis to debug models. MultiViz is publicly available, will be regularly updated with new interpretation tools and metrics, and welcomes inputs from the community.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <published>2022-06-30T18:42:06Z</published>
    <arxiv:comment>ICLR 2023. Code available at: https://github.com/pliang279/MultiViz</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Paul Pu Liang</name>
    </author>
    <author>
      <name>Yiwei Lyu</name>
    </author>
    <author>
      <name>Gunjan Chhablani</name>
    </author>
    <author>
      <name>Nihal Jain</name>
    </author>
    <author>
      <name>Zihao Deng</name>
    </author>
    <author>
      <name>Xingbo Wang</name>
    </author>
    <author>
      <name>Louis-Philippe Morency</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.13396v2</id>
    <title>A Simple Approach for Visual Rearrangement: 3D Mapping and Semantic Search</title>
    <updated>2022-08-09T20:47:35Z</updated>
    <link href="https://arxiv.org/abs/2206.13396v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2206.13396v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>Physically rearranging objects is an important capability for embodied agents. Visual room rearrangement evaluates an agent's ability to rearrange objects in a room to a desired goal based solely on visual input. We propose a simple yet effective method for this problem: (1) search for and map which objects need to be rearranged, and (2) rearrange each object until the task is complete. Our approach consists of an off-the-shelf semantic segmentation model, voxel-based semantic map, and semantic search policy to efficiently find objects that need to be rearranged. On the AI2-THOR Rearrangement Challenge, our method improves on current state-of-the-art end-to-end reinforcement learning-based methods that learn visual rearrangement policies from 0.53% correct rearrangement to 16.56%, using only 2.7% as many samples from the environment.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <published>2022-06-21T02:33:57Z</published>
    <arxiv:comment>Winner of the Rearrangement Challenge at CVPR 2022</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Brandon Trabucco</name>
    </author>
    <author>
      <name>Gunnar Sigurdsson</name>
    </author>
    <author>
      <name>Robinson Piramuthu</name>
    </author>
    <author>
      <name>Gaurav S. Sukhatme</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.07568v2</id>
    <title>Contrastive Learning as Goal-Conditioned Reinforcement Learning</title>
    <updated>2023-02-17T21:53:23Z</updated>
    <link href="https://arxiv.org/abs/2206.07568v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2206.07568v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>In reinforcement learning (RL), it is easier to solve a task if given a good representation. While deep RL should automatically acquire such good representations, prior work often finds that learning representations in an end-to-end fashion is unstable and instead equip RL algorithms with additional representation learning parts (e.g., auxiliary losses, data augmentation). How can we design RL algorithms that directly acquire good representations? In this paper, instead of adding representation learning parts to an existing RL algorithm, we show (contrastive) representation learning methods can be cast as RL algorithms in their own right. To do this, we build upon prior work and apply contrastive representation learning to action-labeled trajectories, in such a way that the (inner product of) learned representations exactly corresponds to a goal-conditioned value function. We use this idea to reinterpret a prior RL method as performing contrastive learning, and then use the idea to propose a much simpler method that achieves similar performance. Across a range of goal-conditioned RL tasks, we demonstrate that contrastive RL methods achieve higher success rates than prior non-contrastive methods, including in the offline RL setting. We also show that contrastive RL outperforms prior methods on image-based tasks, without using data augmentation or auxiliary objectives.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2022-06-15T14:34:15Z</published>
    <arxiv:comment>NeurIPS 2022. Code is available on the website: https://ben-eysenbach.github.io/contrastive_rl</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Benjamin Eysenbach</name>
    </author>
    <author>
      <name>Tianjun Zhang</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
    <author>
      <name>Sergey Levine</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.04615v3</id>
    <title>Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models</title>
    <updated>2023-06-12T17:51:15Z</updated>
    <link href="https://arxiv.org/abs/2206.04615v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2206.04615v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit "breakthrough" behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2022-06-09T17:05:34Z</published>
    <arxiv:comment>27 pages, 17 figures + references and appendices, repo: https://github.com/google/BIG-bench</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <arxiv:journal_ref>Transactions on Machine Learning Research, May/2022, https://openreview.net/forum?id=uyTL5Bvosj</arxiv:journal_ref>
    <author>
      <name>Aarohi Srivastava</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Abhinav Rastogi</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Abhishek Rao</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Abu Awal Md Shoeb</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Abubakar Abid</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Adam Fisch</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Adam R. Brown</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Adam Santoro</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Aditya Gupta</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Adrià Garriga-Alonso</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Agnieszka Kluska</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Aitor Lewkowycz</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Akshat Agarwal</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Alethea Power</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Alex Ray</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Alex Warstadt</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Alexander W. Kocurek</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ali Safaya</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ali Tazarv</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Alice Xiang</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Alicia Parrish</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Allen Nie</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Aman Hussain</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Amanda Askell</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Amanda Dsouza</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ambrose Slone</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ameet Rahane</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Anantharaman S. Iyer</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Anders Andreassen</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Andrea Madotto</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Andrea Santilli</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Andreas Stuhlmüller</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Andrew Dai</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Andrew La</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Andrew Lampinen</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Andy Zou</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Angela Jiang</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Angelica Chen</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Anh Vuong</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Animesh Gupta</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Anna Gottardi</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Antonio Norelli</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Anu Venkatesh</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Arash Gholamidavoodi</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Arfa Tabassum</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Arul Menezes</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Arun Kirubarajan</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Asher Mullokandov</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ashish Sabharwal</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Austin Herrick</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Avia Efrat</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Aykut Erdem</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ayla Karakaş</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>B. Ryan Roberts</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Bao Sheng Loe</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Barret Zoph</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Bartłomiej Bojanowski</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Batuhan Özyurt</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Behnam Hedayatnia</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Behnam Neyshabur</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Benjamin Inden</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Benno Stein</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Berk Ekmekci</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Bill Yuchen Lin</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Blake Howald</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Bryan Orinion</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Cameron Diao</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Cameron Dour</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Catherine Stinson</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Cedrick Argueta</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>César Ferri Ramírez</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Chandan Singh</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Charles Rathkopf</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Chenlin Meng</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Chitta Baral</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Chiyu Wu</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Chris Callison-Burch</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Chris Waites</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Christian Voigt</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Christopher D. Manning</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Christopher Potts</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Cindy Ramirez</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Clara E. Rivera</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Clemencia Siro</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Colin Raffel</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Courtney Ashcraft</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Cristina Garbacea</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Damien Sileo</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Dan Garrette</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Dan Hendrycks</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Dan Kilman</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Dan Roth</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Daniel Freeman</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Daniel Khashabi</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Daniel Levy</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Daniel Moseguí González</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Danielle Perszyk</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Danny Hernandez</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Danqi Chen</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Daphne Ippolito</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Dar Gilboa</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>David Dohan</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>David Drakard</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>David Jurgens</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Debajyoti Datta</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Deep Ganguli</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Denis Emelin</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Denis Kleyko</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Deniz Yuret</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Derek Chen</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Derek Tam</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Dieuwke Hupkes</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Diganta Misra</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Dilyar Buzan</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Dimitri Coelho Mollo</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Diyi Yang</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Dong-Ho Lee</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Dylan Schrader</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ekaterina Shutova</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ekin Dogus Cubuk</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Elad Segal</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Eleanor Hagerman</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Elizabeth Barnes</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Elizabeth Donoway</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ellie Pavlick</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Emanuele Rodola</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Emma Lam</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Eric Chu</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Eric Tang</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Erkut Erdem</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ernie Chang</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ethan A. Chi</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ethan Dyer</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ethan Jerzak</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ethan Kim</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Eunice Engefu Manyasi</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Evgenii Zheltonozhskii</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Fanyue Xia</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Fatemeh Siar</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Fernando Martínez-Plumed</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Francesca Happé</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Francois Chollet</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Frieda Rong</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Gaurav Mishra</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Genta Indra Winata</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Gerard de Melo</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Germán Kruszewski</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Giambattista Parascandolo</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Giorgio Mariani</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Gloria Wang</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Gonzalo Jaimovitch-López</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Gregor Betz</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Guy Gur-Ari</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Hana Galijasevic</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Hannah Kim</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Hannah Rashkin</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Hannaneh Hajishirzi</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Harsh Mehta</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Hayden Bogar</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Henry Shevlin</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Hinrich Schütze</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Hiromu Yakura</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Hongming Zhang</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Hugh Mee Wong</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ian Ng</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Isaac Noble</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jaap Jumelet</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jack Geissinger</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jackson Kernion</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jacob Hilton</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jaehoon Lee</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jaime Fernández Fisac</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>James B. Simon</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>James Koppel</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>James Zheng</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>James Zou</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jan Kocoń</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jana Thompson</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Janelle Wingfield</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jared Kaplan</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jarema Radom</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jascha Sohl-Dickstein</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jason Phang</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jason Wei</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jason Yosinski</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jekaterina Novikova</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jelle Bosscher</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jennifer Marsh</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jeremy Kim</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jeroen Taal</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jesse Engel</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jesujoba Alabi</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jiacheng Xu</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jiaming Song</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jillian Tang</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Joan Waweru</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>John Burden</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>John Miller</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>John U. Balis</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jonathan Batchelder</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jonathan Berant</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jörg Frohberg</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jos Rozen</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jose Hernandez-Orallo</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Joseph Boudeman</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Joseph Guerr</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Joseph Jones</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Joshua B. Tenenbaum</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Joshua S. Rule</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Joyce Chua</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Kamil Kanclerz</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Karen Livescu</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Karl Krauth</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Karthik Gopalakrishnan</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Katerina Ignatyeva</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Katja Markert</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Kaustubh D. Dhole</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Kevin Gimpel</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Kevin Omondi</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Kory Mathewson</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Kristen Chiafullo</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ksenia Shkaruta</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Kumar Shridhar</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Kyle McDonell</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Kyle Richardson</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Laria Reynolds</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Leo Gao</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Li Zhang</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Liam Dugan</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Lianhui Qin</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Lidia Contreras-Ochando</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Louis-Philippe Morency</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Luca Moschella</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Lucas Lam</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Lucy Noble</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ludwig Schmidt</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Luheng He</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Luis Oliveros Colón</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Luke Metz</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Lütfi Kerem Şenel</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Maarten Bosma</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Maarten Sap</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Maartje ter Hoeve</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Maheen Farooqi</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Manaal Faruqui</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Mantas Mazeika</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Marco Baturan</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Marco Marelli</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Marco Maru</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Maria Jose Ramírez Quintana</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Marie Tolkiehn</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Mario Giulianelli</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Martha Lewis</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Martin Potthast</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Matthew L. Leavitt</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Matthias Hagen</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Mátyás Schubert</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Medina Orduna Baitemirova</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Melody Arnaud</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Melvin McElrath</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Michael A. Yee</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Michael Cohen</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Michael Gu</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Michael Ivanitskiy</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Michael Starritt</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Michael Strube</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Michał Swędrowski</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Michele Bevilacqua</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Michihiro Yasunaga</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Mihir Kale</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Mike Cain</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Mimee Xu</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Mirac Suzgun</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Mitch Walker</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Mo Tiwari</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Mohit Bansal</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Moin Aminnaseri</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Mor Geva</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Mozhdeh Gheini</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Mukund Varma T</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Nanyun Peng</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Nathan A. Chi</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Nayeon Lee</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Neta Gur-Ari Krakover</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Nicholas Cameron</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Nicholas Roberts</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Nick Doiron</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Nicole Martinez</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Nikita Nangia</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Niklas Deckers</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Niklas Muennighoff</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Nitish Shirish Keskar</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Niveditha S. Iyer</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Noah Constant</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Noah Fiedel</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Nuan Wen</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Oliver Zhang</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Omar Agha</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Omar Elbaghdadi</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Omer Levy</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Owain Evans</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Pablo Antonio Moreno Casares</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Parth Doshi</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Pascale Fung</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Paul Pu Liang</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Paul Vicol</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Pegah Alipoormolabashi</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Peiyuan Liao</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Percy Liang</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Peter Chang</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Peter Eckersley</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Phu Mon Htut</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Pinyu Hwang</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Piotr Miłkowski</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Piyush Patil</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Pouya Pezeshkpour</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Priti Oli</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Qiaozhu Mei</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Qing Lyu</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Qinlang Chen</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Rabin Banjade</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Rachel Etta Rudolph</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Raefer Gabriel</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Rahel Habacker</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ramon Risco</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Raphaël Millière</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Rhythm Garg</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Richard Barnes</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Rif A. Saurous</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Riku Arakawa</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Robbe Raymaekers</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Robert Frank</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Rohan Sikand</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Roman Novak</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Roman Sitelew</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ronan LeBras</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Rosanne Liu</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Rowan Jacobs</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Rui Zhang</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ryan Chi</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ryan Lee</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ryan Stovall</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ryan Teehan</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Rylan Yang</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Sahib Singh</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Saif M. Mohammad</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Sajant Anand</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Sam Dillavou</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Sam Shleifer</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Sam Wiseman</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Samuel Gruetter</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Samuel R. Bowman</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Samuel S. Schoenholz</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Sanghyun Han</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Sanjeev Kwatra</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Sarah A. Rous</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Sarik Ghazarian</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Sayan Ghosh</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Sean Casey</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Sebastian Bischoff</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Sebastian Gehrmann</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Sebastian Schuster</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Sepideh Sadeghi</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Shadi Hamdan</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Sharon Zhou</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Shashank Srivastava</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Sherry Shi</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Shikhar Singh</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Shima Asaadi</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Shixiang Shane Gu</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Shubh Pachchigar</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Shubham Toshniwal</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Shyam Upadhyay</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name> Shyamolima</name>
      <arxiv:affiliation>Shammie</arxiv:affiliation>
    </author>
    <author>
      <name> Debnath</name>
    </author>
    <author>
      <name>Siamak Shakeri</name>
    </author>
    <author>
      <name>Simon Thormeyer</name>
    </author>
    <author>
      <name>Simone Melzi</name>
    </author>
    <author>
      <name>Siva Reddy</name>
    </author>
    <author>
      <name>Sneha Priscilla Makini</name>
    </author>
    <author>
      <name>Soo-Hwan Lee</name>
    </author>
    <author>
      <name>Spencer Torene</name>
    </author>
    <author>
      <name>Sriharsha Hatwar</name>
    </author>
    <author>
      <name>Stanislas Dehaene</name>
    </author>
    <author>
      <name>Stefan Divic</name>
    </author>
    <author>
      <name>Stefano Ermon</name>
    </author>
    <author>
      <name>Stella Biderman</name>
    </author>
    <author>
      <name>Stephanie Lin</name>
    </author>
    <author>
      <name>Stephen Prasad</name>
    </author>
    <author>
      <name>Steven T. Piantadosi</name>
    </author>
    <author>
      <name>Stuart M. Shieber</name>
    </author>
    <author>
      <name>Summer Misherghi</name>
    </author>
    <author>
      <name>Svetlana Kiritchenko</name>
    </author>
    <author>
      <name>Swaroop Mishra</name>
    </author>
    <author>
      <name>Tal Linzen</name>
    </author>
    <author>
      <name>Tal Schuster</name>
    </author>
    <author>
      <name>Tao Li</name>
    </author>
    <author>
      <name>Tao Yu</name>
    </author>
    <author>
      <name>Tariq Ali</name>
    </author>
    <author>
      <name>Tatsu Hashimoto</name>
    </author>
    <author>
      <name>Te-Lin Wu</name>
    </author>
    <author>
      <name>Théo Desbordes</name>
    </author>
    <author>
      <name>Theodore Rothschild</name>
    </author>
    <author>
      <name>Thomas Phan</name>
    </author>
    <author>
      <name>Tianle Wang</name>
    </author>
    <author>
      <name>Tiberius Nkinyili</name>
    </author>
    <author>
      <name>Timo Schick</name>
    </author>
    <author>
      <name>Timofei Kornev</name>
    </author>
    <author>
      <name>Titus Tunduny</name>
    </author>
    <author>
      <name>Tobias Gerstenberg</name>
    </author>
    <author>
      <name>Trenton Chang</name>
    </author>
    <author>
      <name>Trishala Neeraj</name>
    </author>
    <author>
      <name>Tushar Khot</name>
    </author>
    <author>
      <name>Tyler Shultz</name>
    </author>
    <author>
      <name>Uri Shaham</name>
    </author>
    <author>
      <name>Vedant Misra</name>
    </author>
    <author>
      <name>Vera Demberg</name>
    </author>
    <author>
      <name>Victoria Nyamai</name>
    </author>
    <author>
      <name>Vikas Raunak</name>
    </author>
    <author>
      <name>Vinay Ramasesh</name>
    </author>
    <author>
      <name>Vinay Uday Prabhu</name>
    </author>
    <author>
      <name>Vishakh Padmakumar</name>
    </author>
    <author>
      <name>Vivek Srikumar</name>
    </author>
    <author>
      <name>William Fedus</name>
    </author>
    <author>
      <name>William Saunders</name>
    </author>
    <author>
      <name>William Zhang</name>
    </author>
    <author>
      <name>Wout Vossen</name>
    </author>
    <author>
      <name>Xiang Ren</name>
    </author>
    <author>
      <name>Xiaoyu Tong</name>
    </author>
    <author>
      <name>Xinran Zhao</name>
    </author>
    <author>
      <name>Xinyi Wu</name>
    </author>
    <author>
      <name>Xudong Shen</name>
    </author>
    <author>
      <name>Yadollah Yaghoobzadeh</name>
    </author>
    <author>
      <name>Yair Lakretz</name>
    </author>
    <author>
      <name>Yangqiu Song</name>
    </author>
    <author>
      <name>Yasaman Bahri</name>
    </author>
    <author>
      <name>Yejin Choi</name>
    </author>
    <author>
      <name>Yichi Yang</name>
    </author>
    <author>
      <name>Yiding Hao</name>
    </author>
    <author>
      <name>Yifu Chen</name>
    </author>
    <author>
      <name>Yonatan Belinkov</name>
    </author>
    <author>
      <name>Yu Hou</name>
    </author>
    <author>
      <name>Yufang Hou</name>
    </author>
    <author>
      <name>Yuntao Bai</name>
    </author>
    <author>
      <name>Zachary Seid</name>
    </author>
    <author>
      <name>Zhuoye Zhao</name>
    </author>
    <author>
      <name>Zijian Wang</name>
    </author>
    <author>
      <name>Zijie J. Wang</name>
    </author>
    <author>
      <name>Zirui Wang</name>
    </author>
    <author>
      <name>Ziyi Wu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.03378v2</id>
    <title>Imitating Past Successes can be Very Suboptimal</title>
    <updated>2023-02-17T21:39:57Z</updated>
    <link href="https://arxiv.org/abs/2206.03378v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2206.03378v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>Prior work has proposed a simple strategy for reinforcement learning (RL): label experience with the outcomes achieved in that experience, and then imitate the relabeled experience. These outcome-conditioned imitation learning methods are appealing because of their simplicity, strong performance, and close ties with supervised learning. However, it remains unclear how these methods relate to the standard RL objective, reward maximization. In this paper, we formally relate outcome-conditioned imitation learning to reward maximization, drawing a precise relationship between the learned policy and Q-values and explaining the close connections between these methods and prior EM-based policy search methods. This analysis shows that existing outcome-conditioned imitation learning methods do not necessarily improve the policy, but a simple modification results in a method that does guarantee policy improvement, under some assumptions.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2022-06-07T15:13:43Z</published>
    <arxiv:comment>NeurIPS 2022</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Benjamin Eysenbach</name>
    </author>
    <author>
      <name>Soumith Udatha</name>
    </author>
    <author>
      <name>Sergey Levine</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.12898v1</id>
    <title>Reasoning over Logically Interacted Conditions for Question Answering</title>
    <updated>2022-05-25T16:41:39Z</updated>
    <link href="https://arxiv.org/abs/2205.12898v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2205.12898v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Some questions have multiple answers that are not equally correct, i.e. answers are different under different conditions. Conditions are used to distinguish answers as well as to provide additional information to support them. In this paper, we study a more challenging task where answers are constrained by a list of conditions that logically interact, which requires performing logical reasoning over the conditions to determine the correctness of the answers. Even more challenging, we only provide evidences for a subset of the conditions, so some questions may not have deterministic answers. In such cases, models are asked to find probable answers and identify conditions that need to be satisfied to make the answers correct. We propose a new model, TReasoner, for this challenging reasoning task. TReasoner consists of an entailment module, a reasoning module, and a generation module (if the answers are free-form text spans). TReasoner achieves state-of-the-art performance on two benchmark conditional QA datasets, outperforming the previous state-of-the-art by 3-10 points.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2022-05-25T16:41:39Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Haitian Sun</name>
    </author>
    <author>
      <name>William W. Cohen</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.11130v3</id>
    <title>PACS: A Dataset for Physical Audiovisual CommonSense Reasoning</title>
    <updated>2022-08-01T05:23:54Z</updated>
    <link href="https://arxiv.org/abs/2203.11130v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2203.11130v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>In order for AI to be safely deployed in real-world scenarios such as hospitals, schools, and the workplace, it must be able to robustly reason about the physical world. Fundamental to this reasoning is physical common sense: understanding the physical properties and affordances of available objects, how they can be manipulated, and how they interact with other objects. Physical commonsense reasoning is fundamentally a multi-sensory task, since physical properties are manifested through multiple modalities - two of them being vision and acoustics. Our paper takes a step towards real-world physical commonsense reasoning by contributing PACS: the first audiovisual benchmark annotated for physical commonsense attributes. PACS contains 13,400 question-answer pairs, involving 1,377 unique physical commonsense questions and 1,526 videos. Our dataset provides new opportunities to advance the research field of physical reasoning by bringing audio as a core component of this multimodal problem. Using PACS, we evaluate multiple state-of-the-art models on our new challenging task. While some models show promising results (70% accuracy), they all fall short of human performance (95% accuracy). We conclude the paper by demonstrating the importance of multimodal reasoning and providing possible avenues for future research.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <published>2022-03-21T17:05:23Z</published>
    <arxiv:comment>ECCV 2022, 51 pages, 23 figures, 4 tables</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Samuel Yu</name>
    </author>
    <author>
      <name>Peter Wu</name>
    </author>
    <author>
      <name>Paul Pu Liang</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
    <author>
      <name>Louis-Philippe Morency</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.02018v4</id>
    <title>Zero-shot Transfer Learning within a Heterogeneous Graph via Knowledge Transfer Networks</title>
    <updated>2022-10-12T22:32:10Z</updated>
    <link href="https://arxiv.org/abs/2203.02018v4" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2203.02018v4" rel="related" type="application/pdf" title="pdf"/>
    <summary>Data continuously emitted from industrial ecosystems such as social or e-commerce platforms are commonly represented as heterogeneous graphs (HG) composed of multiple node/edge types. State-of-the-art graph learning methods for HGs known as heterogeneous graph neural networks (HGNNs) are applied to learn deep context-informed node representations. However, many HG datasets from industrial applications suffer from label imbalance between node types. As there is no direct way to learn using labels rooted at different node types, HGNNs have been applied to only a few node types with abundant labels. We propose a zero-shot transfer learning module for HGNNs called a Knowledge Transfer Network (KTN) that transfers knowledge from label-abundant node types to zero-labeled node types through rich relational information given in the HG. KTN is derived from the theoretical relationship, which we introduce in this work, between distinct feature extractors for each node type given in an HGNN model. KTN improves performance of 6 different types of HGNN models by up to 960% for inference on zero-labeled node types and outperforms state-of-the-art transfer learning baselines by up to 73% across 18 different transfer learning tasks on HGs.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2022-03-03T21:00:23Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Minji Yoon</name>
    </author>
    <author>
      <name>John Palowitch</name>
    </author>
    <author>
      <name>Dustin Zelle</name>
    </author>
    <author>
      <name>Ziniu Hu</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
    <author>
      <name>Bryan Perozzi</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.02013v1</id>
    <title>DIME: Fine-grained Interpretations of Multimodal Models via Disentangled Local Explanations</title>
    <updated>2022-03-03T20:52:47Z</updated>
    <link href="https://arxiv.org/abs/2203.02013v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2203.02013v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The ability for a human to understand an Artificial Intelligence (AI) model's decision-making process is critical in enabling stakeholders to visualize model behavior, perform model debugging, promote trust in AI models, and assist in collaborative human-AI decision-making. As a result, the research fields of interpretable and explainable AI have gained traction within AI communities as well as interdisciplinary scientists seeking to apply AI in their subject areas. In this paper, we focus on advancing the state-of-the-art in interpreting multimodal models - a class of machine learning methods that tackle core challenges in representing and capturing interactions between heterogeneous data sources such as images, text, audio, and time-series data. Multimodal models have proliferated numerous real-world applications across healthcare, robotics, multimedia, affective computing, and human-computer interaction. By performing model disentanglement into unimodal contributions (UC) and multimodal interactions (MI), our proposed approach, DIME, enables accurate and fine-grained analysis of multimodal models while maintaining generality across arbitrary modalities, model architectures, and tasks. Through a comprehensive suite of experiments on both synthetic and real-world multimodal tasks, we show that DIME generates accurate disentangled explanations, helps users of multimodal models gain a deeper understanding of model behavior, and presents a step towards debugging and improving these models for real-world deployment. Code for our experiments can be found at https://github.com/lvyiwei1/DIME.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2022-03-03T20:52:47Z</published>
    <arxiv:comment>Code available at https://github.com/lvyiwei1/DIME</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Yiwei Lyu</name>
    </author>
    <author>
      <name>Paul Pu Liang</name>
    </author>
    <author>
      <name>Zihao Deng</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
    <author>
      <name>Louis-Philippe Morency</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.01311v4</id>
    <title>High-Modality Multimodal Transformer: Quantifying Modality &amp; Interaction Heterogeneity for High-Modality Representation Learning</title>
    <updated>2023-06-28T17:58:11Z</updated>
    <link href="https://arxiv.org/abs/2203.01311v4" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2203.01311v4" rel="related" type="application/pdf" title="pdf"/>
    <summary>Many real-world problems are inherently multimodal, from spoken language, gestures, and paralinguistics humans use to communicate, to force, proprioception, and visual sensors on robots. While there has been an explosion of interest in multimodal learning, these methods are focused on a small set of modalities primarily in language, vision, and audio. In order to accelerate generalization towards diverse and understudied modalities, this paper studies efficient representation learning for high-modality scenarios involving a large set of diverse modalities. Since adding new models for every new modality becomes prohibitively expensive, a critical technical challenge is heterogeneity quantification: how can we measure which modalities encode similar information and interactions in order to permit parameter sharing with previous modalities? This paper proposes two new information theoretic metrics for heterogeneity quantification: (1) modality heterogeneity studies how similar 2 modalities {X1,X2} are by measuring how much information can be transferred from X1 to X2, while (2) interaction heterogeneity studies how similarly pairs of modalities {X1,X2}, {X3,X4} interact by measuring how much information can be transferred from fusing {X1,X2} to {X3,X4}. We show the importance of these 2 proposed metrics as a way to automatically prioritize the fusion of modalities that contain unique information or interactions. The result is a single model, HighMMT, that scales up to 10 modalities (text, image, audio, video, sensors, proprioception, speech, time-series, sets, and tables) and 15 tasks from 5 research areas. Not only does HighMMT outperform prior methods on the tradeoff between performance and efficiency, it also demonstrates a crucial scaling behavior: performance continues to improve with each modality added, and it transfers to entirely new modalities and tasks during fine-tuning.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <published>2022-03-02T18:56:20Z</published>
    <arxiv:comment>TMLR 2023, Code available at https://github.com/pliang279/HighMMT</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Paul Pu Liang</name>
    </author>
    <author>
      <name>Yiwei Lyu</name>
    </author>
    <author>
      <name>Xiang Fan</name>
    </author>
    <author>
      <name>Jeffrey Tsaw</name>
    </author>
    <author>
      <name>Yudong Liu</name>
    </author>
    <author>
      <name>Shentong Mo</name>
    </author>
    <author>
      <name>Dani Yogatama</name>
    </author>
    <author>
      <name>Louis-Philippe Morency</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.06670v2</id>
    <title>Learning Weakly-Supervised Contrastive Representations</title>
    <updated>2022-02-18T11:49:01Z</updated>
    <link href="https://arxiv.org/abs/2202.06670v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2202.06670v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>We argue that a form of the valuable information provided by the auxiliary information is its implied data clustering information. For instance, considering hashtags as auxiliary information, we can hypothesize that an Instagram image will be semantically more similar with the same hashtags. With this intuition, we present a two-stage weakly-supervised contrastive learning approach. The first stage is to cluster data according to its auxiliary information. The second stage is to learn similar representations within the same cluster and dissimilar representations for data from different clusters. Our empirical experiments suggest the following three contributions. First, compared to conventional self-supervised representations, the auxiliary-information-infused representations bring the performance closer to the supervised representations, which use direct downstream labels as supervision signals. Second, our approach performs the best in most cases, when comparing our approach with other baseline representation learning methods that also leverage auxiliary data information. Third, we show that our approach also works well with unsupervised constructed clusters (e.g., no auxiliary information), resulting in a strong unsupervised representation learning approach.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2022-02-14T12:57:31Z</published>
    <arxiv:comment>Published as ICLR 2022. arXiv admin note: substantial text overlap with arXiv:2106.02869</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Yao-Hung Hubert Tsai</name>
    </author>
    <author>
      <name>Tianqin Li</name>
    </author>
    <author>
      <name>Weixin Liu</name>
    </author>
    <author>
      <name>Peiyuan Liao</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
    <author>
      <name>Louis-Philippe Morency</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.05458v3</id>
    <title>Conditional Contrastive Learning with Kernel</title>
    <updated>2022-03-15T06:08:14Z</updated>
    <link href="https://arxiv.org/abs/2202.05458v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2202.05458v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>Conditional contrastive learning frameworks consider the conditional sampling procedure that constructs positive or negative data pairs conditioned on specific variables. Fair contrastive learning constructs negative pairs, for example, from the same gender (conditioning on sensitive information), which in turn reduces undesirable information from the learned representations; weakly supervised contrastive learning constructs positive pairs with similar annotative attributes (conditioning on auxiliary information), which in turn are incorporated into the representations. Although conditional contrastive learning enables many applications, the conditional sampling procedure can be challenging if we cannot obtain sufficient data pairs for some values of the conditioning variable. This paper presents Conditional Contrastive Learning with Kernel (CCL-K) that converts existing conditional contrastive objectives into alternative forms that mitigate the insufficient data problem. Instead of sampling data according to the value of the conditioning variable, CCL-K uses the Kernel Conditional Embedding Operator that samples data from all available data and assigns weights to each sampled data given the kernel similarity between the values of the conditioning variable. We conduct experiments using weakly supervised, fair, and hard negatives contrastive learning, showing CCL-K outperforms state-of-the-art baselines.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2022-02-11T05:37:54Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Yao-Hung Hubert Tsai</name>
    </author>
    <author>
      <name>Tianqin Li</name>
    </author>
    <author>
      <name>Martin Q. Ma</name>
    </author>
    <author>
      <name>Han Zhao</name>
    </author>
    <author>
      <name>Kun Zhang</name>
    </author>
    <author>
      <name>Louis-Philippe Morency</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.01001v1</id>
    <title>SEAL: Self-supervised Embodied Active Learning using Exploration and 3D Consistency</title>
    <updated>2021-12-02T06:26:38Z</updated>
    <link href="https://arxiv.org/abs/2112.01001v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2112.01001v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>In this paper, we explore how we can build upon the data and models of Internet images and use them to adapt to robot vision without requiring any extra labels. We present a framework called Self-supervised Embodied Active Learning (SEAL). It utilizes perception models trained on internet images to learn an active exploration policy. The observations gathered by this exploration policy are labelled using 3D consistency and used to improve the perception model. We build and utilize 3D semantic maps to learn both action and perception in a completely self-supervised manner. The semantic map is used to compute an intrinsic motivation reward for training the exploration policy and for labelling the agent observations using spatio-temporal 3D consistency and label propagation. We demonstrate that the SEAL framework can be used to close the action-perception loop: it improves object detection and instance segmentation performance of a pretrained perception model by just moving around in training environments and the improved perception model can be used to improve Object Goal Navigation.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <published>2021-12-02T06:26:38Z</published>
    <arxiv:comment>Published at NeurIPS 2021. See project webpage at https://devendrachaplot.github.io/projects/seal</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Devendra Singh Chaplot</name>
    </author>
    <author>
      <name>Murtaza Dalal</name>
    </author>
    <author>
      <name>Saurabh Gupta</name>
    </author>
    <author>
      <name>Jitendra Malik</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.15360v1</id>
    <title>Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives</title>
    <updated>2021-10-28T17:59:30Z</updated>
    <link href="https://arxiv.org/abs/2110.15360v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2110.15360v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Despite the potential of reinforcement learning (RL) for building general-purpose robotic systems, training RL agents to solve robotics tasks still remains challenging due to the difficulty of exploration in purely continuous action spaces. Addressing this problem is an active area of research with the majority of focus on improving RL methods via better optimization or more efficient exploration. An alternate but important component to consider improving is the interface of the RL algorithm with the robot. In this work, we manually specify a library of robot action primitives (RAPS), parameterized with arguments that are learned by an RL policy. These parameterized primitives are expressive, simple to implement, enable efficient exploration and can be transferred across robots, tasks and environments. We perform a thorough empirical study across challenging tasks in three distinct domains with image input and a sparse terminal reward. We find that our simple change to the action interface substantially improves both the learning efficiency and task performance irrespective of the underlying RL algorithm, significantly outperforming prior methods which learn skills from offline expert data. Code and videos at https://mihdalal.github.io/raps/</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <published>2021-10-28T17:59:30Z</published>
    <arxiv:comment>Published at NeurIPS 2021. Website at https://mihdalal.github.io/raps/</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Murtaza Dalal</name>
    </author>
    <author>
      <name>Deepak Pathak</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.12080v1</id>
    <title>C-Planning: An Automatic Curriculum for Learning Goal-Reaching Tasks</title>
    <updated>2021-10-22T22:05:31Z</updated>
    <link href="https://arxiv.org/abs/2110.12080v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2110.12080v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Goal-conditioned reinforcement learning (RL) can solve tasks in a wide range of domains, including navigation and manipulation, but learning to reach distant goals remains a central challenge to the field. Learning to reach such goals is particularly hard without any offline data, expert demonstrations, and reward shaping. In this paper, we propose an algorithm to solve the distant goal-reaching task by using search at training time to automatically generate a curriculum of intermediate states. Our algorithm, Classifier-Planning (C-Planning), frames the learning of the goal-conditioned policies as expectation maximization: the E-step corresponds to planning an optimal sequence of waypoints using graph search, while the M-step aims to learn a goal-conditioned policy to reach those waypoints. Unlike prior methods that combine goal-conditioned RL with graph search, ours performs search only during training and not testing, significantly decreasing the compute costs of deploying the learned policy. Empirically, we demonstrate that our method is more sample efficient than prior methods. Moreover, it is able to solve very long horizons manipulation and navigation tasks, tasks that prior goal-conditioned methods and methods based on graph search fail to solve.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2021-10-22T22:05:31Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Tianjun Zhang</name>
    </author>
    <author>
      <name>Benjamin Eysenbach</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
    <author>
      <name>Sergey Levine</name>
    </author>
    <author>
      <name>Joseph E. Gonzalez</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.06884v1</id>
    <title>ConditionalQA: A Complex Reading Comprehension Dataset with Conditional Answers</title>
    <updated>2021-10-13T17:16:46Z</updated>
    <link href="https://arxiv.org/abs/2110.06884v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2110.06884v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We describe a Question Answering (QA) dataset that contains complex questions with conditional answers, i.e. the answers are only applicable when certain conditions apply. We call this dataset ConditionalQA. In addition to conditional answers, the dataset also features: (1) long context documents with information that is related in logically complex ways; (2) multi-hop questions that require compositional logical reasoning; (3) a combination of extractive questions, yes/no questions, questions with multiple answers, and not-answerable questions; (4) questions asked without knowing the answers. We show that ConditionalQA is challenging for many of the existing QA models, especially in selecting answer conditions. We believe that this dataset will motivate further research in answering complex questions over long documents. Data and leaderboard are publicly available at \url{https://github.com/haitian-sun/ConditionalQA}.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2021-10-13T17:16:46Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Haitian Sun</name>
    </author>
    <author>
      <name>William W. Cohen</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.07342v3</id>
    <title>FILM: Following Instructions in Language with Modular Methods</title>
    <updated>2022-03-16T22:46:34Z</updated>
    <link href="https://arxiv.org/abs/2110.07342v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2110.07342v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>Recent methods for embodied instruction following are typically trained end-to-end using imitation learning. This often requires the use of expert trajectories and low-level language instructions. Such approaches assume that neural states will integrate multimodal semantics to perform state tracking, building spatial memory, exploration, and long-term planning. In contrast, we propose a modular method with structured representations that (1) builds a semantic map of the scene and (2) performs exploration with a semantic search policy, to achieve the natural language goal. Our modular method achieves SOTA performance (24.46 %) with a substantial (8.17 % absolute) gap from previous work while using less data by eschewing both expert trajectories and low-level instructions. Leveraging low-level language, however, can further increase our performance (26.49 %). Our findings suggest that an explicit spatial memory and a semantic search policy can provide a stronger and more general representation for state-tracking and guidance, even in the absence of expert trajectories or low-level instructions.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2021-10-12T16:40:01Z</published>
    <arxiv:comment>Published as a conference paper at International Conference on Learning Representations (ICLR) 2022</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>So Yeon Min</name>
    </author>
    <author>
      <name>Devendra Singh Chaplot</name>
    </author>
    <author>
      <name>Pradeep Ravikumar</name>
    </author>
    <author>
      <name>Yonatan Bisk</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.05038v3</id>
    <title>Recurrent Model-Free RL Can Be a Strong Baseline for Many POMDPs</title>
    <updated>2022-06-05T01:19:29Z</updated>
    <link href="https://arxiv.org/abs/2110.05038v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2110.05038v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>Many problems in RL, such as meta-RL, robust RL, generalization in RL, and temporal credit assignment, can be cast as POMDPs. In theory, simply augmenting model-free RL with memory-based architectures, such as recurrent neural networks, provides a general approach to solving all types of POMDPs. However, prior work has found that such recurrent model-free RL methods tend to perform worse than more specialized algorithms that are designed for specific types of POMDPs. This paper revisits this claim. We find that careful architecture and hyperparameter decisions can often yield a recurrent model-free implementation that performs on par with (and occasionally substantially better than) more sophisticated recent techniques. We compare to 21 environments from 6 prior specialized methods and find that our implementation achieves greater sample efficiency and asymptotic performance than these methods on 18/21 environments. We also release a simple and efficient implementation of recurrent model-free RL for future work to use as a baseline for POMDPs.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <published>2021-10-11T07:09:14Z</published>
    <arxiv:comment>ICML 2022 camera ready version. Code: https://github.com/twni2016/pomdp-baselines Project site: https://sites.google.com/view/pomdp-baselines</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Tianwei Ni</name>
    </author>
    <author>
      <name>Benjamin Eysenbach</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.02758v2</id>
    <title>Mismatched No More: Joint Model-Policy Optimization for Model-Based RL</title>
    <updated>2023-02-17T21:24:30Z</updated>
    <link href="https://arxiv.org/abs/2110.02758v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2110.02758v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>Many model-based reinforcement learning (RL) methods follow a similar template: fit a model to previously observed data, and then use data from that model for RL or planning. However, models that achieve better training performance (e.g., lower MSE) are not necessarily better for control: an RL agent may seek out the small fraction of states where an accurate model makes mistakes, or it might act in ways that do not expose the errors of an inaccurate model. As noted in prior work, there is an objective mismatch: models are useful if they yield good policies, but they are trained to maximize their accuracy, rather than the performance of the policies that result from them. In this work, we propose a single objective for jointly training the model and the policy, such that updates to either component increase a lower bound on expected return. To the best of our knowledge, this is the first lower bound for model-based RL that holds globally and can be efficiently estimated in continuous settings; it is the only lower bound that mends the objective mismatch problem. A version of this bound becomes tight under certain assumptions. Optimizing this bound resembles a GAN: a classifier distinguishes between real and fake transitions, the model is updated to produce transitions that look realistic, and the policy is updated to avoid states where the model predictions are unrealistic. Numerical simulations demonstrate that optimizing this bound yields reward maximizing policies and yields dynamics that (perhaps surprisingly) can aid in exploration. We also show that a deep RL algorithm loosely based on our lower bound can achieve performance competitive with prior model-based methods, and better performance on certain hard exploration tasks.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <published>2021-10-06T13:43:27Z</published>
    <arxiv:comment>NeurIPS 2022</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Benjamin Eysenbach</name>
    </author>
    <author>
      <name>Alexander Khazatsky</name>
    </author>
    <author>
      <name>Sergey Levine</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.02719v1</id>
    <title>The Information Geometry of Unsupervised Reinforcement Learning</title>
    <updated>2021-10-06T13:08:36Z</updated>
    <link href="https://arxiv.org/abs/2110.02719v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2110.02719v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>How can a reinforcement learning (RL) agent prepare to solve downstream tasks if those tasks are not known a priori? One approach is unsupervised skill discovery, a class of algorithms that learn a set of policies without access to a reward function. Such algorithms bear a close resemblance to representation learning algorithms (e.g., contrastive learning) in supervised learning, in that both are pretraining algorithms that maximize some approximation to a mutual information objective. While prior work has shown that the set of skills learned by such methods can accelerate downstream RL tasks, prior work offers little analysis into whether these skill learning algorithms are optimal, or even what notion of optimality would be appropriate to apply to them. In this work, we show that unsupervised skill discovery algorithms based on mutual information maximization do not learn skills that are optimal for every possible reward function. However, we show that the distribution over skills provides an optimal initialization minimizing regret against adversarially-chosen reward functions, assuming a certain type of adaptation procedure. Our analysis also provides a geometric perspective on these skill learning methods.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2021-10-06T13:08:36Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Benjamin Eysenbach</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
    <author>
      <name>Sergey Levine</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.12742v2</id>
    <title>FewNLU: Benchmarking State-of-the-Art Methods for Few-Shot Natural Language Understanding</title>
    <updated>2022-03-15T08:46:02Z</updated>
    <link href="https://arxiv.org/abs/2109.12742v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2109.12742v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>The few-shot natural language understanding (NLU) task has attracted much recent attention. However, prior methods have been evaluated under a disparate set of protocols, which hinders fair comparison and measuring progress of the field. To address this issue, we introduce an evaluation framework that improves previous evaluation procedures in three key aspects, i.e., test performance, dev-test correlation, and stability. Under this new evaluation framework, we re-evaluate several state-of-the-art few-shot methods for NLU tasks. Our framework reveals new insights: (1) both the absolute performance and relative gap of the methods were not accurately estimated in prior literature; (2) no single method dominates most tasks with consistent performance; (3) improvements of some methods diminish with a larger pretrained model; and (4) gains from different methods are often complementary and the best combined model performs close to a strong fully-supervised baseline. We open-source our toolkit, FewNLU, that implements our evaluation framework along with a number of state-of-the-art methods.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2021-09-27T00:57:30Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Yanan Zheng</name>
    </author>
    <author>
      <name>Jing Zhou</name>
    </author>
    <author>
      <name>Yujie Qian</name>
    </author>
    <author>
      <name>Ming Ding</name>
    </author>
    <author>
      <name>Chonghua Liao</name>
    </author>
    <author>
      <name>Jian Li</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
    <author>
      <name>Jie Tang</name>
    </author>
    <author>
      <name>Sebastian Ruder</name>
    </author>
    <author>
      <name>Zhilin Yang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.03214v1</id>
    <title>Robust Predictable Control</title>
    <updated>2021-09-07T17:29:34Z</updated>
    <link href="https://arxiv.org/abs/2109.03214v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2109.03214v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Many of the challenges facing today's reinforcement learning (RL) algorithms, such as robustness, generalization, transfer, and computational efficiency are closely related to compression. Prior work has convincingly argued why minimizing information is useful in the supervised learning setting, but standard RL algorithms lack an explicit mechanism for compression. The RL setting is unique because (1) its sequential nature allows an agent to use past information to avoid looking at future observations and (2) the agent can optimize its behavior to prefer states where decision making requires few bits. We take advantage of these properties to propose a method (RPC) for learning simple policies. This method brings together ideas from information bottlenecks, model-based RL, and bits-back coding into a simple and theoretically-justified algorithm. Our method jointly optimizes a latent-space model and policy to be self-consistent, such that the policy avoids states where the model is inaccurate. We demonstrate that our method achieves much tighter compression than prior methods, achieving up to 5x higher reward than a standard information bottleneck. We also demonstrate that our method learns policies that are more robust and generalize better to new tasks.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2021-09-07T17:29:34Z</published>
    <arxiv:comment>Project site with videos and code: https://ben-eysenbach.github.io/rpc</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Benjamin Eysenbach</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
    <author>
      <name>Sergey Levine</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.07502v2</id>
    <title>MultiBench: Multiscale Benchmarks for Multimodal Representation Learning</title>
    <updated>2021-11-10T07:31:56Z</updated>
    <link href="https://arxiv.org/abs/2107.07502v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2107.07502v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>Learning multimodal representations involves integrating information from multiple heterogeneous sources of data. It is a challenging yet crucial area with numerous real-world applications in multimedia, affective computing, robotics, finance, human-computer interaction, and healthcare. Unfortunately, multimodal research has seen limited resources to study (1) generalization across domains and modalities, (2) complexity during training and inference, and (3) robustness to noisy and missing modalities. In order to accelerate progress towards understudied modalities and tasks while ensuring real-world robustness, we release MultiBench, a systematic and unified large-scale benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas. MultiBench provides an automated end-to-end machine learning pipeline that simplifies and standardizes data loading, experimental setup, and model evaluation. To enable holistic evaluation, MultiBench offers a comprehensive methodology to assess (1) generalization, (2) time and space complexity, and (3) modality robustness. MultiBench introduces impactful challenges for future research, including scalability to large-scale multimodal datasets and robustness to realistic imperfections. To accompany this benchmark, we also provide a standardized implementation of 20 core approaches in multimodal learning. Simply applying methods proposed in different research areas can improve the state-of-the-art performance on 9/15 datasets. Therefore, MultiBench presents a milestone in unifying disjoint efforts in multimodal research and paves the way towards a better understanding of the capabilities and limitations of multimodal models, all the while ensuring ease of use, accessibility, and reproducibility. MultiBench, our standardized code, and leaderboards are publicly available, will be regularly updated, and welcomes inputs from the community.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <published>2021-07-15T17:54:36Z</published>
    <arxiv:comment>NeurIPS 2021 Datasets and Benchmarks Track. Code: https://github.com/pliang279/MultiBench and Website: https://cmu-multicomp-lab.github.io/multibench/</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Paul Pu Liang</name>
    </author>
    <author>
      <name>Yiwei Lyu</name>
    </author>
    <author>
      <name>Xiang Fan</name>
    </author>
    <author>
      <name>Zetian Wu</name>
    </author>
    <author>
      <name>Yun Cheng</name>
    </author>
    <author>
      <name>Jason Wu</name>
    </author>
    <author>
      <name>Leslie Chen</name>
    </author>
    <author>
      <name>Peter Wu</name>
    </author>
    <author>
      <name>Michelle A. Lee</name>
    </author>
    <author>
      <name>Yuke Zhu</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
    <author>
      <name>Louis-Philippe Morency</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.13219v1</id>
    <title>Towards Understanding and Mitigating Social Biases in Language Models</title>
    <updated>2021-06-24T17:52:43Z</updated>
    <link href="https://arxiv.org/abs/2106.13219v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2106.13219v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>As machine learning methods are deployed in real-world settings such as healthcare, legal systems, and social science, it is crucial to recognize how they shape social biases and stereotypes in these sensitive decision-making processes. Among such real-world deployments are large-scale pretrained language models (LMs) that can be potentially dangerous in manifesting undesirable representational biases - harmful biases resulting from stereotyping that propagate negative generalizations involving gender, race, religion, and other social constructs. As a step towards improving the fairness of LMs, we carefully define several sources of representational biases before proposing new benchmarks and metrics to measure them. With these tools, we propose steps towards mitigating social biases during text generation. Our empirical results and human evaluation demonstrate effectiveness in mitigating bias while retaining crucial contextual information for high-fidelity text generation, thereby pushing forward the performance-fairness Pareto frontier.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2021-06-24T17:52:43Z</published>
    <arxiv:comment>ICML 2021, code available at https://github.com/pliang279/LM_bias</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Paul Pu Liang</name>
    </author>
    <author>
      <name>Chiyu Wu</name>
    </author>
    <author>
      <name>Louis-Philippe Morency</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.13213v1</id>
    <title>Learning Language and Multimodal Privacy-Preserving Markers of Mood from Mobile Data</title>
    <updated>2021-06-24T17:46:03Z</updated>
    <link href="https://arxiv.org/abs/2106.13213v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2106.13213v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Mental health conditions remain underdiagnosed even in countries with common access to advanced medical care. The ability to accurately and efficiently predict mood from easily collectible data has several important implications for the early detection, intervention, and treatment of mental health disorders. One promising data source to help monitor human behavior is daily smartphone usage. However, care must be taken to summarize behaviors without identifying the user through personal (e.g., personally identifiable information) or protected (e.g., race, gender) attributes. In this paper, we study behavioral markers of daily mood using a recent dataset of mobile behaviors from adolescent populations at high risk of suicidal behaviors. Using computational models, we find that language and multimodal representations of mobile typed text (spanning typed characters, words, keystroke timings, and app usage) are predictive of daily mood. However, we find that models trained to predict mood often also capture private user identities in their intermediate representations. To tackle this problem, we evaluate approaches that obfuscate user identity while remaining predictive. By combining multimodal representations with privacy-preserving learning, we are able to push forward the performance-privacy frontier.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <published>2021-06-24T17:46:03Z</published>
    <arxiv:comment>ACL 2021. arXiv admin note: substantial text overlap with arXiv:2012.02359</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Paul Pu Liang</name>
    </author>
    <author>
      <name>Terrance Liu</name>
    </author>
    <author>
      <name>Anna Cai</name>
    </author>
    <author>
      <name>Michal Muszynski</name>
    </author>
    <author>
      <name>Ryo Ishii</name>
    </author>
    <author>
      <name>Nicholas Allen</name>
    </author>
    <author>
      <name>Randy Auerbach</name>
    </author>
    <author>
      <name>David Brent</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
    <author>
      <name>Louis-Philippe Morency</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.07447v1</id>
    <title>HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units</title>
    <updated>2021-06-14T14:14:28Z</updated>
    <link href="https://arxiv.org/abs/2106.07447v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2106.07447v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <published>2021-06-14T14:14:28Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Wei-Ning Hsu</name>
    </author>
    <author>
      <name>Benjamin Bolte</name>
    </author>
    <author>
      <name>Yao-Hung Hubert Tsai</name>
    </author>
    <author>
      <name>Kushal Lakhotia</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
    <author>
      <name>Abdelrahman Mohamed</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.07203v2</id>
    <title>Online Sub-Sampling for Reinforcement Learning with General Function Approximation</title>
    <updated>2023-04-18T12:57:43Z</updated>
    <link href="https://arxiv.org/abs/2106.07203v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2106.07203v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>Most of the existing works for reinforcement learning (RL) with general function approximation (FA) focus on understanding the statistical complexity or regret bounds. However, the computation complexity of such approaches is far from being understood -- indeed, a simple optimization problem over the function class might be as well intractable. In this paper, we tackle this problem by establishing an efficient online sub-sampling framework that measures the information gain of data points collected by an RL algorithm and uses the measurement to guide exploration. For a value-based method with complexity-bounded function class, we show that the policy only needs to be updated for $\propto\operatorname{poly}\log(K)$ times for running the RL algorithm for $K$ episodes while still achieving a small near-optimal regret bound. In contrast to existing approaches that update the policy for at least $Ω(K)$ times, our approach drastically reduces the number of optimization calls in solving for a policy. When applied to settings in \cite{wang2020reinforcement} or \cite{jin2021bellman}, we improve the overall time complexity by at least a factor of $K$. Finally, we show the generality of our online sub-sampling technique by applying it to the reward-free RL setting and multi-agent RL setting.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2021-06-14T07:36:25Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Dingwen Kong</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
    <author>
      <name>Ruosong Wang</name>
    </author>
    <author>
      <name>Lin F. Yang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.02869v1</id>
    <title>Integrating Auxiliary Information in Self-supervised Learning</title>
    <updated>2021-06-05T11:01:15Z</updated>
    <link href="https://arxiv.org/abs/2106.02869v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2106.02869v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>This paper presents to integrate the auxiliary information (e.g., additional attributes for data such as the hashtags for Instagram images) in the self-supervised learning process. We first observe that the auxiliary information may bring us useful information about data structures: for instance, the Instagram images with the same hashtags can be semantically similar. Hence, to leverage the structural information from the auxiliary information, we present to construct data clusters according to the auxiliary information. Then, we introduce the Clustering InfoNCE (Cl-InfoNCE) objective that learns similar representations for augmented variants of data from the same cluster and dissimilar representations for data from different clusters. Our approach contributes as follows: 1) Comparing to conventional self-supervised representations, the auxiliary-information-infused self-supervised representations bring the performance closer to the supervised representations; 2) The presented Cl-InfoNCE can also work with unsupervised constructed clusters (e.g., k-means clusters) and outperform strong clustering-based self-supervised learning approaches, such as the Prototypical Contrastive Learning (PCL) method; 3) We show that Cl-InfoNCE may be a better approach to leverage the data clustering information, by comparing it to the baseline approach - learning to predict the clustering assignments with cross-entropy loss. For analysis, we connect the goodness of the learned representations with the statistical relationships: i) the mutual information between the labels and the clusters and ii) the conditional entropy of the clusters given the labels.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2021-06-05T11:01:15Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Yao-Hung Hubert Tsai</name>
    </author>
    <author>
      <name>Tianqin Li</name>
    </author>
    <author>
      <name>Weixin Liu</name>
    </author>
    <author>
      <name>Peiyuan Liao</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
    <author>
      <name>Louis-Philippe Morency</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.02866v2</id>
    <title>Conditional Contrastive Learning for Improving Fairness in Self-Supervised Learning</title>
    <updated>2022-06-28T03:42:49Z</updated>
    <link href="https://arxiv.org/abs/2106.02866v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2106.02866v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>Contrastive self-supervised learning (SSL) learns an embedding space that maps similar data pairs closer and dissimilar data pairs farther apart. Despite its success, one issue has been overlooked: the fairness aspect of representations learned using contrastive SSL. Without mitigation, contrastive SSL techniques can incorporate sensitive information such as gender or race and cause potentially unfair predictions on downstream tasks. In this paper, we propose a Conditional Contrastive Learning (CCL) approach to improve the fairness of contrastive SSL methods. Our approach samples positive and negative pairs from distributions conditioning on the sensitive attribute, or empirically speaking, sampling positive and negative pairs from the same gender or the same race. We show that our approach provably maximizes the conditional mutual information between the learned representations of the positive pairs, and reduces the effect of the sensitive attribute by taking it as the conditional variable. On seven fairness and vision datasets, we empirically demonstrate that the proposed approach achieves state-of-the-art downstream performances compared to unsupervised baselines and significantly improves the fairness of contrastive SSL models on multiple fairness metrics.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2021-06-05T10:51:26Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Martin Q. Ma</name>
    </author>
    <author>
      <name>Yao-Hung Hubert Tsai</name>
    </author>
    <author>
      <name>Paul Pu Liang</name>
    </author>
    <author>
      <name>Han Zhao</name>
    </author>
    <author>
      <name>Kun Zhang</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
    <author>
      <name>Louis-Philippe Morency</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.00200v2</id>
    <title>Iterative Hierarchical Attention for Answering Complex Questions over Long Documents</title>
    <updated>2021-10-22T01:15:31Z</updated>
    <link href="https://arxiv.org/abs/2106.00200v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2106.00200v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>We propose a new model, DocHopper, that iteratively attends to different parts of long, hierarchically structured documents to answer complex questions. Similar to multi-hop question-answering (QA) systems, at each step, DocHopper uses a query $q$ to attend to information from a document, combines this ``retrieved'' information with $q$ to produce the next query. However, in contrast to most previous multi-hop QA systems, DocHopper is able to ``retrieve'' either short passages or long sections of the document, thus emulating a multi-step process of ``navigating'' through a long document to answer a question. To enable this novel behavior, DocHopper does not combine document information with $q$ by concatenating text to the text of $q$, but by combining a compact neural representation of $q$ with a compact neural representation of a hierarchical part of the document, which can potentially be quite large. We experiment with DocHopper on four different QA tasks that require reading long and complex documents to answer multi-hop questions, and show that DocHopper achieves state-of-the-art results on three of the datasets. Additionally, DocHopper is efficient at inference time, being 3--10 times faster than the baselines.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2021-06-01T03:13:35Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Haitian Sun</name>
    </author>
    <author>
      <name>William W. Cohen</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.08140v1</id>
    <title>Uncertainty Weighted Actor-Critic for Offline Reinforcement Learning</title>
    <updated>2021-05-17T20:16:46Z</updated>
    <link href="https://arxiv.org/abs/2105.08140v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2105.08140v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Offline Reinforcement Learning promises to learn effective policies from previously-collected, static datasets without the need for exploration. However, existing Q-learning and actor-critic based off-policy RL algorithms fail when bootstrapping from out-of-distribution (OOD) actions or states. We hypothesize that a key missing ingredient from the existing methods is a proper treatment of uncertainty in the offline setting. We propose Uncertainty Weighted Actor-Critic (UWAC), an algorithm that detects OOD state-action pairs and down-weights their contribution in the training objectives accordingly. Implementation-wise, we adopt a practical and effective dropout-based uncertainty estimation method that introduces very little overhead over existing RL algorithms. Empirically, we observe that UWAC substantially improves model stability during training. In addition, UWAC out-performs existing offline RL methods on a variety of competitive tasks, and achieves significant performance gains over the state-of-the-art baseline on datasets with sparse demonstrations collected from human experts.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2021-05-17T20:16:46Z</published>
    <arxiv:comment>To appear in ICML 2021</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Yue Wu</name>
    </author>
    <author>
      <name>Shuangfei Zhai</name>
    </author>
    <author>
      <name>Nitish Srivastava</name>
    </author>
    <author>
      <name>Joshua Susskind</name>
    </author>
    <author>
      <name>Jian Zhang</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
    <author>
      <name>Hanlin Goh</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.13712v1</id>
    <title>A Note on Connecting Barlow Twins with Negative-Sample-Free Contrastive Learning</title>
    <updated>2021-04-28T11:36:09Z</updated>
    <link href="https://arxiv.org/abs/2104.13712v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2104.13712v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>In this report, we relate the algorithmic design of Barlow Twins' method to the Hilbert-Schmidt Independence Criterion (HSIC), thus establishing it as a contrastive learning approach that is free of negative samples. Through this perspective, we argue that Barlow Twins (and thus the class of negative-sample-free contrastive learning methods) suggests a possibility to bridge the two major families of self-supervised learning philosophies: non-contrastive and contrastive approaches. In particular, Barlow twins exemplified how we could combine the best practices of both worlds: avoiding the need of large training batch size and negative sample pairing (like non-contrastive methods) and avoiding symmetry-breaking network designs (like contrastive methods).</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2021-04-28T11:36:09Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Yao-Hung Hubert Tsai</name>
    </author>
    <author>
      <name>Shaojie Bai</name>
    </author>
    <author>
      <name>Louis-Philippe Morency</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.12714v1</id>
    <title>Focused Attention Improves Document-Grounded Generation</title>
    <updated>2021-04-26T16:56:29Z</updated>
    <link href="https://arxiv.org/abs/2104.12714v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2104.12714v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Document grounded generation is the task of using the information provided in a document to improve text generation. This work focuses on two different document grounded generation tasks: Wikipedia Update Generation task and Dialogue response generation. Our work introduces two novel adaptations of large scale pre-trained encoder-decoder models focusing on building context driven representation of the document and enabling specific attention to the information in the document. Additionally, we provide a stronger BART baseline for these tasks. Our proposed techniques outperform existing methods on both automated (at least 48% increase in BLEU-4 points) and human evaluation for closeness to reference and relevance to the document. Furthermore, we perform comprehensive manual inspection of the generated output and categorize errors to provide insights into future directions in modeling these tasks.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2021-04-26T16:56:29Z</published>
    <arxiv:comment>Accepted at North American Chapter of the Association for Computational Linguistics (NAACL) 2021</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Shrimai Prabhumoye</name>
    </author>
    <author>
      <name>Kazuma Hashimoto</name>
    </author>
    <author>
      <name>Yingbo Zhou</name>
    </author>
    <author>
      <name>Alan W Black</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.05196v1</id>
    <title>StylePTB: A Compositional Benchmark for Fine-grained Controllable Text Style Transfer</title>
    <updated>2021-04-12T04:25:09Z</updated>
    <link href="https://arxiv.org/abs/2104.05196v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2104.05196v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Text style transfer aims to controllably generate text with targeted stylistic changes while maintaining core meaning from the source sentence constant. Many of the existing style transfer benchmarks primarily focus on individual high-level semantic changes (e.g. positive to negative), which enable controllability at a high level but do not offer fine-grained control involving sentence structure, emphasis, and content of the sentence. In this paper, we introduce a large-scale benchmark, StylePTB, with (1) paired sentences undergoing 21 fine-grained stylistic changes spanning atomic lexical, syntactic, semantic, and thematic transfers of text, as well as (2) compositions of multiple transfers which allow modeling of fine-grained stylistic changes as building blocks for more complex, high-level transfers. By benchmarking existing methods on StylePTB, we find that they struggle to model fine-grained changes and have an even more difficult time composing multiple styles. As a result, StylePTB brings novel challenges that we hope will encourage future research in controllable text style transfer, compositional models, and learning disentangled representations. Solving these challenges would present important steps towards controllable text generation.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2021-04-12T04:25:09Z</published>
    <arxiv:comment>NAACL 2021, code available at https://github.com/lvyiwei1/StylePTB/</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Yiwei Lyu</name>
    </author>
    <author>
      <name>Paul Pu Liang</name>
    </author>
    <author>
      <name>Hai Pham</name>
    </author>
    <author>
      <name>Eduard Hovy</name>
    </author>
    <author>
      <name>Barnabás Póczos</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
    <author>
      <name>Louis-Philippe Morency</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.01655v1</id>
    <title>Efficient Transformers in Reinforcement Learning using Actor-Learner Distillation</title>
    <updated>2021-04-04T17:56:34Z</updated>
    <link href="https://arxiv.org/abs/2104.01655v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2104.01655v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Many real-world applications such as robotics provide hard constraints on power and compute that limit the viable model complexity of Reinforcement Learning (RL) agents. Similarly, in many distributed RL settings, acting is done on un-accelerated hardware such as CPUs, which likewise restricts model size to prevent intractable experiment run times. These "actor-latency" constrained settings present a major obstruction to the scaling up of model complexity that has recently been extremely successful in supervised learning. To be able to utilize large model capacity while still operating within the limits imposed by the system during acting, we develop an "Actor-Learner Distillation" (ALD) procedure that leverages a continual form of distillation that transfers learning progress from a large capacity learner model to a small capacity actor model. As a case study, we develop this procedure in the context of partially-observable environments, where transformer models have had large improvements over LSTMs recently, at the cost of significantly higher computational complexity. With transformer models as the learner and LSTMs as the actor, we demonstrate in several challenging memory environments that using Actor-Learner Distillation recovers the clear sample-efficiency gains of the transformer learner model while maintaining the fast inference and reduced total training time of the LSTM actor model.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2021-04-04T17:56:34Z</published>
    <arxiv:comment>Published at ICLR 2021</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Emilio Parisotto</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.12656v2</id>
    <title>Replacing Rewards with Examples: Example-Based Policy Search via Recursive Classification</title>
    <updated>2021-12-30T20:26:31Z</updated>
    <link href="https://arxiv.org/abs/2103.12656v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2103.12656v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>Reinforcement learning (RL) algorithms assume that users specify tasks by manually writing down a reward function. However, this process can be laborious and demands considerable technical expertise. Can we devise RL algorithms that instead enable users to specify tasks simply by providing examples of successful outcomes? In this paper, we derive a control algorithm that maximizes the future probability of these successful outcome examples. Prior work has approached similar problems with a two-stage process, first learning a reward function and then optimizing this reward function using another RL algorithm. In contrast, our method directly learns a value function from transitions and successful outcomes, without learning this intermediate reward function. Our method therefore requires fewer hyperparameters to tune and lines of code to debug. We show that our method satisfies a new data-driven Bellman equation, where examples take the place of the typical reward function term. Experiments show that our approach outperforms prior methods that learn explicit reward functions.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <published>2021-03-23T16:19:55Z</published>
    <arxiv:comment>NeurIPS 2021 (oral). Website with code, videos, and blog post: https://ben-eysenbach.github.io/rce</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Benjamin Eysenbach</name>
    </author>
    <author>
      <name>Sergey Levine</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
  </entry>
</feed>
