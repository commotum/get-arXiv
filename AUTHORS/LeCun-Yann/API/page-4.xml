<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api/e2ZrSF/puwRAGVA/0pzPuXpQxls</id>
  <title>arXiv Query: search_query=au:"Yann LeCun"&amp;id_list=&amp;start=150&amp;max_results=50</title>
  <updated>2026-02-06T19:44:39Z</updated>
  <link href="https://arxiv.org/api/query?search_query=au:%22Yann+LeCun%22&amp;start=150&amp;max_results=50&amp;id_list=" type="application/atom+xml"/>
  <opensearch:itemsPerPage>50</opensearch:itemsPerPage>
  <opensearch:totalResults>196</opensearch:totalResults>
  <opensearch:startIndex>150</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/1511.03719v7</id>
    <title>Universum Prescription: Regularization using Unlabeled Data</title>
    <updated>2016-11-18T01:15:30Z</updated>
    <link href="https://arxiv.org/abs/1511.03719v7" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1511.03719v7" rel="related" type="application/pdf" title="pdf"/>
    <summary>This paper shows that simply prescribing "none of the above" labels to unlabeled data has a beneficial regularization effect to supervised learning. We call it universum prescription by the fact that the prescribed labels cannot be one of the supervised labels. In spite of its simplicity, universum prescription obtained competitive results in training deep convolutional networks for CIFAR-10, CIFAR-100, STL-10 and ImageNet datasets. A qualitative justification of these approaches using Rademacher complexity is presented. The effect of a regularization parameter -- probability of sampling from unlabeled data -- is also studied empirically.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2015-11-11T22:46:46Z</published>
    <arxiv:comment>7 pages for article, 3 pages for supplemental material. To appear in AAAI-17</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Xiang Zhang</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.05970v2</id>
    <title>Stereo Matching by Training a Convolutional Neural Network to Compare Image Patches</title>
    <updated>2016-05-18T19:53:41Z</updated>
    <link href="https://arxiv.org/abs/1510.05970v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1510.05970v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>We present a method for extracting depth information from a rectified image pair. Our approach focuses on the first stage of many stereo algorithms: the matching cost computation. We approach the problem by learning a similarity measure on small image patches using a convolutional neural network. Training is carried out in a supervised manner by constructing a binary classification data set with examples of similar and dissimilar pairs of patches. We examine two network architectures for this task: one tuned for speed, the other for accuracy. The output of the convolutional neural network is used to initialize the stereo matching cost. A series of post-processing steps follow: cross-based cost aggregation, semiglobal matching, a left-right consistency check, subpixel enhancement, a median filter, and a bilateral filter. We evaluate our method on the KITTI 2012, KITTI 2015, and Middlebury stereo data sets and show that it outperforms other approaches on all three data sets.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2015-10-20T17:15:05Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <arxiv:journal_ref>JMLR 17(65):1-32, 2016</arxiv:journal_ref>
    <author>
      <name>Jure Žbontar</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.08967v2</id>
    <title>Very Deep Multilingual Convolutional Neural Networks for LVCSR</title>
    <updated>2016-01-23T18:18:58Z</updated>
    <link href="https://arxiv.org/abs/1509.08967v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1509.08967v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>Convolutional neural networks (CNNs) are a standard component of many current state-of-the-art Large Vocabulary Continuous Speech Recognition (LVCSR) systems. However, CNNs in LVCSR have not kept pace with recent advances in other domains where deeper neural networks provide superior performance. In this paper we propose a number of architectural advances in CNNs for LVCSR. First, we introduce a very deep convolutional network architecture with up to 14 weight layers. There are multiple convolutional layers before each pooling layer, with small 3x3 kernels, inspired by the VGG Imagenet 2014 architecture. Then, we introduce multilingual CNNs with multiple untied layers. Finally, we introduce multi-scale input features aimed at exploiting more context at negligible computational cost. We evaluate the improvements first on a Babel task for low resource speech recognition, obtaining an absolute 5.77% WER improvement over the baseline PLP DNN by training our CNN on the combined data of six different languages. We then evaluate the very deep CNNs on the Hub5'00 benchmark (using the 262 hours of SWB-1 training data) achieving a word error rate of 11.8% after cross-entropy training, a 1.4% WER improvement (10.6% relative) over the best published CNN result so far.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2015-09-29T22:28:11Z</published>
    <arxiv:comment>Accepted for publication at ICASSP 2016</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Tom Sercu</name>
    </author>
    <author>
      <name>Christian Puhrsch</name>
    </author>
    <author>
      <name>Brian Kingsbury</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.03591v1</id>
    <title>High Performance Computer Acoustic Data Accelerator: A New System for Exploring Marine Mammal Acoustics for Big Data Applications</title>
    <updated>2015-09-11T17:51:56Z</updated>
    <link href="https://arxiv.org/abs/1509.03591v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1509.03591v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>This paper presents a new software model designed for distributed sonic signal detection runtime using machine learning algorithms called DeLMA. A new algorithm--Acoustic Data-mining Accelerator (ADA)--is also presented. ADA is a robust yet scalable solution for efficiently processing big sound archives using distributing computing technologies. Together, DeLMA and the ADA algorithm provide a powerful tool currently being used by the Bioacoustics Research Program (BRP) at the Cornell Lab of Ornithology, Cornell University. This paper provides a high level technical overview of the system, and discusses various aspects of the design. Basic runtime performance and project summary are presented. The DeLMA-ADA baseline performance comparing desktop serial configuration to a 64 core distributed HPC system shows as much as a 44 times faster increase in runtime execution. Performance tests using 48 cores on the HPC shows a 9x to 12x efficiency over a 4 core desktop solution. Project summary results for 19 east coast deployments show that the DeLMA-ADA solution has processed over three million channel hours of sound to date.</summary>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <published>2015-09-11T17:51:56Z</published>
    <arxiv:comment>Seven pages, submitted at International Conference on Machine Learning 2014, Workshop uLearnBio, unsupervised learning for bioacoustic applications</arxiv:comment>
    <arxiv:primary_category term="cs.DC"/>
    <author>
      <name>Peter Dugan</name>
    </author>
    <author>
      <name>John Zollweg</name>
    </author>
    <author>
      <name>Marian Popescu</name>
    </author>
    <author>
      <name>Denise Risch</name>
    </author>
    <author>
      <name>Herve Glotin</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
    <author>
      <name>and Christopher Clark</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.01626v3</id>
    <title>Character-level Convolutional Networks for Text Classification</title>
    <updated>2016-04-04T02:34:30Z</updated>
    <link href="https://arxiv.org/abs/1509.01626v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1509.01626v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2015-09-04T22:31:53Z</published>
    <arxiv:comment>An early version of this work entitled "Text Understanding from Scratch" was posted in Feb 2015 as arXiv:1502.01710. The present paper has considerably more experimental results and a rewritten introduction, Advances in Neural Information Processing Systems 28 (NIPS 2015)</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Xiang Zhang</name>
    </author>
    <author>
      <name>Junbo Zhao</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.05163v1</id>
    <title>Deep Convolutional Networks on Graph-Structured Data</title>
    <updated>2015-06-16T22:31:09Z</updated>
    <link href="https://arxiv.org/abs/1506.05163v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1506.05163v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Deep Learning's recent successes have mostly relied on Convolutional Networks, which exploit fundamental statistical properties of images, sounds and video data: the local stationarity and multi-scale compositional structure, that allows expressing long range interactions in terms of shorter, localized interactions. However, there exist other important examples, such as text documents or bioinformatic data, that may lack some or all of these strong statistical regularities.
  In this paper we consider the general question of how to construct deep architectures with small learning complexity on general non-Euclidean domains, which are typically unknown and need to be estimated from the data. In particular, we develop an extension of Spectral Networks which incorporates a Graph Estimation procedure, that we test on large-scale classification problems, matching or improving over Dropout Networks with far less parameters to estimate.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2015-06-16T22:31:09Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Mikael Henaff</name>
    </author>
    <author>
      <name>Joan Bruna</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.03011v2</id>
    <title>Learning to Linearize Under Uncertainty</title>
    <updated>2015-09-10T15:20:38Z</updated>
    <link href="https://arxiv.org/abs/1506.03011v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1506.03011v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>Training deep feature hierarchies to solve supervised learning tasks has achieved state of the art performance on many problems in computer vision. However, a principled way in which to train such hierarchies in the unsupervised setting has remained elusive. In this work we suggest a new architecture and loss for training deep feature hierarchies that linearize the transformations observed in unlabeled natural video sequences. This is done by training a generative model to predict video frames. We also address the problem of inherent uncertainty in prediction by introducing latent variables that are non-deterministic functions of the input into the network architecture.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2015-06-09T17:22:17Z</published>
    <arxiv:comment>To appear at NIPS 2015</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Ross Goroshin</name>
    </author>
    <author>
      <name>Michael Mathieu</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.02351v8</id>
    <title>Stacked What-Where Auto-encoders</title>
    <updated>2016-02-14T21:09:22Z</updated>
    <link href="https://arxiv.org/abs/1506.02351v8" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1506.02351v8" rel="related" type="application/pdf" title="pdf"/>
    <summary>We present a novel architecture, the "stacked what-where auto-encoders" (SWWAE), which integrates discriminative and generative pathways and provides a unified approach to supervised, semi-supervised and unsupervised learning without relying on sampling during training. An instantiation of SWWAE uses a convolutional net (Convnet) (LeCun et al. (1998)) to encode the input, and employs a deconvolutional net (Deconvnet) (Zeiler et al. (2010)) to produce the reconstruction. The objective function includes reconstruction terms that induce the hidden states in the Deconvnet to be similar to those of the Convnet. Each pooling layer produces two sets of variables: the "what" which are fed to the next layer, and its complementary variable "where" that are fed to the corresponding layer in the generative decoder.</summary>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2015-06-08T04:45:33Z</published>
    <arxiv:comment>Workshop track - ICLR 2016</arxiv:comment>
    <arxiv:primary_category term="stat.ML"/>
    <author>
      <name>Junbo Zhao</name>
    </author>
    <author>
      <name>Michael Mathieu</name>
    </author>
    <author>
      <name>Ross Goroshin</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.02518v2</id>
    <title>Unsupervised Feature Learning from Temporal Data</title>
    <updated>2015-04-15T23:08:30Z</updated>
    <link href="https://arxiv.org/abs/1504.02518v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1504.02518v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>Current state-of-the-art classification and detection algorithms rely on supervised training. In this work we study unsupervised feature learning in the context of temporally coherent video data. We focus on feature learning from unlabeled video data, using the assumption that adjacent video frames contain semantically similar information. This assumption is exploited to train a convolutional pooling auto-encoder regularized by slowness and sparsity. We establish a connection between slow feature learning to metric learning and show that the trained encoder can be used to define a more temporally and semantically coherent metric.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2015-04-09T23:26:26Z</published>
    <arxiv:comment>arXiv admin note: substantial text overlap with arXiv:1412.6056</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Ross Goroshin</name>
    </author>
    <author>
      <name>Joan Bruna</name>
    </author>
    <author>
      <name>Jonathan Tompson</name>
    </author>
    <author>
      <name>David Eigen</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.03438v3</id>
    <title>A mathematical motivation for complex-valued convolutional networks</title>
    <updated>2015-12-12T19:04:02Z</updated>
    <link href="https://arxiv.org/abs/1503.03438v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1503.03438v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>A complex-valued convolutional network (convnet) implements the repeated application of the following composition of three operations, recursively applying the composition to an input vector of nonnegative real numbers: (1) convolution with complex-valued vectors followed by (2) taking the absolute value of every entry of the resulting vectors followed by (3) local averaging. For processing real-valued random vectors, complex-valued convnets can be viewed as "data-driven multiscale windowed power spectra," "data-driven multiscale windowed absolute spectra," "data-driven multiwavelet absolute values," or (in their most general configuration) "data-driven nonlinear multiwavelet packets." Indeed, complex-valued convnets can calculate multiscale windowed spectra when the convnet filters are windowed complex-valued exponentials. Standard real-valued convnets, using rectified linear units (ReLUs), sigmoidal (for example, logistic or tanh) nonlinearities, max. pooling, etc., do not obviously exhibit the same exact correspondence with data-driven wavelets (whereas for complex-valued convnets, the correspondence is much more than just a vague analogy). Courtesy of the exact correspondence, the remarkably rich and rigorous body of mathematical analysis for wavelets applies directly to (complex-valued) convnets.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2015-03-11T18:24:13Z</published>
    <arxiv:comment>11 pages, 3 figures; this is the retitled version submitted to the journal, "Neural Computation"</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <arxiv:journal_ref>Neural Computation, 28 (5): 815-825, May 2016</arxiv:journal_ref>
    <author>
      <name>Joan Bruna</name>
    </author>
    <author>
      <name>Soumith Chintala</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
    <author>
      <name>Serkan Piantino</name>
    </author>
    <author>
      <name>Arthur Szlam</name>
    </author>
    <author>
      <name>Mark Tygert</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.01710v5</id>
    <title>Text Understanding from Scratch</title>
    <updated>2016-04-04T02:40:48Z</updated>
    <link href="https://arxiv.org/abs/1502.01710v5" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1502.01710v5" rel="related" type="application/pdf" title="pdf"/>
    <summary>This article demontrates that we can apply deep learning to text understanding from character-level inputs all the way up to abstract text concepts, using temporal convolutional networks (ConvNets). We apply ConvNets to various large-scale datasets, including ontology classification, sentiment analysis, and text categorization. We show that temporal ConvNets can achieve astonishing performance without the knowledge of words, phrases, sentences and any other syntactic or semantic structures with regards to a human language. Evidence shows that our models can work for both English and Chinese.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2015-02-05T20:45:19Z</published>
    <arxiv:comment>This technical report is superseded by a paper entitled "Character-level Convolutional Networks for Text Classification", arXiv:1509.01626. It has considerably more experimental results and a rewritten introduction</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Xiang Zhang</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.7580v3</id>
    <title>Fast Convolutional Nets With fbfft: A GPU Performance Evaluation</title>
    <updated>2015-04-10T20:01:00Z</updated>
    <link href="https://arxiv.org/abs/1412.7580v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1412.7580v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>We examine the performance profile of Convolutional Neural Network training on the current generation of NVIDIA Graphics Processing Units. We introduce two new Fast Fourier Transform convolution implementations: one based on NVIDIA's cuFFT library, and another based on a Facebook authored FFT implementation, fbfft, that provides significant speedups over cuFFT (over 1.5x) for whole CNNs. Both of these convolution implementations are available in open source, and are faster than NVIDIA's cuDNN implementation for many common convolutional layers (up to 23.5x for some synthetic kernel configurations). We discuss different performance regimes of convolutions, comparing areas where straightforward time domain convolutions outperform Fourier frequency domain convolutions. Details on algorithmic applications of NVIDIA GPU hardware specifics in the implementation of fbfft are also provided.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2014-12-24T01:31:36Z</published>
    <arxiv:comment>Camera ready for ICLR2015</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Nicolas Vasilache</name>
    </author>
    <author>
      <name>Jeff Johnson</name>
    </author>
    <author>
      <name>Michael Mathieu</name>
    </author>
    <author>
      <name>Soumith Chintala</name>
    </author>
    <author>
      <name>Serkan Piantino</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.7022v3</id>
    <title>Audio Source Separation with Discriminative Scattering Networks</title>
    <updated>2015-04-28T02:24:14Z</updated>
    <link href="https://arxiv.org/abs/1412.7022v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1412.7022v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>In this report we describe an ongoing line of research for solving single-channel source separation problems. Many monaural signal decomposition techniques proposed in the literature operate on a feature space consisting of a time-frequency representation of the input data. A challenge faced by these approaches is to effectively exploit the temporal dependencies of the signals at scales larger than the duration of a time-frame. In this work we propose to tackle this problem by modeling the signals using a time-frequency representation with multiple temporal resolutions. The proposed representation consists of a pyramid of wavelet scattering operators, which generalizes Constant Q Transforms (CQT) with extra layers of convolution and complex modulus. We first show that learning standard models with this multi-resolution setting improves source separation results over fixed-resolution methods. As study case, we use Non-Negative Matrix Factorizations (NMF) that has been widely considered in many audio application. Then, we investigate the inclusion of the proposed multi-resolution setting into a discriminative training regime. We discuss several alternatives using different deep neural network architectures.</summary>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2014-12-22T15:15:44Z</published>
    <arxiv:primary_category term="cs.SD"/>
    <author>
      <name>Pablo Sprechmann</name>
    </author>
    <author>
      <name>Joan Bruna</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.6651v8</id>
    <title>Deep learning with Elastic Averaging SGD</title>
    <updated>2015-10-25T12:12:52Z</updated>
    <link href="https://arxiv.org/abs/1412.6651v8" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1412.6651v8" rel="related" type="application/pdf" title="pdf"/>
    <summary>We study the problem of stochastic optimization for deep learning in the parallel computing environment under communication constraints. A new algorithm is proposed in this setting where the communication and coordination of work among concurrent processes (local workers), is based on an elastic force which links the parameters they compute with a center variable stored by the parameter server (master). The algorithm enables the local workers to perform more exploration, i.e. the algorithm allows the local variables to fluctuate further from the center variable by reducing the amount of communication between local workers and the master. We empirically demonstrate that in the deep learning setting, due to the existence of many local optima, allowing more exploration can lead to the improved performance. We propose synchronous and asynchronous variants of the new algorithm. We provide the stability analysis of the asynchronous variant in the round-robin scheme and compare it with the more common parallelized method ADMM. We show that the stability of EASGD is guaranteed when a simple stability condition is satisfied, which is not the case for ADMM. We additionally propose the momentum-based version of our algorithm that can be applied in both synchronous and asynchronous settings. Asynchronous variant of the algorithm is applied to train convolutional neural networks for image classification on the CIFAR and ImageNet datasets. Experiments demonstrate that the new algorithm accelerates the training of deep architectures compared to DOWNPOUR and other common baseline approaches and furthermore is very communication efficient.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2014-12-20T13:22:23Z</published>
    <arxiv:comment>NIPS2015 camera-ready version</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Sixin Zhang</name>
    </author>
    <author>
      <name>Anna Choromanska</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.6615v4</id>
    <title>Explorations on high dimensional landscapes</title>
    <updated>2015-04-06T21:47:50Z</updated>
    <link href="https://arxiv.org/abs/1412.6615v4" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1412.6615v4" rel="related" type="application/pdf" title="pdf"/>
    <summary>Finding minima of a real valued non-convex function over a high dimensional space is a major challenge in science. We provide evidence that some such functions that are defined on high dimensional domains have a narrow band of values whose pre-image contains the bulk of its critical points. This is in contrast with the low dimensional picture in which this band is wide. Our simulations agree with the previous theoretical work on spin glasses that proves the existence of such a band when the dimension of the domain tends to infinity. Furthermore our experiments on teacher-student networks with the MNIST dataset establish a similar phenomenon in deep networks. We finally observe that both the gradient descent and the stochastic gradient descent methods can reach this level within the same number of steps.</summary>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2014-12-20T06:57:12Z</published>
    <arxiv:comment>11 pages, 8 figures, workshop contribution at ICLR 2015</arxiv:comment>
    <arxiv:primary_category term="stat.ML"/>
    <author>
      <name>Levent Sagun</name>
    </author>
    <author>
      <name>V. Ugur Guney</name>
    </author>
    <author>
      <name>Gerard Ben Arous</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.6056v6</id>
    <title>Unsupervised Learning of Spatiotemporally Coherent Metrics</title>
    <updated>2015-09-08T18:39:03Z</updated>
    <link href="https://arxiv.org/abs/1412.6056v6" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1412.6056v6" rel="related" type="application/pdf" title="pdf"/>
    <summary>Current state-of-the-art classification and detection algorithms rely on supervised training. In this work we study unsupervised feature learning in the context of temporally coherent video data. We focus on feature learning from unlabeled video data, using the assumption that adjacent video frames contain semantically similar information. This assumption is exploited to train a convolutional pooling auto-encoder regularized by slowness and sparsity. We establish a connection between slow feature learning to metric learning and show that the trained encoder can be used to define a more temporally and semantically coherent metric.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2014-12-18T20:31:56Z</published>
    <arxiv:comment>To appear at ICCV2015</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Ross Goroshin</name>
    </author>
    <author>
      <name>Joan Bruna</name>
    </author>
    <author>
      <name>Jonathan Tompson</name>
    </author>
    <author>
      <name>David Eigen</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.0233v3</id>
    <title>The Loss Surfaces of Multilayer Networks</title>
    <updated>2015-01-21T22:25:26Z</updated>
    <link href="https://arxiv.org/abs/1412.0233v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1412.0233v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>We study the connection between the highly non-convex loss function of a simple model of the fully-connected feed-forward neural network and the Hamiltonian of the spherical spin-glass model under the assumptions of: i) variable independence, ii) redundancy in network parametrization, and iii) uniformity. These assumptions enable us to explain the complexity of the fully decoupled neural network through the prism of the results from random matrix theory. We show that for large-size decoupled networks the lowest critical values of the random loss function form a layered structure and they are located in a well-defined band lower-bounded by the global minimum. The number of local minima outside that band diminishes exponentially with the size of the network. We empirically verify that the mathematical model exhibits similar behavior as the computer simulations, despite the presence of high dependencies in real networks. We conjecture that both simulated annealing and SGD converge to the band of low critical points, and that all critical points found there are local minima of high quality measured by the test error. This emphasizes a major difference between large- and small-size networks where for the latter poor quality local minima have non-zero probability of being recovered. Finally, we prove that recovering the global minimum becomes harder as the network size increases and that it is in practice irrelevant as global minimum often leads to overfitting.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2014-11-30T15:48:16Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Anna Choromanska</name>
    </author>
    <author>
      <name>Mikael Henaff</name>
    </author>
    <author>
      <name>Michael Mathieu</name>
    </author>
    <author>
      <name>Gérard Ben Arous</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.4280v3</id>
    <title>Efficient Object Localization Using Convolutional Networks</title>
    <updated>2015-06-09T12:29:21Z</updated>
    <link href="https://arxiv.org/abs/1411.4280v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1411.4280v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>Recent state-of-the-art performance on human-body pose estimation has been achieved with Deep Convolutional Networks (ConvNets). Traditional ConvNet architectures include pooling and sub-sampling layers which reduce computational requirements, introduce invariance and prevent over-training. These benefits of pooling come at the cost of reduced localization accuracy. We introduce a novel architecture which includes an efficient `position refinement' model that is trained to estimate the joint offset location within a small region of the image. This refinement model is jointly trained in cascade with a state-of-the-art ConvNet model to achieve improved accuracy in human joint location estimation. We show that the variance of our detector approaches the variance of human annotations on the FLIC dataset and outperforms all existing approaches on the MPII-human-pose dataset.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2014-11-16T17:23:02Z</published>
    <arxiv:comment>8 pages with 1 page of citations</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Jonathan Tompson</name>
    </author>
    <author>
      <name>Ross Goroshin</name>
    </author>
    <author>
      <name>Arjun Jain</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
    <author>
      <name>Christopher Bregler</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.6973v2</id>
    <title>Differentially- and non-differentially-private random decision trees</title>
    <updated>2015-02-05T20:48:11Z</updated>
    <link href="https://arxiv.org/abs/1410.6973v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1410.6973v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>We consider supervised learning with random decision trees, where the tree construction is completely random. The method is popularly used and works well in practice despite the simplicity of the setting, but its statistical mechanism is not yet well-understood. In this paper we provide strong theoretical guarantees regarding learning with random decision trees. We analyze and compare three different variants of the algorithm that have minimal memory requirements: majority voting, threshold averaging and probabilistic averaging. The random structure of the tree enables us to adapt these methods to a differentially-private setting thus we also propose differentially-private versions of all three schemes. We give upper-bounds on the generalization error and mathematically explain how the accuracy depends on the number of random decision trees. Furthermore, we prove that only logarithmic (in the size of the dataset) number of independently selected random decision trees suffice to correctly classify most of the data, even when differential-privacy guarantees must be maintained. We empirically show that majority voting and threshold averaging give the best accuracy, also for conservative users requiring high privacy guarantees. Furthermore, we demonstrate that a simple majority voting rule is an especially good candidate for the differentially-private classifier since it is much less sensitive to the choice of forest parameters than other methods.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2014-10-26T00:16:16Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Mariusz Bojarski</name>
    </author>
    <author>
      <name>Anna Choromanska</name>
    </author>
    <author>
      <name>Krzysztof Choromanski</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.7963v1</id>
    <title>MoDeep: A Deep Learning Framework Using Motion Features for Human Pose Estimation</title>
    <updated>2014-09-28T21:32:15Z</updated>
    <link href="https://arxiv.org/abs/1409.7963v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1409.7963v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>In this work, we propose a novel and efficient method for articulated human pose estimation in videos using a convolutional network architecture, which incorporates both color and motion features. We propose a new human body pose dataset, FLIC-motion, that extends the FLIC dataset with additional motion features. We apply our architecture to this dataset and report significantly better performance than current state-of-the-art pose detection systems.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2014-09-28T21:32:15Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Arjun Jain</name>
    </author>
    <author>
      <name>Jonathan Tompson</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
    <author>
      <name>Christoph Bregler</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.4326v2</id>
    <title>Computing the Stereo Matching Cost with a Convolutional Neural Network</title>
    <updated>2015-10-20T15:08:48Z</updated>
    <link href="https://arxiv.org/abs/1409.4326v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1409.4326v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>We present a method for extracting depth information from a rectified image pair. We train a convolutional neural network to predict how well two image patches match and use it to compute the stereo matching cost. The cost is refined by cross-based cost aggregation and semiglobal matching, followed by a left-right consistency check to eliminate errors in the occluded regions. Our stereo method achieves an error rate of 2.61 % on the KITTI stereo dataset and is currently (August 2014) the top performing method on this dataset.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2014-09-15T16:54:42Z</published>
    <arxiv:comment>Conference on Computer Vision and Pattern Recognition (CVPR), June 2015</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Jure Žbontar</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
    <arxiv:doi>10.1109/CVPR.2015.7298767</arxiv:doi>
    <link rel="related" href="https://doi.org/10.1109/CVPR.2015.7298767" title="doi"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.2984v2</id>
    <title>Joint Training of a Convolutional Network and a Graphical Model for Human Pose Estimation</title>
    <updated>2014-09-17T22:43:45Z</updated>
    <link href="https://arxiv.org/abs/1406.2984v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1406.2984v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>This paper proposes a new hybrid architecture that consists of a deep Convolutional Network and a Markov Random Field. We show how this architecture is successfully applied to the challenging problem of articulated human pose estimation in monocular images. The architecture can exploit structural domain constraints such as geometric relationships between body joint locations. We show that joint training of these two model paradigms improves performance and allows us to significantly outperform existing state-of-the-art techniques.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2014-06-11T18:16:29Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Jonathan Tompson</name>
    </author>
    <author>
      <name>Arjun Jain</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
    <author>
      <name>Christoph Bregler</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.7195v1</id>
    <title>Fast Approximation of Rotations and Hessians matrices</title>
    <updated>2014-04-29T00:08:15Z</updated>
    <link href="https://arxiv.org/abs/1404.7195v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1404.7195v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>A new method to represent and approximate rotation matrices is introduced. The method represents approximations of a rotation matrix $Q$ with linearithmic complexity, i.e. with $\frac{1}{2}n\lg(n)$ rotations over pairs of coordinates, arranged in an FFT-like fashion. The approximation is "learned" using gradient descent. It allows to represent symmetric matrices $H$ as $QDQ^T$ where $D$ is a diagonal matrix. It can be used to approximate covariance matrix of Gaussian models in order to speed up inference, or to estimate and track the inverse Hessian of an objective function by relating changes in parameters to changes in gradient along the trajectory followed by the optimization procedure. Experiments were conducted to approximate synthetic matrices, covariance matrices of real data, and Hessian matrices of objective functions involved in machine learning problems.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2014-04-29T00:08:15Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Michael Mathieu</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.0736v2</id>
    <title>Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation</title>
    <updated>2014-06-09T15:53:55Z</updated>
    <link href="https://arxiv.org/abs/1404.0736v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1404.0736v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>  We present techniques for speeding up the test-time evaluation of large convolutional networks, designed for object recognition tasks. These models deliver impressive accuracy but each image evaluation requires millions of floating point operations, making their deployment on smartphones and Internet-scale clusters problematic. The computation is dominated by the convolution operations in the lower layers of the model. We exploit the linear structure present within the convolutional filters to derive approximations that significantly reduce the required computation. Using large state-of-the-art models, we demonstrate we demonstrate speedups of convolutional layers on both CPU and GPU by a factor of 2x, while keeping the accuracy within 1% of the original model.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2014-04-02T23:31:12Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Remi Denton</name>
    </author>
    <author>
      <name>Wojciech Zaremba</name>
    </author>
    <author>
      <name>Joan Bruna</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
    <author>
      <name>Rob Fergus</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.6229v4</id>
    <title>OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks</title>
    <updated>2014-02-24T03:38:17Z</updated>
    <link href="https://arxiv.org/abs/1312.6229v4" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1312.6229v4" rel="related" type="application/pdf" title="pdf"/>
    <summary>We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2013-12-21T09:52:33Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Pierre Sermanet</name>
    </author>
    <author>
      <name>David Eigen</name>
    </author>
    <author>
      <name>Xiang Zhang</name>
    </author>
    <author>
      <name>Michael Mathieu</name>
    </author>
    <author>
      <name>Rob Fergus</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.6203v3</id>
    <title>Spectral Networks and Locally Connected Networks on Graphs</title>
    <updated>2014-05-21T16:27:09Z</updated>
    <link href="https://arxiv.org/abs/1312.6203v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1312.6203v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2013-12-21T04:25:53Z</published>
    <arxiv:comment>14 pages</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Joan Bruna</name>
    </author>
    <author>
      <name>Wojciech Zaremba</name>
    </author>
    <author>
      <name>Arthur Szlam</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.5851v5</id>
    <title>Fast Training of Convolutional Networks through FFTs</title>
    <updated>2014-03-06T23:27:18Z</updated>
    <link href="https://arxiv.org/abs/1312.5851v5" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1312.5851v5" rel="related" type="application/pdf" title="pdf"/>
    <summary>Convolutional networks are one of the most widely employed architectures in computer vision and machine learning. In order to leverage their ability to learn complex functions, large amounts of data are required for training. Training a large convolutional network to produce state-of-the-art results can take weeks, even when using modern GPUs. Producing labels using a trained network can also be costly when dealing with web-scale datasets. In this work, we present a simple algorithm which accelerates training and inference by a significant factor, and can yield improvements of over an order of magnitude compared to existing state-of-the-art implementations. This is done by computing convolutions as pointwise products in the Fourier domain while reusing the same transformed feature map many times. The algorithm is implemented on a GPU architecture and addresses a number of related challenges.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2013-12-20T08:42:21Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Michael Mathieu</name>
    </author>
    <author>
      <name>Mikael Henaff</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.1847v2</id>
    <title>Understanding Deep Architectures using a Recursive Convolutional Network</title>
    <updated>2014-02-19T17:55:37Z</updated>
    <link href="https://arxiv.org/abs/1312.1847v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1312.1847v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>A key challenge in designing convolutional network models is sizing them appropriately. Many factors are involved in these decisions, including number of layers, feature maps, kernel sizes, etc. Complicating this further is the fact that each of these influence not only the numbers and dimensions of the activation units, but also the total number of parameters. In this paper we focus on assessing the independent contributions of three of these linked variables: The numbers of layers, feature maps, and parameters. To accomplish this, we employ a recursive convolutional network whose weights are tied between layers; this allows us to vary each of the three factors in a controlled setting. We find that while increasing the numbers of layers and parameters each have clear benefit, the number of feature maps (and hence dimensionality of the representation) appears ancillary, and finds most of its benefit through the introduction of more weights. Our results (i) empirically confirm the notion that adding layers alone increases computational power, within the context of convolutional layers, and (ii) suggest that precise sizing of convolutional feature map dimensions is itself of little concern; more attention should be paid to the number of parameters in these layers instead.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2013-12-06T12:55:05Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>David Eigen</name>
    </author>
    <author>
      <name>Jason Rolfe</name>
    </author>
    <author>
      <name>Rob Fergus</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.4025v3</id>
    <title>Signal Recovery from Pooling Representations</title>
    <updated>2014-02-27T22:36:08Z</updated>
    <link href="https://arxiv.org/abs/1311.4025v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1311.4025v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>In this work we compute lower Lipschitz bounds of $\ell_p$ pooling operators for $p=1, 2, \infty$ as well as $\ell_p$ pooling operators preceded by half-rectification layers. These give sufficient conditions for the design of invertible neural network layers. Numerical experiments on MNIST and image patches confirm that pooling layers can be inverted with phase recovery algorithms. Moreover, the regularity of the inverse pooling, controlled by the lower Lipschitz constant, is empirically verified with a nearest neighbor regression.</summary>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2013-11-16T06:53:44Z</published>
    <arxiv:comment>17 pages, 3 figures</arxiv:comment>
    <arxiv:primary_category term="stat.ML"/>
    <author>
      <name>Joan Bruna</name>
    </author>
    <author>
      <name>Arthur Szlam</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.3775v4</id>
    <title>Discriminative Recurrent Sparse Auto-Encoders</title>
    <updated>2013-03-19T18:43:29Z</updated>
    <link href="https://arxiv.org/abs/1301.3775v4" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1301.3775v4" rel="related" type="application/pdf" title="pdf"/>
    <summary>We present the discriminative recurrent sparse auto-encoder model, comprising a recurrent encoder of rectified linear units, unrolled for a fixed number of iterations, and connected to two linear decoders that reconstruct the input and predict its supervised classification. Training via backpropagation-through-time initially minimizes an unsupervised sparse reconstruction error; the loss function is then augmented with a discriminative term on the supervised classification. The depth implicit in the temporally-unrolled form allows the system to exhibit all the power of deep networks, while substantially reducing the number of trainable parameters.
  From an initially unstructured network the hidden units differentiate into categorical-units, each of which represents an input prototype with a well-defined class; and part-units representing deformations of these prototypes. The learned organization of the recurrent encoder is hierarchical: part-units are driven directly by the input, whereas the activity of categorical-units builds up over time through interactions with the part-units. Even using a small number of hidden units per layer, discriminative recurrent sparse auto-encoders achieve excellent performance on MNIST.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2013-01-16T18:07:01Z</published>
    <arxiv:comment>Added clarifications suggested by reviewers. 15 pages, 10 figures</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Jason Tyler Rolfe</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.3764v2</id>
    <title>Adaptive learning rates and parallelization for stochastic, sparse, non-smooth gradients</title>
    <updated>2013-03-27T18:30:41Z</updated>
    <link href="https://arxiv.org/abs/1301.3764v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1301.3764v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>Recent work has established an empirically successful framework for adapting learning rates for stochastic gradient descent (SGD). This effectively removes all needs for tuning, while automatically reducing learning rates over time on stationary problems, and permitting learning rates to grow appropriately in non-stationary tasks. Here, we extend the idea in three directions, addressing proper minibatch parallelization, including reweighted updates for sparse or orthogonal gradients, improving robustness on non-smooth loss functions, in the process replacing the diagonal Hessian estimation procedure that may not always be available by a robust finite-difference approximation. The final algorithm integrates all these components, has linear complexity and is hyper-parameter free.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2013-01-16T17:48:38Z</published>
    <arxiv:comment>Published at the First International Conference on Learning Representations (ICLR-2013). Public reviews are available at http://openreview.net/document/c14f2204-fd66-4d91-bed4-153523694041#c14f2204-fd66-4d91-bed4-153523694041</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Tom Schaul</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.3577v3</id>
    <title>Saturating Auto-Encoders</title>
    <updated>2013-03-20T15:37:33Z</updated>
    <link href="https://arxiv.org/abs/1301.3577v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1301.3577v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>We introduce a simple new regularizer for auto-encoders whose hidden-unit activation functions contain at least one zero-gradient (saturated) region. This regularizer explicitly encourages activations in the saturated region(s) of the corresponding activation function. We call these Saturating Auto-Encoders (SATAE). We show that the saturation regularizer explicitly limits the SATAE's ability to reconstruct inputs which are not near the data manifold. Furthermore, we show that a wide variety of features can be learned when different activation functions are used. Finally, connections are established with the Contractive and Sparse Auto-Encoders.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2013-01-16T04:07:46Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Rostislav Goroshin</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.3572v2</id>
    <title>Indoor Semantic Segmentation using depth information</title>
    <updated>2013-03-14T18:18:17Z</updated>
    <link href="https://arxiv.org/abs/1301.3572v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1301.3572v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>This work addresses multi-class segmentation of indoor scenes with RGB-D inputs. While this area of research has gained much attention recently, most works still rely on hand-crafted features. In contrast, we apply a multiscale convolutional network to learn features directly from the images and the depth information. We obtain state-of-the-art on the NYU-v2 depth dataset with an accuracy of 64.5%. We illustrate the labeling of indoor scenes in videos sequences that could be processed in real-time using appropriate hardware such as an FPGA.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2013-01-16T03:31:30Z</published>
    <arxiv:comment>8 pages, 3 figures</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Camille Couprie</name>
    </author>
    <author>
      <name>Clément Farabet</name>
    </author>
    <author>
      <name>Laurent Najman</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.3537v1</id>
    <title>Learning Stable Group Invariant Representations with Convolutional Networks</title>
    <updated>2013-01-16T00:49:38Z</updated>
    <link href="https://arxiv.org/abs/1301.3537v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1301.3537v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Transformation groups, such as translations or rotations, effectively express part of the variability observed in many recognition problems. The group structure enables the construction of invariant signal representations with appealing mathematical properties, where convolutions, together with pooling operators, bring stability to additive and geometric perturbations of the input. Whereas physical transformation groups are ubiquitous in image and audio applications, they do not account for all the variability of complex signal classes.
  We show that the invariance properties built by deep convolutional networks can be cast as a form of stable group invariance. The network wiring architecture determines the invariance group, while the trainable filter coefficients characterize the group action. We give explanatory examples which illustrate how the network architecture controls the resulting invariance group. We also explore the principle by which additional convolutional layers induce a group factorization enabling more abstract, powerful invariant representations.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <published>2013-01-16T00:49:38Z</published>
    <arxiv:comment>4 pages</arxiv:comment>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Joan Bruna</name>
    </author>
    <author>
      <name>Arthur Szlam</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.3476v3</id>
    <title>Pushing Stochastic Gradient towards Second-Order Methods -- Backpropagation Learning with Transformations in Nonlinearities</title>
    <updated>2013-03-11T18:00:00Z</updated>
    <link href="https://arxiv.org/abs/1301.3476v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1301.3476v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>Recently, we proposed to transform the outputs of each hidden neuron in a multi-layer perceptron network to have zero output and zero slope on average, and use separate shortcut connections to model the linear dependencies instead. We continue the work by firstly introducing a third transformation to normalize the scale of the outputs of each hidden neuron, and secondly by analyzing the connections to second order optimization methods. We show that the transformations make a simple stochastic gradient behave closer to second-order optimization methods and thus speed up learning. This is shown both in theory and with experiments. The experiments on the third transformation show that while it further increases the speed of learning, it can also hurt performance by converging to a worse local optimum, where both the inputs and outputs of many hidden neurons are close to zero.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2013-01-15T20:21:54Z</published>
    <arxiv:comment>10 pages, 5 figures, ICLR2013</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Tommi Vatanen</name>
    </author>
    <author>
      <name>Tapani Raiko</name>
    </author>
    <author>
      <name>Harri Valpola</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.1671v1</id>
    <title>Causal graph-based video segmentation</title>
    <updated>2013-01-08T20:56:17Z</updated>
    <link href="https://arxiv.org/abs/1301.1671v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1301.1671v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Numerous approaches in image processing and computer vision are making use of super-pixels as a pre-processing step. Among the different methods producing such over-segmentation of an image, the graph-based approach of Felzenszwalb and Huttenlocher is broadly employed. One of its interesting properties is that the regions are computed in a greedy manner in quasi-linear time. The algorithm may be trivially extended to video segmentation by considering a video as a 3D volume, however, this can not be the case for causal segmentation, when subsequent frames are unknown. We propose an efficient video segmentation approach that computes temporally consistent pixels in a causal manner, filling the need for causal and real time applications.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2013-01-08T20:56:17Z</published>
    <arxiv:comment>6 pages, 5 figures</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Camille Couprie</name>
    </author>
    <author>
      <name>Clément Farabet</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.0142v2</id>
    <title>Pedestrian Detection with Unsupervised Multi-Stage Feature Learning</title>
    <updated>2013-04-02T18:05:46Z</updated>
    <link href="https://arxiv.org/abs/1212.0142v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1212.0142v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>Pedestrian detection is a problem of considerable practical interest. Adding to the list of successful applications of deep learning methods to vision, we report state-of-the-art and competitive results on all major pedestrian datasets with a convolutional network model. The model uses a few new twists, such as multi-stage features, connections that skip layers to integrate global shape information with local distinctive motif information, and an unsupervised method based on convolutional sparse coding to pre-train the filters at each stage.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2012-12-01T18:13:03Z</published>
    <arxiv:comment>12 pages</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Pierre Sermanet</name>
    </author>
    <author>
      <name>Koray Kavukcuoglu</name>
    </author>
    <author>
      <name>Soumith Chintala</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.1106v2</id>
    <title>No More Pesky Learning Rates</title>
    <updated>2013-02-18T16:09:50Z</updated>
    <link href="https://arxiv.org/abs/1206.1106v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1206.1106v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>The performance of stochastic gradient descent (SGD) depends critically on how learning rates are tuned and decreased over time. We propose a method to automatically adjust multiple learning rates so as to minimize the expected error at any one time. The method relies on local gradient variations across samples. In our approach, learning rates can increase as well as decrease, making it suitable for non-stationary problems. Using a number of convex and non-convex learning tasks, we show that the resulting algorithm matches the performance of SGD or other adaptive approaches with their best settings obtained through systematic search, and effectively removes the need for learning rate tuning.</summary>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2012-06-06T02:06:57Z</published>
    <arxiv:primary_category term="stat.ML"/>
    <author>
      <name>Tom Schaul</name>
    </author>
    <author>
      <name>Sixin Zhang</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.3968v1</id>
    <title>Convolutional Neural Networks Applied to House Numbers Digit Classification</title>
    <updated>2012-04-18T03:48:38Z</updated>
    <link href="https://arxiv.org/abs/1204.3968v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1204.3968v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We classify digits of real-world house numbers using convolutional neural networks (ConvNets). ConvNets are hierarchical feature learning neural networks whose structure is biologically inspired. Unlike many popular vision approaches that are hand-designed, ConvNets can automatically learn a unique set of features optimized for a given task. We augmented the traditional ConvNet architecture by learning multi-stage features and by using Lp pooling and establish a new state-of-the-art of 94.85% accuracy on the SVHN dataset (45.2% error improvement). Furthermore, we analyze the benefits of different pooling methods and multi-stage features in ConvNets. The source code and a tutorial are available at eblearn.sf.net.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2012-04-18T03:48:38Z</published>
    <arxiv:comment>4 pages, 6 figures, 2 tables</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Pierre Sermanet</name>
    </author>
    <author>
      <name>Soumith Chintala</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.6384v1</id>
    <title>Fast approximations to structured sparse coding and applications to object classification</title>
    <updated>2012-02-28T21:27:14Z</updated>
    <link href="https://arxiv.org/abs/1202.6384v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1202.6384v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We describe a method for fast approximation of sparse coding. The input space is subdivided by a binary decision tree, and we simultaneously learn a dictionary and assignment of allowed dictionary elements for each leaf of the tree. We store a lookup table with the assignments and the pseudoinverses for each node, allowing for very fast inference. We give an algorithm for learning the tree, the dictionary and the dictionary element assignment, and In the process of describing this algorithm, we discuss the more general problem of learning the groups in group structured sparse modelling. We show that our method creates good sparse representations by using it in the object recognition framework of \cite{lazebnik06,yang-cvpr-09}. Implementing our own fast version of the SIFT descriptor the whole system runs at 20 frames per second on $321 \times 481$ sized images on a laptop with a quad-core cpu, while sacrificing very little accuracy on the Caltech 101 and 15 scenes benchmarks.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2012-02-28T21:27:14Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Arthur Szlam</name>
    </author>
    <author>
      <name>Karol Gregor</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.2160v2</id>
    <title>Scene Parsing with Multiscale Feature Learning, Purity Trees, and Optimal Covers</title>
    <updated>2012-07-13T21:32:24Z</updated>
    <link href="https://arxiv.org/abs/1202.2160v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1202.2160v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>Scene parsing, or semantic segmentation, consists in labeling each pixel in an image with the category of the object it belongs to. It is a challenging task that involves the simultaneous detection, segmentation and recognition of all the objects in the image.
  The scene parsing method proposed here starts by computing a tree of segments from a graph of pixel dissimilarities. Simultaneously, a set of dense feature vectors is computed which encodes regions of multiple sizes centered on each pixel. The feature extractor is a multiscale convolutional network trained from raw pixels. The feature vectors associated with the segments covered by each node in the tree are aggregated and fed to a classifier which produces an estimate of the distribution of object categories contained in the segment. A subset of tree nodes that cover the image are then selected so as to maximize the average "purity" of the class distributions, hence maximizing the overall likelihood that each segment will contain a single object. The convolutional network feature extractor is trained end-to-end from raw pixels, alleviating the need for engineered features. After training, the system is parameter free.
  The system yields record accuracies on the Stanford Background Dataset (8 classes), the Sift Flow Dataset (33 classes) and the Barcelona Dataset (170 classes) while being an order of magnitude faster than competing approaches, producing a 320 \times 240 image labeling in less than 1 second.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2012-02-10T00:30:48Z</published>
    <arxiv:comment>9 pages, 4 figures - Published in 29th International Conference on Machine Learning (ICML 2012), Jun 2012, Edinburgh, United Kingdom</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Clément Farabet</name>
    </author>
    <author>
      <name>Camille Couprie</name>
    </author>
    <author>
      <name>Laurent Najman</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.1169v1</id>
    <title>Learning Representations by Maximizing Compression</title>
    <updated>2011-08-04T19:00:14Z</updated>
    <link href="https://arxiv.org/abs/1108.1169v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1108.1169v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We give an algorithm that learns a representation of data through compression. The algorithm 1) predicts bits sequentially from those previously seen and 2) has a structure and a number of computations similar to an autoencoder. The likelihood under the model can be calculated exactly, and arithmetic coding can be used directly for compression. When training on digits the algorithm learns filters similar to those of restricted boltzman machines and denoising autoencoders. Independent samples can be drawn from the model by a single sweep through the pixels. The algorithm has a good compression performance when compared to other methods that work under random ordering of pixels.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2011-08-04T19:00:14Z</published>
    <arxiv:comment>8 pages, 3 figures</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Karol Gregor</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.5307v1</id>
    <title>Efficient Learning of Sparse Invariant Representations</title>
    <updated>2011-05-26T14:31:58Z</updated>
    <link href="https://arxiv.org/abs/1105.5307v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1105.5307v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We propose a simple and efficient algorithm for learning sparse invariant representations from unlabeled data with fast inference. When trained on short movies sequences, the learned features are selective to a range of orientations and spatial frequencies, but robust to a wide range of positions, similar to complex cells in the primary visual cortex. We give a hierarchical version of the algorithm, and give guarantees of fast convergence under certain conditions.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2011-05-26T14:31:58Z</published>
    <arxiv:comment>9 pages + 6 supplement pages</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Karol Gregor</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1010.3467v1</id>
    <title>Fast Inference in Sparse Coding Algorithms with Applications to Object Recognition</title>
    <updated>2010-10-18T02:31:21Z</updated>
    <link href="https://arxiv.org/abs/1010.3467v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1010.3467v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Adaptive sparse coding methods learn a possibly overcomplete set of basis functions, such that natural image patches can be reconstructed by linearly combining a small subset of these bases. The applicability of these methods to visual object recognition tasks has been limited because of the prohibitive cost of the optimization algorithms required to compute the sparse representation. In this work we propose a simple and efficient algorithm to learn basis functions. After training, this model also provides a fast and smooth approximator to the optimal representation, achieving even better accuracy than exact sparse coding algorithms on visual object recognition tasks.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2010-10-18T02:31:21Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Koray Kavukcuoglu</name>
    </author>
    <author>
      <name>Marc'Aurelio Ranzato</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1010.0422v1</id>
    <title>Convolutional Matching Pursuit and Dictionary Training</title>
    <updated>2010-10-03T16:55:56Z</updated>
    <link href="https://arxiv.org/abs/1010.0422v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1010.0422v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Matching pursuit and K-SVD is demonstrated in the translation invariant setting</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2010-10-03T16:55:56Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Arthur Szlam</name>
    </author>
    <author>
      <name>Koray Kavukcuoglu</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.0448v1</id>
    <title>Emergence of Complex-Like Cells in a Temporal Product Network with Local Receptive Fields</title>
    <updated>2010-06-02T17:08:29Z</updated>
    <link href="https://arxiv.org/abs/1006.0448v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1006.0448v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We introduce a new neural architecture and an unsupervised algorithm for learning invariant representations from temporal sequence of images. The system uses two groups of complex cells whose outputs are combined multiplicatively: one that represents the content of the image, constrained to be constant over several consecutive frames, and one that represents the precise location of features, which is allowed to vary over time but constrained to be sparse. The architecture uses an encoder to extract features, and a decoder to reconstruct the input from the features. The method was applied to patches extracted from consecutive movie frames and produces orientation and frequency selective units analogous to the complex cells in V1. An extension of the method is proposed to train a network composed of units with local receptive field spread over a large image of arbitrary size. A layer of complex cells, subject to sparsity constraints, pool feature units over overlapping local neighborhoods, which causes the feature units to organize themselves into pinwheel patterns of orientation-selective receptive fields, similar to those observed in the mammalian visual cortex. A feed-forward encoder efficiently computes the feature representation of full images.</summary>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2010-06-02T17:08:29Z</published>
    <arxiv:primary_category term="cs.NE"/>
    <author>
      <name>Karo Gregor</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
  </entry>
</feed>
